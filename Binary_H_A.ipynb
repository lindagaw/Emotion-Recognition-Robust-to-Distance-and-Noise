{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data: (16632, 48, 100)\n",
      "training label: (16632, 2)\n",
      "evaluation data: (3684, 48, 100)\n",
      "evaluation label: (3684, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ash Gao\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:192: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(input_shape=(48, 100), kernel_constraint=<keras.con..., filters=512, kernel_size=3)`\n",
      "C:\\Users\\Ash Gao\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:198: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(kernel_constraint=<keras.con..., filters=1024, kernel_size=3)`\n",
      "C:\\Users\\Ash Gao\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:204: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(kernel_constraint=<keras.con..., filters=1536, kernel_size=3)`\n",
      "C:\\Users\\Ash Gao\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:210: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(kernel_constraint=<keras.con..., filters=1024, kernel_size=3)`\n",
      "C:\\Users\\Ash Gao\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:245: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_5 (Conv1D)            (None, 46, 512)           154112    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 46, 512)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 23, 512)           0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 23, 512)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 21, 1024)          1573888   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 21, 1024)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 10, 1024)          0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 10, 1024)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 8, 1536)           4720128   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 8, 1536)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 4, 1536)           0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 4, 1536)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 2, 1024)           4719616   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 2, 1024)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 1, 1024)           0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 1, 1024)           0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 2050      \n",
      "=================================================================\n",
      "Total params: 11,169,794\n",
      "Trainable params: 11,169,794\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 12474 samples, validate on 4158 samples\n",
      "Epoch 1/50000\n",
      "12474/12474 [==============================] - 208s 17ms/step - loss: 0.6740 - acc: 0.5771 - val_loss: 0.6591 - val_acc: 0.5931\n",
      "Epoch 2/50000\n",
      "12474/12474 [==============================] - 203s 16ms/step - loss: 0.6466 - acc: 0.6259 - val_loss: 0.6396 - val_acc: 0.6253\n",
      "Epoch 3/50000\n",
      "12474/12474 [==============================] - 203s 16ms/step - loss: 0.6176 - acc: 0.6638 - val_loss: 0.6022 - val_acc: 0.6799\n",
      "Epoch 4/50000\n",
      "12474/12474 [==============================] - 204s 16ms/step - loss: 0.5940 - acc: 0.6893 - val_loss: 0.5920 - val_acc: 0.6835\n",
      "Epoch 5/50000\n",
      "12474/12474 [==============================] - 204s 16ms/step - loss: 0.5777 - acc: 0.6968 - val_loss: 0.5606 - val_acc: 0.7100\n",
      "Epoch 6/50000\n",
      "12474/12474 [==============================] - 203s 16ms/step - loss: 0.5605 - acc: 0.7116 - val_loss: 0.5548 - val_acc: 0.7196\n",
      "Epoch 7/50000\n",
      "12474/12474 [==============================] - 203s 16ms/step - loss: 0.5430 - acc: 0.7268 - val_loss: 0.5388 - val_acc: 0.7268\n",
      "Epoch 8/50000\n",
      "12474/12474 [==============================] - 203s 16ms/step - loss: 0.5273 - acc: 0.7366 - val_loss: 0.5214 - val_acc: 0.7338\n",
      "Epoch 9/50000\n",
      "12474/12474 [==============================] - 204s 16ms/step - loss: 0.5086 - acc: 0.7500 - val_loss: 0.5047 - val_acc: 0.7472\n",
      "Epoch 10/50000\n",
      "12474/12474 [==============================] - 203s 16ms/step - loss: 0.5075 - acc: 0.7504 - val_loss: 0.5046 - val_acc: 0.7489\n",
      "Epoch 11/50000\n",
      "12474/12474 [==============================] - 204s 16ms/step - loss: 0.4856 - acc: 0.7686 - val_loss: 0.4890 - val_acc: 0.7595\n",
      "Epoch 12/50000\n",
      "12474/12474 [==============================] - 202s 16ms/step - loss: 0.4705 - acc: 0.7769 - val_loss: 0.4775 - val_acc: 0.7689\n",
      "Epoch 13/50000\n",
      "12474/12474 [==============================] - 204s 16ms/step - loss: 0.4568 - acc: 0.7822 - val_loss: 0.4905 - val_acc: 0.7518\n",
      "Epoch 14/50000\n",
      "12474/12474 [==============================] - 203s 16ms/step - loss: 0.4480 - acc: 0.7906 - val_loss: 0.4643 - val_acc: 0.7725\n",
      "Epoch 15/50000\n",
      "12474/12474 [==============================] - 202s 16ms/step - loss: 0.4285 - acc: 0.8029 - val_loss: 0.4554 - val_acc: 0.7874\n",
      "Epoch 16/50000\n",
      "12474/12474 [==============================] - 202s 16ms/step - loss: 0.4182 - acc: 0.8066 - val_loss: 0.4636 - val_acc: 0.7766\n",
      "Epoch 17/50000\n",
      "12474/12474 [==============================] - 202s 16ms/step - loss: 0.4013 - acc: 0.8172 - val_loss: 0.4361 - val_acc: 0.7949\n",
      "Epoch 18/50000\n",
      "12474/12474 [==============================] - 203s 16ms/step - loss: 0.3884 - acc: 0.8231 - val_loss: 0.4310 - val_acc: 0.8016\n",
      "Epoch 19/50000\n",
      "12474/12474 [==============================] - 202s 16ms/step - loss: 0.3833 - acc: 0.8268 - val_loss: 0.4764 - val_acc: 0.7698\n",
      "Epoch 20/50000\n",
      "12474/12474 [==============================] - 203s 16ms/step - loss: 0.3607 - acc: 0.8404 - val_loss: 0.4350 - val_acc: 0.7977\n",
      "Epoch 21/50000\n",
      "12474/12474 [==============================] - 202s 16ms/step - loss: 0.3418 - acc: 0.8512 - val_loss: 0.4238 - val_acc: 0.8045\n",
      "Epoch 22/50000\n",
      "12474/12474 [==============================] - 203s 16ms/step - loss: 0.3475 - acc: 0.8466 - val_loss: 0.4279 - val_acc: 0.8059\n",
      "Epoch 23/50000\n",
      "12474/12474 [==============================] - 202s 16ms/step - loss: 0.3243 - acc: 0.8575 - val_loss: 0.4055 - val_acc: 0.8143\n",
      "Epoch 24/50000\n",
      "12474/12474 [==============================] - 203s 16ms/step - loss: 0.3139 - acc: 0.8663 - val_loss: 0.4046 - val_acc: 0.8167\n",
      "Epoch 25/50000\n",
      "12474/12474 [==============================] - 203s 16ms/step - loss: 0.3037 - acc: 0.8685 - val_loss: 0.3893 - val_acc: 0.8249\n",
      "Epoch 26/50000\n",
      "12474/12474 [==============================] - 203s 16ms/step - loss: 0.2904 - acc: 0.8797 - val_loss: 0.3838 - val_acc: 0.8292\n",
      "Epoch 27/50000\n",
      "12474/12474 [==============================] - 202s 16ms/step - loss: 0.2672 - acc: 0.8919 - val_loss: 0.4079 - val_acc: 0.8218\n",
      "Epoch 28/50000\n",
      "12474/12474 [==============================] - 202s 16ms/step - loss: 0.2773 - acc: 0.8834 - val_loss: 0.3840 - val_acc: 0.8312\n",
      "Epoch 29/50000\n",
      "12474/12474 [==============================] - 202s 16ms/step - loss: 0.2596 - acc: 0.8927 - val_loss: 0.3692 - val_acc: 0.8367\n",
      "Epoch 30/50000\n",
      "12474/12474 [==============================] - 202s 16ms/step - loss: 0.2407 - acc: 0.9017 - val_loss: 0.3653 - val_acc: 0.8384\n",
      "Epoch 31/50000\n",
      "12474/12474 [==============================] - 203s 16ms/step - loss: 0.2370 - acc: 0.9055 - val_loss: 0.3753 - val_acc: 0.8343\n",
      "Epoch 32/50000\n",
      "12474/12474 [==============================] - 202s 16ms/step - loss: 0.2322 - acc: 0.9036 - val_loss: 0.4026 - val_acc: 0.8179\n",
      "Epoch 33/50000\n",
      "12474/12474 [==============================] - 202s 16ms/step - loss: 0.2151 - acc: 0.9129 - val_loss: 0.3525 - val_acc: 0.8458\n",
      "Epoch 34/50000\n",
      "12474/12474 [==============================] - 202s 16ms/step - loss: 0.2056 - acc: 0.9197 - val_loss: 0.3489 - val_acc: 0.8463\n",
      "Epoch 35/50000\n",
      "12474/12474 [==============================] - 203s 16ms/step - loss: 0.1926 - acc: 0.9263 - val_loss: 0.3507 - val_acc: 0.8446\n",
      "Epoch 36/50000\n",
      "12474/12474 [==============================] - 203s 16ms/step - loss: 0.1886 - acc: 0.9276 - val_loss: 0.3436 - val_acc: 0.8463\n",
      "Epoch 37/50000\n",
      "12474/12474 [==============================] - 201s 16ms/step - loss: 0.1788 - acc: 0.9303 - val_loss: 0.3402 - val_acc: 0.8519\n",
      "Epoch 38/50000\n",
      "12474/12474 [==============================] - 202s 16ms/step - loss: 0.1695 - acc: 0.9354 - val_loss: 0.3335 - val_acc: 0.8550\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/50000\n",
      "12474/12474 [==============================] - 203s 16ms/step - loss: 0.1612 - acc: 0.9400 - val_loss: 0.3326 - val_acc: 0.8550\n",
      "Epoch 40/50000\n",
      "12474/12474 [==============================] - 202s 16ms/step - loss: 0.1549 - acc: 0.9428 - val_loss: 0.3766 - val_acc: 0.8442\n",
      "Epoch 41/50000\n",
      "12474/12474 [==============================] - 202s 16ms/step - loss: 0.1557 - acc: 0.9412 - val_loss: 0.3468 - val_acc: 0.8535\n",
      "Epoch 42/50000\n",
      "12474/12474 [==============================] - 202s 16ms/step - loss: 0.1605 - acc: 0.9384 - val_loss: 0.3253 - val_acc: 0.8608\n",
      "Epoch 43/50000\n",
      "12474/12474 [==============================] - 202s 16ms/step - loss: 0.1384 - acc: 0.9505 - val_loss: 0.3252 - val_acc: 0.8598\n",
      "Epoch 44/50000\n",
      "12474/12474 [==============================] - 202s 16ms/step - loss: 0.1204 - acc: 0.9565 - val_loss: 0.3560 - val_acc: 0.8528\n",
      "Epoch 45/50000\n",
      "12474/12474 [==============================] - 205s 16ms/step - loss: 0.1229 - acc: 0.9549 - val_loss: 0.3333 - val_acc: 0.8651\n",
      "Epoch 46/50000\n",
      "12474/12474 [==============================] - 203s 16ms/step - loss: 0.1124 - acc: 0.9602 - val_loss: 0.3288 - val_acc: 0.8665\n",
      "Epoch 47/50000\n",
      "12474/12474 [==============================] - 202s 16ms/step - loss: 0.1086 - acc: 0.9610 - val_loss: 0.3318 - val_acc: 0.8653\n",
      "Epoch 48/50000\n",
      "12474/12474 [==============================] - 202s 16ms/step - loss: 0.1074 - acc: 0.9624 - val_loss: 0.3328 - val_acc: 0.8612\n",
      "Epoch 49/50000\n",
      "12474/12474 [==============================] - 203s 16ms/step - loss: 0.1046 - acc: 0.9619 - val_loss: 0.3187 - val_acc: 0.8704\n",
      "Epoch 50/50000\n",
      "12474/12474 [==============================] - 202s 16ms/step - loss: 0.0996 - acc: 0.9669 - val_loss: 0.3181 - val_acc: 0.8716\n",
      "Epoch 51/50000\n",
      "12474/12474 [==============================] - 203s 16ms/step - loss: 0.0913 - acc: 0.9695 - val_loss: 0.3164 - val_acc: 0.8704\n",
      "Epoch 52/50000\n",
      "12474/12474 [==============================] - 203s 16ms/step - loss: 0.0868 - acc: 0.9697 - val_loss: 0.3112 - val_acc: 0.8769\n",
      "Epoch 53/50000\n",
      "12474/12474 [==============================] - 202s 16ms/step - loss: 0.0794 - acc: 0.9733 - val_loss: 0.3327 - val_acc: 0.8692\n",
      "Epoch 54/50000\n",
      "12474/12474 [==============================] - 203s 16ms/step - loss: 0.0775 - acc: 0.9743 - val_loss: 0.3273 - val_acc: 0.8684\n",
      "Epoch 55/50000\n",
      "12474/12474 [==============================] - 202s 16ms/step - loss: 0.0753 - acc: 0.9747 - val_loss: 0.3195 - val_acc: 0.8752\n",
      "Epoch 56/50000\n",
      "12474/12474 [==============================] - 202s 16ms/step - loss: 0.0730 - acc: 0.9758 - val_loss: 0.3156 - val_acc: 0.8769\n",
      "Epoch 57/50000\n",
      "12474/12474 [==============================] - 204s 16ms/step - loss: 0.0679 - acc: 0.9785 - val_loss: 0.3170 - val_acc: 0.8752\n",
      "Epoch 58/50000\n",
      "12474/12474 [==============================] - 203s 16ms/step - loss: 0.0643 - acc: 0.9789 - val_loss: 0.3418 - val_acc: 0.8675\n",
      "Epoch 59/50000\n",
      "12474/12474 [==============================] - 202s 16ms/step - loss: 0.0649 - acc: 0.9796 - val_loss: 0.3218 - val_acc: 0.8785\n",
      "Epoch 60/50000\n",
      "12474/12474 [==============================] - 203s 16ms/step - loss: 0.0577 - acc: 0.9828 - val_loss: 0.3182 - val_acc: 0.8795\n",
      "Epoch 61/50000\n",
      "12474/12474 [==============================] - 202s 16ms/step - loss: 0.0565 - acc: 0.9823 - val_loss: 0.3200 - val_acc: 0.8773\n",
      "Epoch 62/50000\n",
      "12474/12474 [==============================] - 202s 16ms/step - loss: 0.0577 - acc: 0.9796 - val_loss: 0.3503 - val_acc: 0.8730\n",
      "Epoch 63/50000\n",
      "12474/12474 [==============================] - 203s 16ms/step - loss: 0.0584 - acc: 0.9813 - val_loss: 0.3912 - val_acc: 0.8583\n",
      "Epoch 64/50000\n",
      "12474/12474 [==============================] - 202s 16ms/step - loss: 0.0485 - acc: 0.9861 - val_loss: 0.3116 - val_acc: 0.8805\n",
      "Epoch 65/50000\n",
      "12474/12474 [==============================] - 203s 16ms/step - loss: 0.0470 - acc: 0.9863 - val_loss: 0.3201 - val_acc: 0.8817\n",
      "Epoch 66/50000\n",
      "12474/12474 [==============================] - 203s 16ms/step - loss: 0.0500 - acc: 0.9832 - val_loss: 0.3274 - val_acc: 0.8790\n",
      "Epoch 67/50000\n",
      "12474/12474 [==============================] - 202s 16ms/step - loss: 0.0481 - acc: 0.9848 - val_loss: 0.3083 - val_acc: 0.8846\n",
      "Epoch 68/50000\n",
      "12474/12474 [==============================] - 203s 16ms/step - loss: 0.0430 - acc: 0.9869 - val_loss: 0.3269 - val_acc: 0.8836\n",
      "Epoch 69/50000\n",
      "12474/12474 [==============================] - 204s 16ms/step - loss: 0.0453 - acc: 0.9861 - val_loss: 0.3180 - val_acc: 0.8846\n",
      "Epoch 70/50000\n",
      "12474/12474 [==============================] - 203s 16ms/step - loss: 0.0419 - acc: 0.9865 - val_loss: 0.3211 - val_acc: 0.8853\n",
      "Epoch 71/50000\n",
      "12474/12474 [==============================] - 203s 16ms/step - loss: 0.0414 - acc: 0.9875 - val_loss: 0.3971 - val_acc: 0.8629\n",
      "Epoch 72/50000\n",
      "12474/12474 [==============================] - 203s 16ms/step - loss: 0.0391 - acc: 0.9880 - val_loss: 0.3304 - val_acc: 0.8790\n",
      "Epoch 73/50000\n",
      "12474/12474 [==============================] - 203s 16ms/step - loss: 0.0441 - acc: 0.9865 - val_loss: 0.3168 - val_acc: 0.8862\n",
      "Epoch 74/50000\n",
      "12474/12474 [==============================] - 202s 16ms/step - loss: 0.0392 - acc: 0.9885 - val_loss: 0.3219 - val_acc: 0.8858\n",
      "Epoch 75/50000\n",
      "12474/12474 [==============================] - 204s 16ms/step - loss: 0.0340 - acc: 0.9897 - val_loss: 0.3188 - val_acc: 0.8829\n",
      "Epoch 76/50000\n",
      "12474/12474 [==============================] - 203s 16ms/step - loss: 0.0318 - acc: 0.9899 - val_loss: 0.3197 - val_acc: 0.8822\n",
      "Epoch 77/50000\n",
      "12474/12474 [==============================] - 202s 16ms/step - loss: 0.0331 - acc: 0.9899 - val_loss: 0.3245 - val_acc: 0.8843\n",
      "Epoch 78/50000\n",
      "12474/12474 [==============================] - 203s 16ms/step - loss: 0.0281 - acc: 0.9933 - val_loss: 0.3479 - val_acc: 0.8838\n",
      "Epoch 79/50000\n",
      "12474/12474 [==============================] - 203s 16ms/step - loss: 0.0299 - acc: 0.9916 - val_loss: 0.3207 - val_acc: 0.8824\n",
      "Epoch 80/50000\n",
      "12474/12474 [==============================] - 202s 16ms/step - loss: 0.0286 - acc: 0.9917 - val_loss: 0.3201 - val_acc: 0.8879\n",
      "Epoch 81/50000\n",
      "12474/12474 [==============================] - 203s 16ms/step - loss: 0.0279 - acc: 0.9927 - val_loss: 0.3275 - val_acc: 0.8841\n",
      "Epoch 82/50000\n",
      "12474/12474 [==============================] - 203s 16ms/step - loss: 0.0293 - acc: 0.9917 - val_loss: 0.3321 - val_acc: 0.8879\n",
      "Epoch 83/50000\n",
      "12474/12474 [==============================] - 202s 16ms/step - loss: 0.0278 - acc: 0.9918 - val_loss: 0.3397 - val_acc: 0.8867\n",
      "Epoch 84/50000\n",
      "12474/12474 [==============================] - 202s 16ms/step - loss: 0.0279 - acc: 0.9919 - val_loss: 0.3265 - val_acc: 0.8903\n",
      "Epoch 85/50000\n",
      "12474/12474 [==============================] - 203s 16ms/step - loss: 0.0255 - acc: 0.9921 - val_loss: 0.3293 - val_acc: 0.8886\n",
      "Epoch 86/50000\n",
      "12474/12474 [==============================] - 202s 16ms/step - loss: 0.0238 - acc: 0.9930 - val_loss: 0.3290 - val_acc: 0.8870\n",
      "Epoch 87/50000\n",
      "12474/12474 [==============================] - 202s 16ms/step - loss: 0.0245 - acc: 0.9933 - val_loss: 0.3255 - val_acc: 0.8877\n",
      "Epoch 88/50000\n",
      "12474/12474 [==============================] - 202s 16ms/step - loss: 0.0217 - acc: 0.9946 - val_loss: 0.3347 - val_acc: 0.8853\n",
      "Epoch 89/50000\n",
      "12474/12474 [==============================] - 203s 16ms/step - loss: 0.0231 - acc: 0.9933 - val_loss: 0.3267 - val_acc: 0.8918\n",
      "Epoch 90/50000\n",
      "12474/12474 [==============================] - 203s 16ms/step - loss: 0.0257 - acc: 0.9931 - val_loss: 0.3413 - val_acc: 0.8865\n",
      "Epoch 91/50000\n",
      "12474/12474 [==============================] - 202s 16ms/step - loss: 0.0208 - acc: 0.9948 - val_loss: 0.3364 - val_acc: 0.8877\n",
      "Epoch 92/50000\n",
      "12474/12474 [==============================] - 203s 16ms/step - loss: 0.0234 - acc: 0.9929 - val_loss: 0.3292 - val_acc: 0.8870\n",
      "Epoch 93/50000\n",
      "12474/12474 [==============================] - 203s 16ms/step - loss: 0.0240 - acc: 0.9935 - val_loss: 0.3272 - val_acc: 0.8884\n",
      "Epoch 94/50000\n",
      "12474/12474 [==============================] - 203s 16ms/step - loss: 0.0234 - acc: 0.9930 - val_loss: 0.3316 - val_acc: 0.8901\n",
      "Epoch 95/50000\n",
      "12474/12474 [==============================] - 202s 16ms/step - loss: 0.0210 - acc: 0.9935 - val_loss: 0.3282 - val_acc: 0.8899\n",
      "Epoch 96/50000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12474/12474 [==============================] - 203s 16ms/step - loss: 0.0207 - acc: 0.9944 - val_loss: 0.3390 - val_acc: 0.8867\n",
      "Epoch 97/50000\n",
      "12474/12474 [==============================] - 202s 16ms/step - loss: 0.0184 - acc: 0.9957 - val_loss: 0.3670 - val_acc: 0.8790\n",
      "Epoch 98/50000\n",
      "12474/12474 [==============================] - 202s 16ms/step - loss: 0.0182 - acc: 0.9953 - val_loss: 0.3385 - val_acc: 0.8906\n",
      "Epoch 99/50000\n",
      "12474/12474 [==============================] - 202s 16ms/step - loss: 0.0170 - acc: 0.9956 - val_loss: 0.3352 - val_acc: 0.8891\n",
      "Epoch 100/50000\n",
      "12474/12474 [==============================] - 202s 16ms/step - loss: 0.0166 - acc: 0.9961 - val_loss: 0.3293 - val_acc: 0.8911\n",
      "Epoch 101/50000\n",
      "12474/12474 [==============================] - 202s 16ms/step - loss: 0.0175 - acc: 0.9952 - val_loss: 0.3522 - val_acc: 0.8865\n",
      "Epoch 102/50000\n",
      "12474/12474 [==============================] - 202s 16ms/step - loss: 0.0184 - acc: 0.9946 - val_loss: 0.3377 - val_acc: 0.8913\n",
      "Epoch 103/50000\n",
      "12474/12474 [==============================] - 202s 16ms/step - loss: 0.0176 - acc: 0.9950 - val_loss: 0.3315 - val_acc: 0.8927\n",
      "Epoch 104/50000\n",
      "12474/12474 [==============================] - 202s 16ms/step - loss: 0.0172 - acc: 0.9957 - val_loss: 0.3412 - val_acc: 0.8874\n",
      "Epoch 105/50000\n",
      "12474/12474 [==============================] - 202s 16ms/step - loss: 0.0162 - acc: 0.9956 - val_loss: 0.3559 - val_acc: 0.8853\n",
      "Epoch 106/50000\n",
      "12474/12474 [==============================] - 202s 16ms/step - loss: 0.0148 - acc: 0.9964 - val_loss: 0.3259 - val_acc: 0.8935\n",
      "Epoch 107/50000\n",
      "12474/12474 [==============================] - 202s 16ms/step - loss: 0.0161 - acc: 0.9958 - val_loss: 0.3399 - val_acc: 0.8937\n",
      "Epoch 108/50000\n",
      "12474/12474 [==============================] - 202s 16ms/step - loss: 0.0156 - acc: 0.9957 - val_loss: 0.3359 - val_acc: 0.8918\n",
      "Epoch 109/50000\n",
      "12474/12474 [==============================] - 202s 16ms/step - loss: 0.0143 - acc: 0.9967 - val_loss: 0.3261 - val_acc: 0.8966\n",
      "Epoch 110/50000\n",
      "12474/12474 [==============================] - 202s 16ms/step - loss: 0.0148 - acc: 0.9960 - val_loss: 0.3266 - val_acc: 0.8959\n",
      "Epoch 111/50000\n",
      "12474/12474 [==============================] - 202s 16ms/step - loss: 0.0117 - acc: 0.9978 - val_loss: 0.3705 - val_acc: 0.8862\n",
      "Epoch 112/50000\n",
      "12474/12474 [==============================] - 214s 17ms/step - loss: 0.0138 - acc: 0.9971 - val_loss: 0.3427 - val_acc: 0.8935\n",
      "Epoch 113/50000\n",
      "  512/12474 [>.............................] - ETA: 3:07 - loss: 0.0128 - acc: 0.9961"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import gc\n",
    "import sys\n",
    "import h5py\n",
    "import time\n",
    "import datetime\n",
    "import pickle\n",
    "import librosa\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import pandas as pd\n",
    "from pandas.plotting import parallel_coordinates\n",
    "from pydub import AudioSegment\n",
    "\n",
    "#imported for testing\n",
    "import wave\n",
    "import contextlib\n",
    "\n",
    "# for outputing file\n",
    "from scipy.cluster.vq import vq, kmeans, whiten\n",
    "import scipy.stats.stats as st\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.decomposition import PCA as sklearnPCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import auc, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn import mixture\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "import keras\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.constraints import maxnorm\n",
    "from keras.layers import Convolution1D, Dense, MaxPooling1D, Flatten, Add, Dropout, Input, Activation\n",
    "from keras.layers import TimeDistributed, Bidirectional, LSTM, LeakyReLU\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers, regularizers\n",
    "from keras.utils import np_utils, to_categorical\n",
    "\n",
    "from colorama import Fore, Back, Style\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "#warnings.filterwarnings('ignore')\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# confirm TensorFlow sees the GPU\n",
    "from tensorflow.python.client import device_lib\n",
    "# assert 'GPU' in str(device_lib.list_local_devices())\n",
    "\n",
    "# confirm Keras sees the GPU\n",
    "from keras import backend\n",
    "# print(len(backend.tensorflow_backend._get_available_gpus()) > 0)\n",
    "\n",
    "#warnings.filterwarnings('ignore')\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "sample_rate = 44100\n",
    "frame_number = 48\n",
    "hop_length = 441  # frame size= 2 * hop\n",
    "segment_length = int(sample_rate * 0.2)  # 0.2\n",
    "segment_pad = int(sample_rate * 0.02)     # 0.02\n",
    "overlapping = int(sample_rate * 0.1)   # 0.1\n",
    "\n",
    "classes = 2\n",
    "NumofFeaturetoUse = 100\n",
    "n_neurons = 4096\n",
    "dense_layers = 1\n",
    "num_layers = 4\n",
    "fillength = 3\n",
    "nbindex = 512\n",
    "dropout = 0.2\n",
    "n_batch = 128\n",
    "n_epoch = 50000\n",
    "\n",
    "def update_progress(progress):\n",
    "    bar_length = 100\n",
    "    if isinstance(progress, int):\n",
    "        progress = float(progress)\n",
    "    if not isinstance(progress, float):\n",
    "        progress = 0\n",
    "    if progress < 0:\n",
    "        progress = 0\n",
    "    if progress >= 1:\n",
    "        progress = 1\n",
    "\n",
    "    block = int(round(bar_length * progress))\n",
    "    clear_output(wait = True)\n",
    "    \n",
    "    text = \"Progress: [{0}] {1:.1f}%\".format( \"#\" * block + \"-\" * (bar_length - block), progress * 100)\n",
    "    print(text)\n",
    "\n",
    "prefix = '..//'\n",
    "h_feature_vector = np.load(prefix + 'Features//h_feature_vector_48.npy')\n",
    "h_label_vector = np.load(prefix + 'Features//h_label_vector_48.npy')\n",
    "a_feature_vector = np.load(prefix + 'Features//a_feature_vector_48.npy')\n",
    "a_label_vector = np.load(prefix + 'Features//a_label_vector_48.npy')\n",
    "n_feature_vector = np.load(prefix + 'Features//n_feature_vector_48.npy')\n",
    "n_label_vector = np.load(prefix + 'Features//n_label_vector_48.npy')\n",
    "s_feature_vector = np.load(prefix + 'Features//s_feature_vector_48.npy')\n",
    "s_label_vector = np.load(prefix + 'Features//s_label_vector_48.npy')\n",
    "\n",
    "h_feature_vector_test = np.load(prefix + 'Features//h_feature_vector_test_48.npy')\n",
    "h_label_vector_test = np.load(prefix + 'Features//h_label_vector_test_48.npy')\n",
    "a_feature_vector_test = np.load(prefix + 'Features//a_feature_vector_test_48.npy')\n",
    "a_label_vector_test = np.load(prefix + 'Features//a_label_vector_test_48.npy')\n",
    "n_feature_vector_test = np.load(prefix + 'Features//n_feature_vector_test_48.npy')\n",
    "n_label_vector_test = np.load(prefix + 'Features//n_label_vector_test_48.npy')\n",
    "s_feature_vector_test = np.load(prefix + 'Features//s_feature_vector_test_48.npy')\n",
    "s_label_vector_test = np.load(prefix + 'Features//s_label_vector_test_48.npy')\n",
    "\n",
    "h_label_vector[h_label_vector == 0] = 0\n",
    "a_label_vector[a_label_vector == 1] = 1\n",
    "h_label_vector_test[h_label_vector_test == 0] = 0\n",
    "a_label_vector_test[a_label_vector_test == 1] = 1\n",
    "\n",
    "h_label_vector = to_categorical(h_label_vector, num_classes = 2)\n",
    "a_label_vector = to_categorical(a_label_vector, num_classes = 2)\n",
    "h_label_vector_test = to_categorical(h_label_vector_test, num_classes = 2)\n",
    "a_label_vector_test = to_categorical(a_label_vector_test, num_classes = 2)\n",
    "\n",
    "# Load training npy files\n",
    "featureSet_training = np.vstack((h_feature_vector, a_feature_vector))\n",
    "label_training = np.vstack((h_label_vector, a_label_vector))\n",
    "\n",
    "# Load testing npy files\n",
    "featureSet_testing = np.vstack((h_feature_vector_test, a_feature_vector_test))\n",
    "label_testing = np.vstack((h_label_vector_test, a_label_vector_test))\n",
    "\n",
    "def float_compatible(input_np):\n",
    "\n",
    "    x = np.where(input_np >= np.finfo(np.float32).max)\n",
    "    for index in range(0, len(x[0])):\n",
    "        x_position = x[0][index]\n",
    "        y_position = x[1][index]      \n",
    "        input_np[x_position, y_position] = 0.0\n",
    "    input_np = np.nan_to_num(input_np)\n",
    "        \n",
    "    return input_np\n",
    "\n",
    "train_data = float_compatible((featureSet_training).astype(np.float32))\n",
    "eval_data = float_compatible((featureSet_testing).astype(np.float32))\n",
    "\n",
    "adam = optimizers.Adam(lr = 3e-5, beta_1 = 0.9, beta_2 = 0.999, epsilon = None, decay = 0, amsgrad = True)\n",
    "sgd = optimizers.SGD(lr = 0.01, decay = 1e-6, momentum = 0.9, nesterov = True)\n",
    "rmsprop = optimizers.RMSprop(lr = 0.0001, rho = 0.9, epsilon = None, decay = 0.0)\n",
    "adagrad = optimizers.Adagrad(lr = 0.01, epsilon = None, decay = 0.0)\n",
    "adadelta = optimizers.Adadelta(lr = 1.0, rho = 0.95, epsilon = None, decay = 0.0)\n",
    "adamax = optimizers.Adamax(lr = 0.002, beta_1 = 0.9, beta_2 = 0.999, epsilon = None, decay = 0.0)\n",
    "nadam = optimizers.Nadam(lr = 0.002, beta_1 = 0.9, beta_2 = 0.999, epsilon = None, schedule_decay = 0.004)\n",
    "\n",
    "featureSet = train_data\n",
    "Label = label_training\n",
    "featureSet = np.split(featureSet, np.array([NumofFeaturetoUse]), axis = 2)[0]\n",
    "\n",
    "print('training data: ' + str(featureSet.shape))\n",
    "print('training label: ' + str(Label.shape))\n",
    "\n",
    "featureSet_val = eval_data\n",
    "Label_val = label_testing\n",
    "featureSet_val = np.split(featureSet_val, np.array([NumofFeaturetoUse]), axis = 2)[0]\n",
    "\n",
    "print('evaluation data: ' + str(featureSet_val.shape))\n",
    "print('evaluation label: ' + str(Label_val.shape))\n",
    "\n",
    "def record(str_message, log_file):\n",
    "    str_message = str_message + '\\n'\n",
    "    file = open(log_file, 'a')\n",
    "    file.write(str_message)\n",
    "    file.close()\n",
    "\n",
    "def create_cnn(title, num_layers, n_neurons, n_batch, nbindex, dropout, classes, dense_layers):\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Convolution1D(nb_filter=nbindex, filter_length=fillength,\n",
    "                            input_shape=(featureSet.shape[1], featureSet.shape[2]), kernel_constraint=maxnorm(3)))\n",
    "    model.add(LeakyReLU(alpha=0.05))\n",
    "    model.add(MaxPooling1D(pool_size=2, strides=2, padding='valid'))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(Convolution1D(nb_filter=nbindex*2, filter_length=fillength,\n",
    "                            kernel_constraint=maxnorm(3)))\n",
    "    model.add(LeakyReLU(alpha=0.05))\n",
    "    model.add(MaxPooling1D(pool_size=2, strides=2, padding='valid'))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(Convolution1D(nb_filter=nbindex*3, filter_length=fillength,\n",
    "                            kernel_constraint=maxnorm(3)))\n",
    "    model.add(LeakyReLU(alpha=0.05))\n",
    "    model.add(MaxPooling1D(pool_size=2, strides=2, padding='valid'))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(Convolution1D(nb_filter=nbindex*2, filter_length=fillength,\n",
    "                            kernel_constraint=maxnorm(3)))  \n",
    "    model.add(LeakyReLU(alpha=0.05))\n",
    "    model.add(MaxPooling1D(pool_size=2, strides=2, padding='valid'))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(classes, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_cnn():\n",
    "    \n",
    "    save_to_path = prefix + str(num_layers) + \"_Layer(s)//\"\n",
    "\n",
    "    checkpoint_filepath = prefix + str(num_layers) + \"_Layer(s)//Checkpoint_\" + title + \".hdf5\"\n",
    "    final_filepath = prefix + str(num_layers) + \"_Layer(s)//Final_\" + title + \".hdf5\"\n",
    "\n",
    "    if not os.path.exists(save_to_path):\n",
    "        os.mkdir(save_to_path)\n",
    "\n",
    "    X, X_test, Y, Y_test= train_test_split(featureSet, Label, test_size = 0.25, shuffle = True)\n",
    "\n",
    "    model = create_cnn(title, num_layers, n_neurons, n_batch, nbindex, dropout, classes, dense_layers)\n",
    "\n",
    "    checkpoint = ModelCheckpoint(checkpoint_filepath, monitor = 'val_acc', verbose = 0, save_best_only = True, mode = 'auto')\n",
    "\n",
    "    early_stopping_monitor = EarlyStopping(patience = 100)\n",
    "\n",
    "    callbacks_list = [checkpoint, early_stopping_monitor]\n",
    "\n",
    "    model.fit(X, Y, nb_epoch = n_epoch, batch_size = n_batch,  callbacks = callbacks_list, validation_data = (X_test, Y_test), verbose = 1)\n",
    "\n",
    "    model.save_weights(final_filepath)\n",
    "\n",
    "    model.load_weights(checkpoint_filepath)\n",
    "\n",
    "    return model\n",
    "\n",
    "def predict_cnn(model):\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    for item in list(Label_val):\n",
    "            if item[0] > item[1]:\n",
    "                y_true.append(0)\n",
    "            elif item[0] < item[1]:\n",
    "                y_true.append(1)\n",
    "            else:\n",
    "                y_true.append(0)\n",
    "\n",
    "    for item in list(model.predict(featureSet_val)):\n",
    "            if item[0] > item[1]:\n",
    "                y_pred.append(0)\n",
    "            elif item[0] < item[1]:\n",
    "                y_pred.append(1)\n",
    "            else:\n",
    "                y_pred.append(0)\n",
    "\n",
    "    print('Accuracy: ' + str(accuracy_score(y_true, y_pred)))\n",
    "    print('Precision: ' + str(precision_score(y_true, y_pred)))\n",
    "    print('Recall: ' + str(recall_score(y_true, y_pred)))\n",
    "    print('f1 score: ' + str(f1_score(y_true, y_pred)))\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "\n",
    "    print('true positive ' + str(tp))\n",
    "    print('false positive ' + str(fp))\n",
    "    print('false negative ' + str(fn))\n",
    "    print('true negative ' + str(tn))\n",
    "\n",
    "title = 'H_A_neurons_' + str(n_neurons) + '_filters_' + str(\n",
    "    nbindex) + '_dropout_' + str(dropout) + '_epoch_' + str(n_epoch)\n",
    "\n",
    "final_filepath = prefix + str(num_layers) + \"_Layer(s)//Final_\" + title + \".hdf5\"\n",
    "#model = load_model(final_filepath)\n",
    "model = train_cnn()\n",
    "predict_cnn(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
