{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data: (16632, 48, 272)\n",
      "training label: (16632, 2)\n",
      "evaluation data: (3684, 48, 272)\n",
      "evaluation label: (3684, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0909 10:11:44.686500  9824 deprecation_wrapper.py:119] From C:\\Users\\Ash Gao\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "C:\\Users\\Ash Gao\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:192: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(input_shape=(48, 272), kernel_constraint=<keras.con..., filters=512, kernel_size=3)`\n",
      "W0909 10:11:44.704055  9824 deprecation_wrapper.py:119] From C:\\Users\\Ash Gao\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0909 10:11:44.714860  9824 deprecation_wrapper.py:119] From C:\\Users\\Ash Gao\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0909 10:11:44.810032  9824 deprecation_wrapper.py:119] From C:\\Users\\Ash Gao\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0909 10:11:44.814000  9824 deprecation_wrapper.py:119] From C:\\Users\\Ash Gao\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0909 10:11:44.828077  9824 deprecation.py:506] From C:\\Users\\Ash Gao\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "C:\\Users\\Ash Gao\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:198: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(kernel_constraint=<keras.con..., filters=1024, kernel_size=3)`\n",
      "C:\\Users\\Ash Gao\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:204: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(kernel_constraint=<keras.con..., filters=1536, kernel_size=3)`\n",
      "C:\\Users\\Ash Gao\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:210: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(kernel_constraint=<keras.con..., filters=1024, kernel_size=3)`\n",
      "W0909 10:11:44.994857  9824 deprecation_wrapper.py:119] From C:\\Users\\Ash Gao\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0909 10:11:45.001306  9824 deprecation_wrapper.py:119] From C:\\Users\\Ash Gao\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0909 10:11:45.005770  9824 deprecation.py:323] From C:\\Users\\Ash Gao\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "C:\\Users\\Ash Gao\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:245: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 46, 512)           418304    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 46, 512)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 23, 512)           0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 23, 512)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 21, 1024)          1573888   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 21, 1024)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 10, 1024)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 10, 1024)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 8, 1536)           4720128   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 8, 1536)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 4, 1536)           0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 1536)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 2, 1024)           4719616   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 2, 1024)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1, 1024)           0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1, 1024)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 2050      \n",
      "=================================================================\n",
      "Total params: 11,433,986\n",
      "Trainable params: 11,433,986\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 12474 samples, validate on 4158 samples\n",
      "Epoch 1/50000\n",
      "12474/12474 [==============================] - 222s 18ms/step - loss: 0.6789 - acc: 0.5680 - val_loss: 0.6628 - val_acc: 0.6046\n",
      "Epoch 2/50000\n",
      "12474/12474 [==============================] - 220s 18ms/step - loss: 0.6444 - acc: 0.6324 - val_loss: 0.6295 - val_acc: 0.6498\n",
      "Epoch 3/50000\n",
      "12474/12474 [==============================] - 223s 18ms/step - loss: 0.6234 - acc: 0.6538 - val_loss: 0.6582 - val_acc: 0.6097\n",
      "Epoch 4/50000\n",
      "12474/12474 [==============================] - 221s 18ms/step - loss: 0.6004 - acc: 0.6770 - val_loss: 0.5972 - val_acc: 0.6780\n",
      "Epoch 5/50000\n",
      "12474/12474 [==============================] - 220s 18ms/step - loss: 0.5755 - acc: 0.7040 - val_loss: 0.5680 - val_acc: 0.6987\n",
      "Epoch 6/50000\n",
      "12474/12474 [==============================] - 219s 18ms/step - loss: 0.5572 - acc: 0.7157 - val_loss: 0.5809 - val_acc: 0.6924\n",
      "Epoch 7/50000\n",
      "12474/12474 [==============================] - 220s 18ms/step - loss: 0.5320 - acc: 0.7314 - val_loss: 0.5277 - val_acc: 0.7290\n",
      "Epoch 8/50000\n",
      "12474/12474 [==============================] - 220s 18ms/step - loss: 0.5241 - acc: 0.7409 - val_loss: 0.5794 - val_acc: 0.6914\n",
      "Epoch 9/50000\n",
      "12474/12474 [==============================] - 221s 18ms/step - loss: 0.4922 - acc: 0.7617 - val_loss: 0.4950 - val_acc: 0.7583\n",
      "Epoch 10/50000\n",
      "12474/12474 [==============================] - 220s 18ms/step - loss: 0.4732 - acc: 0.7740 - val_loss: 0.4772 - val_acc: 0.7708\n",
      "Epoch 11/50000\n",
      "12474/12474 [==============================] - 220s 18ms/step - loss: 0.4322 - acc: 0.7971 - val_loss: 0.4835 - val_acc: 0.7677\n",
      "Epoch 12/50000\n",
      "12474/12474 [==============================] - 219s 18ms/step - loss: 0.4192 - acc: 0.8047 - val_loss: 0.4481 - val_acc: 0.7862\n",
      "Epoch 13/50000\n",
      "12474/12474 [==============================] - 220s 18ms/step - loss: 0.3956 - acc: 0.8188 - val_loss: 0.4568 - val_acc: 0.7814\n",
      "Epoch 14/50000\n",
      "12474/12474 [==============================] - 219s 18ms/step - loss: 0.3722 - acc: 0.8353 - val_loss: 0.4248 - val_acc: 0.7982\n",
      "Epoch 15/50000\n",
      "12474/12474 [==============================] - 219s 18ms/step - loss: 0.3420 - acc: 0.8530 - val_loss: 0.4171 - val_acc: 0.8086\n",
      "Epoch 16/50000\n",
      "12474/12474 [==============================] - 221s 18ms/step - loss: 0.3217 - acc: 0.8634 - val_loss: 0.4083 - val_acc: 0.8136\n",
      "Epoch 17/50000\n",
      "12474/12474 [==============================] - 221s 18ms/step - loss: 0.3098 - acc: 0.8680 - val_loss: 0.3925 - val_acc: 0.8215\n",
      "Epoch 18/50000\n",
      "12474/12474 [==============================] - 219s 18ms/step - loss: 0.2708 - acc: 0.8894 - val_loss: 0.3952 - val_acc: 0.8213\n",
      "Epoch 19/50000\n",
      "12474/12474 [==============================] - 221s 18ms/step - loss: 0.2561 - acc: 0.8974 - val_loss: 0.3878 - val_acc: 0.8276\n",
      "Epoch 20/50000\n",
      "12474/12474 [==============================] - 220s 18ms/step - loss: 0.2480 - acc: 0.8971 - val_loss: 0.3764 - val_acc: 0.8384\n",
      "Epoch 21/50000\n",
      "12474/12474 [==============================] - 219s 18ms/step - loss: 0.2320 - acc: 0.9047 - val_loss: 0.4184 - val_acc: 0.8177\n",
      "Epoch 22/50000\n",
      "12474/12474 [==============================] - 218s 17ms/step - loss: 0.2041 - acc: 0.9197 - val_loss: 0.3752 - val_acc: 0.8350\n",
      "Epoch 23/50000\n",
      "12474/12474 [==============================] - 218s 17ms/step - loss: 0.1942 - acc: 0.9233 - val_loss: 0.3819 - val_acc: 0.8365\n",
      "Epoch 24/50000\n",
      "12474/12474 [==============================] - 219s 18ms/step - loss: 0.1774 - acc: 0.9315 - val_loss: 0.3601 - val_acc: 0.8521\n",
      "Epoch 25/50000\n",
      "12474/12474 [==============================] - 219s 18ms/step - loss: 0.1662 - acc: 0.9357 - val_loss: 0.3572 - val_acc: 0.8545\n",
      "Epoch 26/50000\n",
      "12474/12474 [==============================] - 218s 17ms/step - loss: 0.1560 - acc: 0.9398 - val_loss: 0.3585 - val_acc: 0.8482\n",
      "Epoch 27/50000\n",
      "12474/12474 [==============================] - 218s 17ms/step - loss: 0.1390 - acc: 0.9481 - val_loss: 0.3498 - val_acc: 0.8617\n",
      "Epoch 28/50000\n",
      "12474/12474 [==============================] - 218s 18ms/step - loss: 0.1308 - acc: 0.9516 - val_loss: 0.3686 - val_acc: 0.8535\n",
      "Epoch 29/50000\n",
      "12474/12474 [==============================] - 219s 18ms/step - loss: 0.1226 - acc: 0.9532 - val_loss: 0.3613 - val_acc: 0.8595\n",
      "Epoch 30/50000\n",
      "12474/12474 [==============================] - 220s 18ms/step - loss: 0.1128 - acc: 0.9579 - val_loss: 0.4091 - val_acc: 0.8360\n",
      "Epoch 31/50000\n",
      "12474/12474 [==============================] - 218s 17ms/step - loss: 0.1104 - acc: 0.9590 - val_loss: 0.3523 - val_acc: 0.8651\n",
      "Epoch 32/50000\n",
      "12474/12474 [==============================] - 221s 18ms/step - loss: 0.0917 - acc: 0.9698 - val_loss: 0.3507 - val_acc: 0.8658\n",
      "Epoch 33/50000\n",
      "12474/12474 [==============================] - 221s 18ms/step - loss: 0.0933 - acc: 0.9644 - val_loss: 0.3555 - val_acc: 0.8723\n",
      "Epoch 34/50000\n",
      "12474/12474 [==============================] - 221s 18ms/step - loss: 0.0848 - acc: 0.9716 - val_loss: 0.3746 - val_acc: 0.8622\n",
      "Epoch 35/50000\n",
      "12474/12474 [==============================] - 220s 18ms/step - loss: 0.0897 - acc: 0.9662 - val_loss: 0.3596 - val_acc: 0.8665\n",
      "Epoch 36/50000\n",
      "12474/12474 [==============================] - 219s 18ms/step - loss: 0.0798 - acc: 0.9723 - val_loss: 0.3598 - val_acc: 0.8725\n",
      "Epoch 37/50000\n",
      "12474/12474 [==============================] - 220s 18ms/step - loss: 0.0690 - acc: 0.9764 - val_loss: 0.3529 - val_acc: 0.8718\n",
      "Epoch 38/50000\n",
      "12474/12474 [==============================] - 222s 18ms/step - loss: 0.0581 - acc: 0.9821 - val_loss: 0.3780 - val_acc: 0.8694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/50000\n",
      "12474/12474 [==============================] - 220s 18ms/step - loss: 0.0669 - acc: 0.9759 - val_loss: 0.3875 - val_acc: 0.8665\n",
      "Epoch 40/50000\n",
      "12474/12474 [==============================] - 217s 17ms/step - loss: 0.0526 - acc: 0.9836 - val_loss: 0.3649 - val_acc: 0.8749\n",
      "Epoch 41/50000\n",
      "12474/12474 [==============================] - 218s 17ms/step - loss: 0.0550 - acc: 0.9811 - val_loss: 0.3674 - val_acc: 0.8740\n",
      "Epoch 42/50000\n",
      "12474/12474 [==============================] - 220s 18ms/step - loss: 0.0453 - acc: 0.9861 - val_loss: 0.3856 - val_acc: 0.8725\n",
      "Epoch 43/50000\n",
      "12474/12474 [==============================] - 218s 18ms/step - loss: 0.0436 - acc: 0.9873 - val_loss: 0.3710 - val_acc: 0.8759\n",
      "Epoch 44/50000\n",
      "12474/12474 [==============================] - 218s 17ms/step - loss: 0.0400 - acc: 0.9877 - val_loss: 0.3944 - val_acc: 0.8728\n",
      "Epoch 45/50000\n",
      "12474/12474 [==============================] - 220s 18ms/step - loss: 0.0417 - acc: 0.9867 - val_loss: 0.3989 - val_acc: 0.8737\n",
      "Epoch 46/50000\n",
      "12474/12474 [==============================] - 219s 18ms/step - loss: 0.0395 - acc: 0.9873 - val_loss: 0.3747 - val_acc: 0.8802\n",
      "Epoch 47/50000\n",
      "12474/12474 [==============================] - 219s 18ms/step - loss: 0.0325 - acc: 0.9905 - val_loss: 0.3893 - val_acc: 0.8771\n",
      "Epoch 48/50000\n",
      "12474/12474 [==============================] - 221s 18ms/step - loss: 0.0324 - acc: 0.9903 - val_loss: 0.3978 - val_acc: 0.8747\n",
      "Epoch 49/50000\n",
      "12474/12474 [==============================] - 220s 18ms/step - loss: 0.0332 - acc: 0.9896 - val_loss: 0.4063 - val_acc: 0.8694\n",
      "Epoch 50/50000\n",
      "12474/12474 [==============================] - 219s 18ms/step - loss: 0.0338 - acc: 0.9896 - val_loss: 0.3933 - val_acc: 0.8776\n",
      "Epoch 51/50000\n",
      "12474/12474 [==============================] - 218s 17ms/step - loss: 0.0292 - acc: 0.9911 - val_loss: 0.3715 - val_acc: 0.8797\n",
      "Epoch 52/50000\n",
      "12474/12474 [==============================] - 218s 17ms/step - loss: 0.0258 - acc: 0.9927 - val_loss: 0.4142 - val_acc: 0.8800\n",
      "Epoch 53/50000\n",
      "12474/12474 [==============================] - 220s 18ms/step - loss: 0.0269 - acc: 0.9919 - val_loss: 0.3978 - val_acc: 0.8807\n",
      "Epoch 54/50000\n",
      "12474/12474 [==============================] - 223s 18ms/step - loss: 0.0286 - acc: 0.9907 - val_loss: 0.3851 - val_acc: 0.8802\n",
      "Epoch 55/50000\n",
      "12474/12474 [==============================] - 220s 18ms/step - loss: 0.0290 - acc: 0.9905 - val_loss: 0.4792 - val_acc: 0.8636\n",
      "Epoch 56/50000\n",
      "12474/12474 [==============================] - 221s 18ms/step - loss: 0.0239 - acc: 0.9926 - val_loss: 0.4264 - val_acc: 0.8766\n",
      "Epoch 57/50000\n",
      "12474/12474 [==============================] - 219s 18ms/step - loss: 0.0267 - acc: 0.9918 - val_loss: 0.3881 - val_acc: 0.8817\n",
      "Epoch 58/50000\n",
      "12474/12474 [==============================] - 218s 18ms/step - loss: 0.0258 - acc: 0.9918 - val_loss: 0.3985 - val_acc: 0.8834\n",
      "Epoch 59/50000\n",
      "12474/12474 [==============================] - 218s 17ms/step - loss: 0.0225 - acc: 0.9931 - val_loss: 0.3839 - val_acc: 0.8834\n",
      "Epoch 60/50000\n",
      "12474/12474 [==============================] - 219s 18ms/step - loss: 0.0182 - acc: 0.9947 - val_loss: 0.3884 - val_acc: 0.8843\n",
      "Epoch 61/50000\n",
      "12474/12474 [==============================] - 220s 18ms/step - loss: 0.0192 - acc: 0.9945 - val_loss: 0.3890 - val_acc: 0.8838\n",
      "Epoch 62/50000\n",
      "12474/12474 [==============================] - 219s 18ms/step - loss: 0.0197 - acc: 0.9945 - val_loss: 0.4089 - val_acc: 0.8836\n",
      "Epoch 63/50000\n",
      "12474/12474 [==============================] - 219s 18ms/step - loss: 0.0185 - acc: 0.9948 - val_loss: 0.3939 - val_acc: 0.8834\n",
      "Epoch 64/50000\n",
      "12474/12474 [==============================] - 220s 18ms/step - loss: 0.0197 - acc: 0.9937 - val_loss: 0.4331 - val_acc: 0.8776\n",
      "Epoch 65/50000\n",
      "12474/12474 [==============================] - 222s 18ms/step - loss: 0.0178 - acc: 0.9953 - val_loss: 0.3893 - val_acc: 0.8853\n",
      "Epoch 66/50000\n",
      "12474/12474 [==============================] - 218s 17ms/step - loss: 0.0173 - acc: 0.9944 - val_loss: 0.4109 - val_acc: 0.8788\n",
      "Epoch 67/50000\n",
      "12474/12474 [==============================] - 217s 17ms/step - loss: 0.0164 - acc: 0.9949 - val_loss: 0.4194 - val_acc: 0.8812\n",
      "Epoch 68/50000\n",
      "12474/12474 [==============================] - 223s 18ms/step - loss: 0.0180 - acc: 0.9941 - val_loss: 0.4468 - val_acc: 0.8788\n",
      "Epoch 69/50000\n",
      "12474/12474 [==============================] - 220s 18ms/step - loss: 0.0173 - acc: 0.9943 - val_loss: 0.4099 - val_acc: 0.8824\n",
      "Epoch 70/50000\n",
      "12474/12474 [==============================] - 219s 18ms/step - loss: 0.0185 - acc: 0.9945 - val_loss: 0.4088 - val_acc: 0.8841\n",
      "Epoch 71/50000\n",
      "12474/12474 [==============================] - 218s 17ms/step - loss: 0.0141 - acc: 0.9962 - val_loss: 0.3915 - val_acc: 0.8855\n",
      "Epoch 72/50000\n",
      "12474/12474 [==============================] - 218s 17ms/step - loss: 0.0140 - acc: 0.9962 - val_loss: 0.4029 - val_acc: 0.8886\n",
      "Epoch 73/50000\n",
      "12474/12474 [==============================] - 219s 18ms/step - loss: 0.0127 - acc: 0.9972 - val_loss: 0.4145 - val_acc: 0.8872\n",
      "Epoch 74/50000\n",
      "12474/12474 [==============================] - 221s 18ms/step - loss: 0.0128 - acc: 0.9966 - val_loss: 0.4167 - val_acc: 0.8850\n",
      "Epoch 75/50000\n",
      "12474/12474 [==============================] - 217s 17ms/step - loss: 0.0127 - acc: 0.9961 - val_loss: 0.4097 - val_acc: 0.8889\n",
      "Epoch 76/50000\n",
      "12474/12474 [==============================] - 219s 18ms/step - loss: 0.0150 - acc: 0.9951 - val_loss: 0.4040 - val_acc: 0.8862\n",
      "Epoch 77/50000\n",
      "12474/12474 [==============================] - 220s 18ms/step - loss: 0.0106 - acc: 0.9969 - val_loss: 0.4074 - val_acc: 0.8855\n",
      "Epoch 78/50000\n",
      "12474/12474 [==============================] - 218s 17ms/step - loss: 0.0119 - acc: 0.9966 - val_loss: 0.4171 - val_acc: 0.8877\n",
      "Epoch 79/50000\n",
      "12474/12474 [==============================] - 220s 18ms/step - loss: 0.0108 - acc: 0.9970 - val_loss: 0.4233 - val_acc: 0.8862\n",
      "Epoch 80/50000\n",
      "12474/12474 [==============================] - 220s 18ms/step - loss: 0.0099 - acc: 0.9973 - val_loss: 0.4140 - val_acc: 0.8860\n",
      "Epoch 81/50000\n",
      "12474/12474 [==============================] - 219s 18ms/step - loss: 0.0137 - acc: 0.9958 - val_loss: 0.4192 - val_acc: 0.8894\n",
      "Epoch 82/50000\n",
      "12474/12474 [==============================] - 219s 18ms/step - loss: 0.0118 - acc: 0.9958 - val_loss: 0.4067 - val_acc: 0.8891\n",
      "Epoch 83/50000\n",
      "12474/12474 [==============================] - 220s 18ms/step - loss: 0.0101 - acc: 0.9974 - val_loss: 0.4092 - val_acc: 0.8891\n",
      "Epoch 84/50000\n",
      "12474/12474 [==============================] - 219s 18ms/step - loss: 0.0096 - acc: 0.9975 - val_loss: 0.4069 - val_acc: 0.8886\n",
      "Epoch 85/50000\n",
      "12474/12474 [==============================] - 218s 17ms/step - loss: 0.0116 - acc: 0.9962 - val_loss: 0.4014 - val_acc: 0.8889\n",
      "Epoch 86/50000\n",
      "12474/12474 [==============================] - 219s 18ms/step - loss: 0.0090 - acc: 0.9975 - val_loss: 0.4400 - val_acc: 0.8860\n",
      "Epoch 87/50000\n",
      "12474/12474 [==============================] - 219s 18ms/step - loss: 0.0113 - acc: 0.9966 - val_loss: 0.4220 - val_acc: 0.8886\n",
      "Epoch 88/50000\n",
      "12474/12474 [==============================] - 218s 17ms/step - loss: 0.0093 - acc: 0.9978 - val_loss: 0.4215 - val_acc: 0.8884\n",
      "Epoch 89/50000\n",
      "12474/12474 [==============================] - 216s 17ms/step - loss: 0.0076 - acc: 0.9983 - val_loss: 0.4619 - val_acc: 0.8819\n",
      "Epoch 90/50000\n",
      "12474/12474 [==============================] - 217s 17ms/step - loss: 0.0085 - acc: 0.9977 - val_loss: 0.4173 - val_acc: 0.8913\n",
      "Epoch 91/50000\n",
      "12474/12474 [==============================] - 219s 18ms/step - loss: 0.0119 - acc: 0.9960 - val_loss: 0.4149 - val_acc: 0.8886\n",
      "Epoch 92/50000\n",
      "12474/12474 [==============================] - 220s 18ms/step - loss: 0.0090 - acc: 0.9973 - val_loss: 0.4128 - val_acc: 0.8913\n",
      "Epoch 93/50000\n",
      "12474/12474 [==============================] - 216s 17ms/step - loss: 0.0088 - acc: 0.9976 - val_loss: 0.4619 - val_acc: 0.8829\n",
      "Epoch 94/50000\n",
      "12474/12474 [==============================] - 216s 17ms/step - loss: 0.0081 - acc: 0.9981 - val_loss: 0.4147 - val_acc: 0.8930\n",
      "Epoch 95/50000\n",
      "12474/12474 [==============================] - 220s 18ms/step - loss: 0.0089 - acc: 0.9973 - val_loss: 0.4458 - val_acc: 0.8848\n",
      "Epoch 96/50000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12474/12474 [==============================] - 218s 17ms/step - loss: 0.0112 - acc: 0.9969 - val_loss: 0.4216 - val_acc: 0.8915\n",
      "Epoch 97/50000\n",
      "12474/12474 [==============================] - 216s 17ms/step - loss: 0.0101 - acc: 0.9970 - val_loss: 0.4154 - val_acc: 0.8913\n",
      "Epoch 98/50000\n",
      "12474/12474 [==============================] - 219s 18ms/step - loss: 0.0067 - acc: 0.9982 - val_loss: 0.4136 - val_acc: 0.8927\n",
      "Epoch 99/50000\n",
      "12474/12474 [==============================] - 218s 17ms/step - loss: 0.0078 - acc: 0.9975 - val_loss: 0.4355 - val_acc: 0.8860\n",
      "Epoch 100/50000\n",
      "12474/12474 [==============================] - 217s 17ms/step - loss: 0.0068 - acc: 0.9982 - val_loss: 0.4201 - val_acc: 0.8896\n",
      "Epoch 101/50000\n",
      "12474/12474 [==============================] - 219s 18ms/step - loss: 0.0095 - acc: 0.9972 - val_loss: 0.4213 - val_acc: 0.8915\n",
      "Epoch 102/50000\n",
      "12474/12474 [==============================] - 218s 17ms/step - loss: 0.0078 - acc: 0.9978 - val_loss: 0.4229 - val_acc: 0.8901\n",
      "Epoch 103/50000\n",
      "12474/12474 [==============================] - 220s 18ms/step - loss: 0.0075 - acc: 0.9978 - val_loss: 0.4616 - val_acc: 0.8862\n",
      "Epoch 104/50000\n",
      "12474/12474 [==============================] - 219s 18ms/step - loss: 0.0068 - acc: 0.9984 - val_loss: 0.4168 - val_acc: 0.8920\n",
      "Epoch 105/50000\n",
      "12474/12474 [==============================] - 218s 17ms/step - loss: 0.0069 - acc: 0.9983 - val_loss: 0.4253 - val_acc: 0.8886\n",
      "Epoch 106/50000\n",
      "12474/12474 [==============================] - 220s 18ms/step - loss: 0.0069 - acc: 0.9981 - val_loss: 0.4117 - val_acc: 0.8906\n",
      "Epoch 107/50000\n",
      "12474/12474 [==============================] - 217s 17ms/step - loss: 0.0081 - acc: 0.9972 - val_loss: 0.4163 - val_acc: 0.8937\n",
      "Epoch 108/50000\n",
      "12474/12474 [==============================] - 218s 17ms/step - loss: 0.0100 - acc: 0.9970 - val_loss: 0.4902 - val_acc: 0.8790\n",
      "Epoch 109/50000\n",
      "12474/12474 [==============================] - 217s 17ms/step - loss: 0.0067 - acc: 0.9982 - val_loss: 0.4192 - val_acc: 0.8927\n",
      "Epoch 110/50000\n",
      "12474/12474 [==============================] - 218s 17ms/step - loss: 0.0077 - acc: 0.9978 - val_loss: 0.4327 - val_acc: 0.8923\n",
      "Epoch 111/50000\n",
      "12474/12474 [==============================] - 216s 17ms/step - loss: 0.0063 - acc: 0.9984 - val_loss: 0.4194 - val_acc: 0.8927\n",
      "Epoch 112/50000\n",
      "12474/12474 [==============================] - 217s 17ms/step - loss: 0.0072 - acc: 0.9977 - val_loss: 0.4264 - val_acc: 0.8930\n",
      "Epoch 113/50000\n",
      "12474/12474 [==============================] - 217s 17ms/step - loss: 0.0063 - acc: 0.9985 - val_loss: 0.4258 - val_acc: 0.8937\n",
      "Epoch 114/50000\n",
      "12474/12474 [==============================] - 217s 17ms/step - loss: 0.0059 - acc: 0.9989 - val_loss: 0.4262 - val_acc: 0.8920\n",
      "Epoch 115/50000\n",
      "12474/12474 [==============================] - 217s 17ms/step - loss: 0.0064 - acc: 0.9978 - val_loss: 0.4171 - val_acc: 0.8906\n",
      "Epoch 116/50000\n",
      "12474/12474 [==============================] - 218s 18ms/step - loss: 0.0069 - acc: 0.9985 - val_loss: 0.4206 - val_acc: 0.8911\n",
      "Epoch 117/50000\n",
      "12474/12474 [==============================] - 217s 17ms/step - loss: 0.0057 - acc: 0.9981 - val_loss: 0.4281 - val_acc: 0.8901\n",
      "Epoch 118/50000\n",
      "12474/12474 [==============================] - 217s 17ms/step - loss: 0.0069 - acc: 0.9982 - val_loss: 0.4485 - val_acc: 0.8894\n",
      "Epoch 119/50000\n",
      "12474/12474 [==============================] - 218s 17ms/step - loss: 0.0055 - acc: 0.9985 - val_loss: 0.4407 - val_acc: 0.8886\n",
      "Epoch 120/50000\n",
      "12474/12474 [==============================] - 218s 18ms/step - loss: 0.0060 - acc: 0.9984 - val_loss: 0.4643 - val_acc: 0.8879\n",
      "Epoch 121/50000\n",
      "12474/12474 [==============================] - 218s 18ms/step - loss: 0.0066 - acc: 0.9982 - val_loss: 0.4536 - val_acc: 0.8867\n",
      "Epoch 122/50000\n",
      "12474/12474 [==============================] - 217s 17ms/step - loss: 0.0077 - acc: 0.9970 - val_loss: 0.4294 - val_acc: 0.8918\n",
      "Epoch 123/50000\n",
      "12474/12474 [==============================] - 217s 17ms/step - loss: 0.0049 - acc: 0.9989 - val_loss: 0.4352 - val_acc: 0.8913\n",
      "Epoch 124/50000\n",
      "12474/12474 [==============================] - 218s 17ms/step - loss: 0.0053 - acc: 0.9993 - val_loss: 0.4223 - val_acc: 0.8915\n",
      "Epoch 125/50000\n",
      "12474/12474 [==============================] - 219s 18ms/step - loss: 0.0047 - acc: 0.9990 - val_loss: 0.4654 - val_acc: 0.8879\n",
      "Epoch 126/50000\n",
      "12474/12474 [==============================] - 218s 17ms/step - loss: 0.0058 - acc: 0.9986 - val_loss: 0.5098 - val_acc: 0.8797\n",
      "Epoch 127/50000\n",
      "12474/12474 [==============================] - 219s 18ms/step - loss: 0.0056 - acc: 0.9984 - val_loss: 0.4542 - val_acc: 0.8899\n",
      "Accuracy: 0.74728555917481\n",
      "Precision: 0.7582417582417582\n",
      "Recall: 0.71875\n",
      "f1 score: 0.7379679144385026\n",
      "true positive 1311\n",
      "false positive 418\n",
      "false negative 513\n",
      "true negative 1442\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import gc\n",
    "import sys\n",
    "import h5py\n",
    "import time\n",
    "import datetime\n",
    "import pickle\n",
    "import librosa\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import pandas as pd\n",
    "from pandas.plotting import parallel_coordinates\n",
    "from pydub import AudioSegment\n",
    "\n",
    "#imported for testing\n",
    "import wave\n",
    "import contextlib\n",
    "\n",
    "# for outputing file\n",
    "from scipy.cluster.vq import vq, kmeans, whiten\n",
    "import scipy.stats.stats as st\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.decomposition import PCA as sklearnPCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import auc, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn import mixture\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "import keras\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.constraints import maxnorm\n",
    "from keras.layers import Convolution1D, Dense, MaxPooling1D, Flatten, Add, Dropout, Input, Activation\n",
    "from keras.layers import TimeDistributed, Bidirectional, LSTM, LeakyReLU\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers, regularizers\n",
    "from keras.utils import np_utils, to_categorical\n",
    "\n",
    "from colorama import Fore, Back, Style\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "#warnings.filterwarnings('ignore')\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# confirm TensorFlow sees the GPU\n",
    "from tensorflow.python.client import device_lib\n",
    "# assert 'GPU' in str(device_lib.list_local_devices())\n",
    "\n",
    "# confirm Keras sees the GPU\n",
    "from keras import backend\n",
    "# print(len(backend.tensorflow_backend._get_available_gpus()) > 0)\n",
    "\n",
    "#warnings.filterwarnings('ignore')\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "sample_rate = 44100\n",
    "frame_number = 48\n",
    "hop_length = 441  # frame size= 2 * hop\n",
    "segment_length = int(sample_rate * 0.2)  # 0.2\n",
    "segment_pad = int(sample_rate * 0.02)     # 0.02\n",
    "overlapping = int(sample_rate * 0.1)   # 0.1\n",
    "\n",
    "classes = 2\n",
    "NumofFeaturetoUse = 272\n",
    "n_neurons = 4096\n",
    "dense_layers = 1\n",
    "num_layers = 4\n",
    "fillength = 3\n",
    "nbindex = 512\n",
    "dropout = 0.2\n",
    "n_batch = 128\n",
    "n_epoch = 50000\n",
    "\n",
    "def update_progress(progress):\n",
    "    bar_length = 100\n",
    "    if isinstance(progress, int):\n",
    "        progress = float(progress)\n",
    "    if not isinstance(progress, float):\n",
    "        progress = 0\n",
    "    if progress < 0:\n",
    "        progress = 0\n",
    "    if progress >= 1:\n",
    "        progress = 1\n",
    "\n",
    "    block = int(round(bar_length * progress))\n",
    "    clear_output(wait = True)\n",
    "    \n",
    "    text = \"Progress: [{0}] {1:.1f}%\".format( \"#\" * block + \"-\" * (bar_length - block), progress * 100)\n",
    "    print(text)\n",
    "\n",
    "prefix = '..//'\n",
    "h_feature_vector = np.load(prefix + 'Features//h_feature_vector_48.npy')\n",
    "h_label_vector = np.load(prefix + 'Features//h_label_vector_48.npy')\n",
    "a_feature_vector = np.load(prefix + 'Features//a_feature_vector_48.npy')\n",
    "a_label_vector = np.load(prefix + 'Features//a_label_vector_48.npy')\n",
    "n_feature_vector = np.load(prefix + 'Features//n_feature_vector_48.npy')\n",
    "n_label_vector = np.load(prefix + 'Features//n_label_vector_48.npy')\n",
    "s_feature_vector = np.load(prefix + 'Features//s_feature_vector_48.npy')\n",
    "s_label_vector = np.load(prefix + 'Features//s_label_vector_48.npy')\n",
    "\n",
    "h_feature_vector_test = np.load(prefix + 'Features//h_feature_vector_test_48.npy')\n",
    "h_label_vector_test = np.load(prefix + 'Features//h_label_vector_test_48.npy')\n",
    "a_feature_vector_test = np.load(prefix + 'Features//a_feature_vector_test_48.npy')\n",
    "a_label_vector_test = np.load(prefix + 'Features//a_label_vector_test_48.npy')\n",
    "n_feature_vector_test = np.load(prefix + 'Features//n_feature_vector_test_48.npy')\n",
    "n_label_vector_test = np.load(prefix + 'Features//n_label_vector_test_48.npy')\n",
    "s_feature_vector_test = np.load(prefix + 'Features//s_feature_vector_test_48.npy')\n",
    "s_label_vector_test = np.load(prefix + 'Features//s_label_vector_test_48.npy')\n",
    "\n",
    "h_label_vector[h_label_vector == 0] = 0\n",
    "a_label_vector[a_label_vector == 1] = 1\n",
    "h_label_vector_test[h_label_vector_test == 0] = 0\n",
    "a_label_vector_test[a_label_vector_test == 1] = 1\n",
    "\n",
    "h_label_vector = to_categorical(h_label_vector, num_classes = 2)\n",
    "a_label_vector = to_categorical(a_label_vector, num_classes = 2)\n",
    "h_label_vector_test = to_categorical(h_label_vector_test, num_classes = 2)\n",
    "a_label_vector_test = to_categorical(a_label_vector_test, num_classes = 2)\n",
    "\n",
    "# Load training npy files\n",
    "featureSet_training = np.vstack((h_feature_vector, a_feature_vector))\n",
    "label_training = np.vstack((h_label_vector, a_label_vector))\n",
    "\n",
    "# Load testing npy files\n",
    "featureSet_testing = np.vstack((h_feature_vector_test, a_feature_vector_test))\n",
    "label_testing = np.vstack((h_label_vector_test, a_label_vector_test))\n",
    "\n",
    "def float_compatible(input_np):\n",
    "\n",
    "    x = np.where(input_np >= np.finfo(np.float32).max)\n",
    "    for index in range(0, len(x[0])):\n",
    "        x_position = x[0][index]\n",
    "        y_position = x[1][index]      \n",
    "        input_np[x_position, y_position] = 0.0\n",
    "    input_np = np.nan_to_num(input_np)\n",
    "        \n",
    "    return input_np\n",
    "\n",
    "train_data = float_compatible((featureSet_training).astype(np.float32))\n",
    "eval_data = float_compatible((featureSet_testing).astype(np.float32))\n",
    "\n",
    "adam = optimizers.Adam(lr = 3e-5, beta_1 = 0.9, beta_2 = 0.999, epsilon = None, decay = 0, amsgrad = True)\n",
    "sgd = optimizers.SGD(lr = 0.01, decay = 1e-6, momentum = 0.9, nesterov = True)\n",
    "rmsprop = optimizers.RMSprop(lr = 0.0001, rho = 0.9, epsilon = None, decay = 0.0)\n",
    "adagrad = optimizers.Adagrad(lr = 0.01, epsilon = None, decay = 0.0)\n",
    "adadelta = optimizers.Adadelta(lr = 1.0, rho = 0.95, epsilon = None, decay = 0.0)\n",
    "adamax = optimizers.Adamax(lr = 0.002, beta_1 = 0.9, beta_2 = 0.999, epsilon = None, decay = 0.0)\n",
    "nadam = optimizers.Nadam(lr = 0.002, beta_1 = 0.9, beta_2 = 0.999, epsilon = None, schedule_decay = 0.004)\n",
    "\n",
    "featureSet = train_data\n",
    "Label = label_training\n",
    "featureSet = np.split(featureSet, np.array([NumofFeaturetoUse]), axis = 2)[0]\n",
    "\n",
    "print('training data: ' + str(featureSet.shape))\n",
    "print('training label: ' + str(Label.shape))\n",
    "\n",
    "featureSet_val = eval_data\n",
    "Label_val = label_testing\n",
    "featureSet_val = np.split(featureSet_val, np.array([NumofFeaturetoUse]), axis = 2)[0]\n",
    "\n",
    "print('evaluation data: ' + str(featureSet_val.shape))\n",
    "print('evaluation label: ' + str(Label_val.shape))\n",
    "\n",
    "def record(str_message, log_file):\n",
    "    str_message = str_message + '\\n'\n",
    "    file = open(log_file, 'a')\n",
    "    file.write(str_message)\n",
    "    file.close()\n",
    "\n",
    "def create_cnn(title, num_layers, n_neurons, n_batch, nbindex, dropout, classes, dense_layers):\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Convolution1D(nb_filter=nbindex, filter_length=fillength,\n",
    "                            input_shape=(featureSet.shape[1], featureSet.shape[2]), kernel_constraint=maxnorm(3)))\n",
    "    model.add(LeakyReLU(alpha=0.05))\n",
    "    model.add(MaxPooling1D(pool_size=2, strides=2, padding='valid'))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(Convolution1D(nb_filter=nbindex*2, filter_length=fillength,\n",
    "                            kernel_constraint=maxnorm(3)))\n",
    "    model.add(LeakyReLU(alpha=0.05))\n",
    "    model.add(MaxPooling1D(pool_size=2, strides=2, padding='valid'))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(Convolution1D(nb_filter=nbindex*3, filter_length=fillength,\n",
    "                            kernel_constraint=maxnorm(3)))\n",
    "    model.add(LeakyReLU(alpha=0.05))\n",
    "    model.add(MaxPooling1D(pool_size=2, strides=2, padding='valid'))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(Convolution1D(nb_filter=nbindex*2, filter_length=fillength,\n",
    "                            kernel_constraint=maxnorm(3)))  \n",
    "    model.add(LeakyReLU(alpha=0.05))\n",
    "    model.add(MaxPooling1D(pool_size=2, strides=2, padding='valid'))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(classes, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_cnn():\n",
    "    \n",
    "    save_to_path = prefix + str(num_layers) + \"_Layer(s)//\"\n",
    "\n",
    "    checkpoint_filepath = prefix + str(num_layers) + \"_Layer(s)//Checkpoint_\" + title + \".hdf5\"\n",
    "    final_filepath = prefix + str(num_layers) + \"_Layer(s)//Final_\" + title + \".hdf5\"\n",
    "\n",
    "    if not os.path.exists(save_to_path):\n",
    "        os.mkdir(save_to_path)\n",
    "\n",
    "    X, X_test, Y, Y_test= train_test_split(featureSet, Label, test_size = 0.25, shuffle = True)\n",
    "\n",
    "    model = create_cnn(title, num_layers, n_neurons, n_batch, nbindex, dropout, classes, dense_layers)\n",
    "\n",
    "    checkpoint = ModelCheckpoint(checkpoint_filepath, monitor = 'val_acc', verbose = 0, save_best_only = True, mode = 'auto')\n",
    "\n",
    "    early_stopping_monitor = EarlyStopping(patience = 100)\n",
    "\n",
    "    callbacks_list = [checkpoint, early_stopping_monitor]\n",
    "\n",
    "    model.fit(X, Y, nb_epoch = n_epoch, batch_size = n_batch,  callbacks = callbacks_list, validation_data = (X_test, Y_test), verbose = 1)\n",
    "\n",
    "    model.save_weights(final_filepath)\n",
    "\n",
    "    model.load_weights(checkpoint_filepath)\n",
    "\n",
    "    return model\n",
    "\n",
    "def predict_cnn(model):\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    for item in list(Label_val):\n",
    "            if item[0] > item[1]:\n",
    "                y_true.append(0)\n",
    "            elif item[0] < item[1]:\n",
    "                y_true.append(1)\n",
    "            else:\n",
    "                y_true.append(0)\n",
    "\n",
    "    for item in list(model.predict(featureSet_val)):\n",
    "            if item[0] > item[1]:\n",
    "                y_pred.append(0)\n",
    "            elif item[0] < item[1]:\n",
    "                y_pred.append(1)\n",
    "            else:\n",
    "                y_pred.append(0)\n",
    "\n",
    "    print('Accuracy: ' + str(accuracy_score(y_true, y_pred)))\n",
    "    print('Precision: ' + str(precision_score(y_true, y_pred)))\n",
    "    print('Recall: ' + str(recall_score(y_true, y_pred)))\n",
    "    print('f1 score: ' + str(f1_score(y_true, y_pred)))\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "\n",
    "    print('true positive ' + str(tp))\n",
    "    print('false positive ' + str(fp))\n",
    "    print('false negative ' + str(fn))\n",
    "    print('true negative ' + str(tn))\n",
    "\n",
    "title = 'H_A_neurons_' + str(n_neurons) + '_filters_' + str(\n",
    "    nbindex) + '_dropout_' + str(dropout) + '_epoch_' + str(n_epoch)\n",
    "\n",
    "final_filepath = prefix + str(num_layers) + \"_Layer(s)//Final_\" + title + \".hdf5\"\n",
    "#model = load_model(final_filepath)\n",
    "model = train_cnn()\n",
    "predict_cnn(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
