{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features are unspecified. Defaut is set to = 272.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import pandas as pd\n",
    "from pydub import AudioSegment\n",
    "import os, shutil, glob\n",
    "import gc\n",
    "import sys\n",
    "import h5py\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#imported for testing\n",
    "import wave\n",
    "import contextlib\n",
    "\n",
    "# for outputing file\n",
    "import scipy.stats.stats as st\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from scipy.cluster.vq import vq, kmeans, whiten\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import mixture\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "import pickle\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA as sklearnPCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.metrics import auc, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "#from pandas.plotting import parallel_coordinates\n",
    "import pickle\n",
    "\n",
    "from keras import optimizers\n",
    "from keras.layers import Convolution1D, Dense, MaxPooling1D, Flatten\n",
    "from keras.constraints import maxnorm\n",
    "from keras.layers import Add\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Add\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Dropout, Input\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Activation\n",
    "from keras.utils import np_utils\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "os.environ['KMP_WARNINGS'] = 'off'\n",
    "sample_rate=44100\n",
    "hop_length = 441  # frame size= 2*hop\n",
    "segment_length=int(sample_rate*0.2)  #0.2\n",
    "segment_pad=int(sample_rate*0.02)     #0.02\n",
    "overlappiong=int(sample_rate*0.1)   #0.1\n",
    "\n",
    "NumofFeaturetoUse = 272 # this will re-assigned for different classifiers\n",
    "frame_number = 48\n",
    "\n",
    "try:\n",
    "    NumofFeaturetoUse = int(sys.argv[1])\n",
    "    print('Number of features to use is set to ' + str(sys.argv[1]) )\n",
    "except:\n",
    "    print('Number of features are unspecified. Defaut is set to = 272.')\n",
    "\n",
    "# input new indices file here\n",
    "indices_filename = 'D://indices_filename.npy'\n",
    "indices=np.load(indices_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmsprop = optimizers.RMSprop(lr=0.0001, rho=0.9, epsilon=None, decay=0.0)\n",
    "adam = optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "adagrad = optimizers.Adagrad(lr=0.01, epsilon=None, decay=0.0)\n",
    "adadelta = optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)\n",
    "adamax = optimizers.Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n",
    "nadam = optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avail_modules(directory, prefix):\n",
    "    module_names = []\n",
    "    for item in os.listdir(directory):\n",
    "        if prefix in item:\n",
    "            module_names.append(directory + item)\n",
    "            i = module_names.index(directory + item)\n",
    "            print(str(i) + 'th module')\n",
    "            print(directory + item)\n",
    "            \n",
    "    return module_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_parameters(this_name_hdf5):\n",
    "    parameters = this_name_hdf5.split('_')\n",
    "\n",
    "    neuron_index = parameters.index('neurons') + 1\n",
    "    n_neurons = parameters[neuron_index]\n",
    "    print('n_neurons = ' + str(n_neurons))\n",
    "    \n",
    "    batch_index = parameters.index('batches') + 1\n",
    "    n_batch = parameters[batch_index]\n",
    "    print('n_batch = ' + str(n_batch))\n",
    "    '''\n",
    "    epoch_index = parameters.index('n_epoch') + 1\n",
    "    n_epoch = parameters[epoch_index]\n",
    "    print('n_epoch = ' + str(n_epoch))\n",
    "    '''\n",
    "    filter_index = parameters.index('filters') + 1\n",
    "    nbindex = parameters[filter_index]\n",
    "    print('nbindex = ' + str(nbindex))\n",
    "\n",
    "    dropout_index = parameters.index('dropout') + 1\n",
    "    dropout = parameters[dropout_index]\n",
    "    print('dropout = ' + str(dropout))\n",
    "\n",
    "    try:\n",
    "        class_index = parameters.index('classes') + 1\n",
    "        classes = parameters[class_index]\n",
    "    except:\n",
    "        classes = 2\n",
    "    print('classes = ' + str(classes))\n",
    "\n",
    "    kerSize_index = parameters.index('kerSize') + 1\n",
    "    fillength = parameters[kerSize_index]\n",
    "    print('fillength = ' + str(fillength))\n",
    "\n",
    "    dense_index = parameters.index('dense') + 1\n",
    "    dense_layers = parameters[dense_index][0]\n",
    "    print('dense_layers = ' + str(dense_layers))\n",
    "    \n",
    "    n_epoch = 1000\n",
    "    \n",
    "    ret = [int(n_neurons), int(n_batch), int(n_epoch), int(nbindex), int(fillength), int(classes), float(dropout), int(dense_layers)]\n",
    "    \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_FeatureExtractfromSinglewindow(y,hop_length,sr):\n",
    "\n",
    "    genFeatures=np.array([])\n",
    "\n",
    "    mfcc0 = librosa.feature.mfcc(y=y, sr=sr, n_fft=hop_length*2, hop_length=hop_length, n_mfcc=13)\n",
    "    mfcc=np.transpose(mfcc0)\n",
    "\n",
    "    genFeatures = np.hstack((genFeatures, np.amin(mfcc, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.amax(mfcc, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.median(mfcc, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.mean(mfcc, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.std(mfcc, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.var(mfcc, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, st.skew(mfcc, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, st.kurtosis(mfcc, 0)))\n",
    "    #print(genFeatures.shape)\n",
    "\n",
    "    mfcc_delta=librosa.feature.delta(mfcc0)\n",
    "    mfcc_delta=np.transpose(mfcc_delta)\n",
    "    genFeatures = np.hstack((genFeatures, np.amin(mfcc_delta, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.amax(mfcc_delta, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.median(mfcc_delta, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.mean(mfcc_delta, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.std(mfcc_delta, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.var(mfcc_delta, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, st.skew(mfcc_delta, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, st.kurtosis(mfcc_delta, 0)))\n",
    "    #print(genFeatures.shape)\n",
    "\n",
    "    zcr0=librosa.feature.zero_crossing_rate(y=y, frame_length=hop_length*2, hop_length=hop_length)\n",
    "    zcr=np.transpose(zcr0)\n",
    "    genFeatures = np.hstack((genFeatures, np.amin(zcr, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.amax(zcr, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.median(zcr, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.mean(zcr, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.std(zcr, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.var(zcr, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, st.skew(zcr, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, st.kurtosis(zcr, 0)))\n",
    "    #print(genFeatures.shape)\n",
    "\n",
    "    zcr_delta=librosa.feature.delta(zcr0)\n",
    "    zcr_delta=np.transpose(zcr_delta)\n",
    "    genFeatures = np.hstack((genFeatures, np.amin(zcr_delta, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.amax(zcr_delta, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.median(zcr_delta, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.mean(zcr_delta, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.std(zcr_delta, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.var(zcr_delta, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, st.skew(zcr_delta, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, st.kurtosis(zcr_delta, 0)))\n",
    "    #print(genFeatures.shape)\n",
    "\n",
    "    Erms0=librosa.feature.rmse(y=y, frame_length=hop_length*2, hop_length=hop_length)\n",
    "    Erms=np.transpose(Erms0)\n",
    "    genFeatures = np.hstack((genFeatures, np.amin(Erms, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.amax(Erms, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.median(Erms, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.mean(Erms, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.std(Erms, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.var(Erms, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, st.skew(Erms, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, st.kurtosis(Erms, 0)))\n",
    "    #print(genFeatures.shape)\n",
    "\n",
    "    Erms_delta=librosa.feature.delta(Erms0)\n",
    "    Erms_delta=np.transpose(Erms_delta)\n",
    "    genFeatures = np.hstack((genFeatures, np.amin(Erms_delta, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.amax(Erms_delta, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.median(Erms_delta, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.mean(Erms_delta, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.std(Erms_delta, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.var(Erms_delta, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, st.skew(Erms_delta, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, st.kurtosis(Erms_delta, 0)))\n",
    "    #print(genFeatures.shape)\n",
    "\n",
    "    cent0 = librosa.feature.spectral_centroid(y=y, sr=sr, n_fft=hop_length*2, hop_length=hop_length)\n",
    "    cent=np.transpose(cent0)\n",
    "    genFeatures = np.hstack((genFeatures, np.amin(cent, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.amax(cent, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.median(cent, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.mean(cent, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.std(cent, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.var(cent, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, st.skew(cent, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, st.kurtosis(cent, 0)))\n",
    "    #print(genFeatures.shape)\n",
    "\n",
    "    cent_delta=librosa.feature.delta(cent0)\n",
    "    cent_delta=np.transpose(cent_delta)\n",
    "    genFeatures = np.hstack((genFeatures, np.amin(cent_delta, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.amax(cent_delta, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.median(cent_delta, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.mean(cent_delta, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.std(cent_delta, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.var(cent_delta, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, st.skew(cent_delta, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, st.kurtosis(cent_delta, 0)))\n",
    "    #print(genFeatures.shape)\n",
    "    #Each frame of a magnitude spectrogram is normalized and treated as a distribution over frequency bins, from which the mean (centroid) is extracted per frame.\n",
    "\n",
    "    ############### pitch at certain frame\n",
    "    pitches, magnitudes = librosa.core.piptrack(y=y, sr=sr, fmin=75, fmax=8000, n_fft=hop_length*2, hop_length=hop_length)\n",
    "    p=[pitches[magnitudes[:,i].argmax(),i] for i in range(0,pitches.shape[1])]\n",
    "    pitch0=np.array(p)   #shape (305,)\n",
    "    pitch=np.transpose(pitch0)\n",
    "    genFeatures = np.hstack((genFeatures, np.amin(pitch, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.amax(pitch, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.median(pitch, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.mean(pitch, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.std(pitch, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.var(pitch, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, st.skew(pitch, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, st.kurtosis(pitch, 0)))\n",
    "    #print(genFeatures.shape)\n",
    "\n",
    "    pitch_delta=librosa.feature.delta(pitch0)\n",
    "    pitch_delta=np.transpose(pitch_delta)\n",
    "    genFeatures = np.hstack((genFeatures, np.amin(pitch_delta, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.amax(pitch_delta, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.median(pitch_delta, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.mean(pitch_delta, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.std(pitch_delta, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.var(pitch_delta, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, st.skew(pitch_delta, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, st.kurtosis(pitch_delta, 0)))\n",
    "    #print(genFeatures.shape)    #272\n",
    "    return genFeatures\n",
    "\n",
    "\n",
    "def function_FeatureExtract1(audiofile, NumofFeatures):\n",
    "    NumofFeaturetoUse = NumofFeatures #needs to be reassigned, takes two parameters\n",
    "    ListOfFrame2Vec = np.empty((0, frame_number, NumofFeaturetoUse))\n",
    "    audio, s_rate = librosa.load(audiofile, sr=sample_rate)\n",
    "    segment_start_flag = 0\n",
    "    start_seg = 0\n",
    "    while (start_seg + segment_length) < len(audio):\n",
    "        flag = 1\n",
    "        sound1 = audio[start_seg:(start_seg + segment_length)]\n",
    "\n",
    "        featureSet = function_FeatureExtractfromSinglewindow(sound1, hop_length, sample_rate)\n",
    "\n",
    "        if segment_start_flag == 0:\n",
    "            SegAllFeat = featureSet\n",
    "            segment_start_flag = 1\n",
    "        else:\n",
    "            SegAllFeat = np.vstack((SegAllFeat, featureSet))\n",
    "\n",
    "        start_seg = start_seg + overlappiong\n",
    "\n",
    "    if segment_start_flag == 1:\n",
    "        SegAllFeat = np.nan_to_num(SegAllFeat)\n",
    "        float_compatible(SegAllFeat)\n",
    "        SegAllFeat = normalize(SegAllFeat, norm='l2', axis=0)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    if SegAllFeat.shape[0] > (frame_number-1):\n",
    "        SegAllFeat=SegAllFeat[0:frame_number]\n",
    "        SegFeatX = SegAllFeat[:, indices[0:NumofFeaturetoUse]]\n",
    "        ListOfFrame2Vec = np.append(ListOfFrame2Vec, array([SegFeatX]), axis=0)\n",
    "        return ListOfFrame2Vec\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to find minimum and maximum position in list \n",
    "def maxima(a): \n",
    "  \n",
    "    # inbuilt function to find the position of minimum  \n",
    "    minpos = a.index(min(a)) \n",
    "      \n",
    "    # inbuilt function to find the position of maximum  \n",
    "    maxpos = a.index(max(a))  \n",
    "    return maxpos\n",
    "\n",
    "def minima(a): \n",
    "  \n",
    "    # inbuilt function to find the position of minimum  \n",
    "    minpos = a.index(min(a)) \n",
    "      \n",
    "    # inbuilt function to find the position of maximum  \n",
    "    maxpos = a.index(max(a))  \n",
    "    return minpos\n",
    "\n",
    "def float_compatible(input_np):\n",
    "\n",
    "    x = np.where(input_np >= np.finfo(np.float32).max)\n",
    "    for index in range(0, len(x[0])):\n",
    "        x_position = x[0][index]\n",
    "        y_position = x[1][index]\n",
    "        \n",
    "        input_np[x_position, y_position] = 0.0\n",
    "    input_np = np.nan_to_num(input_np)\n",
    "        \n",
    "    return input_np\n",
    "\n",
    "def update_progress(progress):\n",
    "    bar_length = 100\n",
    "    if isinstance(progress, int):\n",
    "        progress = float(progress)\n",
    "    if not isinstance(progress, float):\n",
    "        progress = 0\n",
    "    if progress < 0:\n",
    "        progress = 0\n",
    "    if progress >= 1:\n",
    "        progress = 1\n",
    "\n",
    "    block = int(round(bar_length * progress))\n",
    "    clear_output(wait = True)\n",
    "    \n",
    "    text = \"Progress: [{0}] {1:.1f}%\".format( \"#\" * block + \"-\" * (bar_length - block), progress * 100)\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_TESS = 'D://Datasets//TESS//PAD_REVERB_NOISE//Happy//'\n",
    "a_TESS = 'D://Datasets//TESS//PAD_REVERB_NOISE//Angry//'\n",
    "n_TESS = 'D://Datasets//TESS//PAD_REVERB_NOISE//Neutral//'\n",
    "s_TESS = 'D://Datasets//TESS//PAD_REVERB_NOISE//Sad//'\n",
    "\n",
    "h_EMO = 'D://Datasets//EMO-DB//wav//Happy//'\n",
    "a_EMO = 'D://Datasets//EMO-DB//wav//Angry//'\n",
    "n_EMO = 'D://Datasets//EMO-DB//wav//Neutral//'\n",
    "s_EMO = 'D://Datasets//EMO-DB//wav//Sad//'\n",
    "o_EMO = 'D://Datasets//EMO-DB//wav//Other//'\n",
    "\n",
    "'''\n",
    "test_Sad = 'D://Datasets//TRAINING//TRAINING_PADDING_NOISE//Sad_test//'\n",
    "test_Happy = 'D://Datasets//TRAINING//TRAINING_PADDING_NOISE//Happy_test//'\n",
    "test_Angry = 'D://Datasets//TRAINING//TRAINING_PADDING_NOISE//Angry_test//'\n",
    "test_Neutral = 'D://Datasets//TRAINING//TRAINING_PADDING_NOISE//Neutral_test//'\n",
    "'''\n",
    "\n",
    "test_Happy = 'D://Datasets//TRAINING//TRAINING_NOISE_REVERB//Happy_test//'\n",
    "test_Angry = 'D://Datasets//TRAINING//TRAINING_NOISE_REVERB//Angry_test//'\n",
    "test_Neutral = 'D://Datasets//TRAINING//TRAINING_NOISE_REVERB//Neutral_test//'\n",
    "test_Sad = 'D://Datasets//TRAINING//TRAINING_NOISE_REVERB//Sad_test//'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_top_conv(num_layers):\n",
    "    \n",
    "    n_neurons = 1024\n",
    "    n_batch = 128\n",
    "    n_epoch = 1000\n",
    "    nbindex = 512\n",
    "    fillength = 4\n",
    "    classes = 2\n",
    "    \n",
    "    #ofilepath = \"C://Users//yg9ca//Documents//3_Layer(s)//CNN_final_Top_1024_128_1000.hdf5\"\n",
    "\n",
    "    model=Sequential()\n",
    "    model.add(Convolution1D(nb_filter=nbindex, filter_length=fillength, activation='relu', input_shape=(frame_number, 272), kernel_constraint=maxnorm(3)))\n",
    "    model.add(MaxPooling1D(pool_size=2, strides=2, padding='same'))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    for layer in range(0, num_layers):\n",
    "        model.add(Convolution1D(nb_filter=nbindex, filter_length=fillength, activation='relu', kernel_constraint=maxnorm(3)))\n",
    "        model.add(MaxPooling1D(pool_size=2, strides=2, padding='same'))\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(n_neurons, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(classes, activation='sigmoid'))\n",
    "    model.compile(loss='mean_squared_error', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_function_FeatureExtract3(InputFolderName):\n",
    "    X = function_FeatureExtract1(InputFolderName, 272)\n",
    "    y_pred = omodel.predict(X)\n",
    "    #print(y_pred)\n",
    "    x = maxima(list(y_pred[0]))\n",
    "    return x\n",
    "\n",
    "def top_classifier_eval(emotionFolders):\n",
    "    print('Predicted with overall classifier: ' + ofilepath)\n",
    "\n",
    "    correct = 0\n",
    "    incorrect = 0\n",
    "    \n",
    "    for emotionFolder in emotionFolders:\n",
    "        if 'Happy' in emotionFolder: val = 0\n",
    "        elif 'Angry' in emotionFolder: val = 0\n",
    "        elif 'Neutral' in emotionFolder: val = 1\n",
    "        else: val = 1\n",
    "\n",
    "        for emotionfile in os.listdir(emotionFolder):\n",
    "\n",
    "            cond1 = '0_' not in emotionfile and '1_' not in emotionfile\n",
    "            cond2 = emotionfile[len(emotionfile)-4] != '-' and emotionfile[len(emotionfile)-6] != '-' \n",
    "            cond3 = len(emotionfile) != len('03-01-05-01-01-01-06-6-8-4')\n",
    "            cond4 = emotionfile[0] != '.'\n",
    "\n",
    "            if cond2 and cond3 and cond4 and cond1:\n",
    "                try:\n",
    "                    x = top_function_FeatureExtract3(\n",
    "                        InputFolderName=emotionFolder+emotionfile)\n",
    "                    if(x == val): correct += 1\n",
    "                    else: incorrect += 1\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "    return correct/(correct+incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "omodel = optimal_top_conv(3)\n",
    "ofilepath = \"3_Layer(s)//CNN_final_Top_1024_128_1000.hdf5\"\n",
    "omodel.load_weights(ofilepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "top_classifier_eval([test_Happy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "top_classifier_eval([test_Angry])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "top_classifier_eval([test_Neutral])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "top_classifier_eval([test_Sad])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Top classifier accuracy on unseen Happy data - 0.9774436090225563\n",
    "Top classifier accuracy on unseen Angry data - 0.984313725490196\n",
    "Top classifier accuracy on unseen Neutral data - 0.9518518518518518\n",
    "Top classifier accuracy on unseen Sad data - 0.8954248366013072"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Happy vs Angry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0th module\n",
      "C://Users//yg9ca//Documents//2_Layer(s)//Checkpoint_H_A_neurons_56_batches_128_filters_96_dropout_0.2_kerSize_4_dense_1.hdf5\n",
      "1th module\n",
      "C://Users//yg9ca//Documents//2_Layer(s)//H_A_neurons_56_batches_128_filters_96_dropout_0.2_kerSize_4_dense_1.hdf5\n"
     ]
    }
   ],
   "source": [
    "directory = 'C://Users//yg9ca//Documents//2_Layer(s)//'\n",
    "module_prefix = 'H_A_'\n",
    "modules = avail_modules(directory, module_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_neurons = 56\n",
      "n_batch = 128\n",
      "nbindex = 96\n",
      "dropout = 0.2\n",
      "classes = 2\n",
      "fillength = 4\n",
      "dense_layers = 1\n"
     ]
    }
   ],
   "source": [
    "this_title = modules[1]\n",
    "n_neurons, n_batch, n_epoch, nbindex, fillength, classes, dropout, dense_layers = extract_parameters(this_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_network_model_opt(title, num_layers, n_neurons, n_batch, nbindex, dropout, classes, dense_layers):\n",
    "\n",
    "    model=Sequential()\n",
    "    \n",
    "    model.add(Convolution1D(nb_filter=nbindex, filter_length=fillength, activation='relu', input_shape=(48, 272), kernel_constraint=maxnorm(3)))\n",
    "    model.add(MaxPooling1D(pool_size=2, strides=2, padding='valid'))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    for layer in range(0, num_layers-1):\n",
    "        model.add(Convolution1D(nb_filter=nbindex, filter_length=fillength, activation='relu', kernel_constraint=maxnorm(3)))\n",
    "        model.add(MaxPooling1D(pool_size=2, strides=2, padding='valid'))\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    for layer in range(0, dense_layers):\n",
    "        model.add(Dense(n_neurons, activation='relu'))\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(Dense(classes, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer=rmsprop, metrics=['accuracy'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hafilepath = this_title\n",
    "hamodel = conv_network_model_opt(this_title, num_layers=2, n_neurons=n_neurons, n_batch=n_batch, nbindex=nbindex, dropout=dropout, classes=classes, dense_layers=dense_layers)\n",
    "hamodel.load_weights(hafilepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h_a_function_FeatureExtract3(InputFolderName):\n",
    "    X = function_FeatureExtract1(InputFolderName, 272)\n",
    "    y_pred = hamodel.predict(X)\n",
    "    print(y_pred)\n",
    "    x = maxima(list(y_pred[0]))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h_a_classifier_eval(emotionFolders):\n",
    "    print('Predicted with overall classifier: ' + hafilepath)\n",
    "\n",
    "    correct = 0\n",
    "    incorrect = 0\n",
    "    for emotionFolder in emotionFolders:\n",
    "\n",
    "        if 'Happy' in emotionFolder: val = 0\n",
    "        elif 'Angry' in emotionFolder: val = 1\n",
    "\n",
    "        for emotionfile in os.listdir(emotionFolder):\n",
    "            cond1 = '0_' not in emotionfile and '1_' not in emotionfile\n",
    "            cond2 = emotionfile[len(emotionfile)-4] != '-' and emotionfile[len(emotionfile)-6] != '-' \n",
    "            cond4 = emotionfile[0] != '.'\n",
    "            if cond2 and cond4 and cond1:\n",
    "                # print(correct+incorrect)\n",
    "                x = h_a_function_FeatureExtract3(InputFolderName=emotionFolder+emotionfile)\n",
    "                if(x == val): correct += 1\n",
    "                else: incorrect += 1\n",
    "        \n",
    "    return correct/(correct+incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted with overall classifier: C://Users//yg9ca//Documents//2_Layer(s)//H_A_neurons_56_batches_128_filters_96_dropout_0.2_kerSize_4_dense_1.hdf5\n",
      "[[0.74890906 0.251091  ]]\n",
      "[[0.97407    0.02593003]]\n",
      "[[0.14799935 0.8520006 ]]\n",
      "[[0.58812225 0.41187766]]\n",
      "[[0.21640372 0.7835963 ]]\n",
      "[[0.06110035 0.93889964]]\n",
      "[[0.60668635 0.39331365]]\n",
      "[[0.93618387 0.06381612]]\n",
      "[[0.6528038  0.34719628]]\n",
      "[[0.5123581 0.4876418]]\n",
      "[[0.9690671  0.03093291]]\n",
      "[[0.92169726 0.07830275]]\n",
      "[[0.9663058  0.03369416]]\n",
      "[[0.5147309  0.48526907]]\n",
      "[[0.80409694 0.19590299]]\n",
      "[[0.939043   0.06095706]]\n",
      "[[0.68040943 0.3195906 ]]\n",
      "[[0.95295334 0.04704668]]\n",
      "[[0.03914863 0.9608514 ]]\n",
      "[[0.2712291 0.7287709]]\n",
      "[[0.70522887 0.29477108]]\n",
      "[[0.6110266  0.38897339]]\n",
      "[[0.49119914 0.50880086]]\n",
      "[[0.02680217 0.9731978 ]]\n",
      "[[0.06860747 0.9313925 ]]\n",
      "[[0.04573684 0.9542632 ]]\n",
      "[[0.08970661 0.91029346]]\n",
      "[[0.81369525 0.18630478]]\n",
      "[[0.7057874 0.2942126]]\n",
      "[[0.90288484 0.09711519]]\n",
      "[[0.9843837  0.01561624]]\n",
      "[[0.99576086 0.00423917]]\n",
      "[[0.03194663 0.96805334]]\n",
      "[[0.128739 0.871261]]\n",
      "[[0.99575377 0.0042462 ]]\n",
      "[[0.7482191  0.25178096]]\n",
      "[[0.848382   0.15161802]]\n",
      "[[0.823416   0.17658404]]\n",
      "[[0.9870504  0.01294957]]\n",
      "[[0.9982021  0.00179786]]\n",
      "[[0.9873547  0.01264532]]\n",
      "[[0.9938048 0.0061952]]\n",
      "[[0.75814056 0.24185944]]\n",
      "[[0.9485042  0.05149583]]\n",
      "[[0.99896455 0.00103548]]\n",
      "[[0.99718684 0.00281311]]\n",
      "[[0.9847184  0.01528161]]\n",
      "[[0.89140123 0.10859881]]\n",
      "[[0.9931229  0.00687711]]\n",
      "[[0.9945557  0.00544434]]\n",
      "[[0.96826655 0.03173345]]\n",
      "[[0.09785536 0.9021446 ]]\n",
      "[[0.1410633 0.8589367]]\n",
      "[[0.9727418  0.02725816]]\n",
      "[[0.34230295 0.6576971 ]]\n",
      "[[0.6903374  0.30966264]]\n",
      "[[0.84866184 0.15133822]]\n",
      "[[0.09994775 0.9000522 ]]\n",
      "[[0.11832567 0.88167435]]\n",
      "[[0.5426017 0.4573983]]\n",
      "[[0.52444315 0.47555688]]\n",
      "[[0.83312666 0.16687337]]\n",
      "[[0.82918125 0.17081876]]\n",
      "[[0.7863833 0.2136168]]\n",
      "[[0.18033418 0.81966585]]\n",
      "[[0.67031693 0.32968313]]\n",
      "[[0.9914659  0.00853407]]\n",
      "[[0.64827245 0.35172758]]\n",
      "[[0.9923407  0.00765931]]\n",
      "[[0.9721781  0.02782185]]\n",
      "[[0.981087   0.01891293]]\n",
      "[[0.42941624 0.57058376]]\n",
      "[[0.25786218 0.74213785]]\n",
      "[[0.4978633  0.50213677]]\n",
      "[[0.9583431  0.04165696]]\n",
      "[[0.44113237 0.55886763]]\n",
      "[[0.8316471  0.16835286]]\n",
      "[[0.55853575 0.44146425]]\n",
      "[[0.07162292 0.92837703]]\n",
      "[[0.8098779  0.19012214]]\n",
      "[[0.9922374  0.00776257]]\n",
      "[[0.9983973  0.00160275]]\n",
      "[[0.72594297 0.27405706]]\n",
      "[[0.96048695 0.03951302]]\n",
      "[[0.9871644 0.0128356]]\n",
      "[[0.99824095 0.00175905]]\n",
      "[[0.96703064 0.03296932]]\n",
      "[[0.9932637  0.00673625]]\n",
      "[[0.805944   0.19405597]]\n",
      "[[0.73655146 0.2634485 ]]\n",
      "[[0.48196548 0.5180345 ]]\n",
      "[[0.8649079  0.13509202]]\n",
      "[[0.23953575 0.76046425]]\n",
      "[[0.61088395 0.38911602]]\n",
      "[[0.9787315  0.02126844]]\n",
      "[[0.5690789 0.4309211]]\n",
      "[[0.9422876  0.05771235]]\n",
      "[[0.62304205 0.37695798]]\n",
      "[[0.9855806  0.01441937]]\n",
      "[[0.63085294 0.36914706]]\n",
      "[[0.90455425 0.0954458 ]]\n",
      "[[0.57922685 0.4207731 ]]\n",
      "[[0.7997853  0.20021471]]\n",
      "[[0.936705   0.06329504]]\n",
      "[[0.88859236 0.11140759]]\n",
      "[[0.9605448  0.03945522]]\n",
      "[[0.10267556 0.89732444]]\n",
      "[[0.37568006 0.6243199 ]]\n",
      "[[0.41303676 0.5869632 ]]\n",
      "[[0.9544142  0.04558584]]\n",
      "[[0.9925021  0.00749787]]\n",
      "[[0.9945163  0.00548368]]\n",
      "[[0.8901772  0.10982282]]\n",
      "[[0.91547257 0.08452739]]\n",
      "[[0.93746275 0.06253725]]\n",
      "[[0.95889556 0.04110445]]\n",
      "[[0.58392644 0.41607356]]\n",
      "[[0.90787727 0.0921227 ]]\n",
      "[[0.9026991 0.0973009]]\n",
      "[[0.83915997 0.16084003]]\n",
      "[[0.78603476 0.2139652 ]]\n",
      "[[0.17740162 0.82259834]]\n",
      "[[0.99879885 0.00120118]]\n",
      "[[0.9939569  0.00604311]]\n",
      "[[0.9906641  0.00933589]]\n",
      "[[0.7747624  0.22523764]]\n",
      "[[0.47465077 0.52534926]]\n",
      "[[0.18959923 0.8104007 ]]\n",
      "[[0.27122965 0.7287704 ]]\n",
      "[[0.90541965 0.0945804 ]]\n",
      "[[0.7755248  0.22447518]]\n",
      "[[0.03883279 0.9611672 ]]\n",
      "[[0.02485337 0.9751466 ]]\n",
      "[[0.06172862 0.9382714 ]]\n",
      "[[0.99425656 0.00574352]]\n",
      "[[0.97179663 0.02820332]]\n",
      "[[0.8871113  0.11288866]]\n",
      "[[0.70453435 0.29546562]]\n",
      "[[0.07951221 0.9204878 ]]\n",
      "[[0.9983699 0.0016301]]\n",
      "[[0.10998648 0.8900135 ]]\n",
      "[[0.66807663 0.33192334]]\n",
      "[[0.8142751  0.18572496]]\n",
      "[[0.48346144 0.51653856]]\n",
      "[[0.61903393 0.38096604]]\n",
      "[[0.9201011  0.07989889]]\n",
      "[[0.2546049 0.7453951]]\n",
      "[[0.646967 0.353033]]\n",
      "[[0.38937104 0.61062896]]\n",
      "[[0.9983146  0.00168535]]\n",
      "[[0.82707244 0.17292751]]\n",
      "[[0.24697919 0.7530208 ]]\n",
      "[[0.97823745 0.02176255]]\n",
      "[[0.4715142 0.5284858]]\n",
      "[[0.94719946 0.05280047]]\n",
      "[[0.84575784 0.15424216]]\n",
      "[[0.12107144 0.8789286 ]]\n",
      "[[0.04652327 0.9534767 ]]\n",
      "[[0.982714   0.01728603]]\n",
      "[[0.95142895 0.04857109]]\n",
      "[[0.86511445 0.13488555]]\n",
      "[[0.31776243 0.6822376 ]]\n",
      "[[0.87028307 0.12971692]]\n",
      "[[0.19444902 0.80555105]]\n",
      "[[0.8693234  0.13067664]]\n",
      "[[0.9847829  0.01521714]]\n",
      "[[0.99497014 0.00502989]]\n",
      "[[0.17288136 0.82711864]]\n",
      "[[0.28436404 0.715636  ]]\n",
      "[[0.04016725 0.9598327 ]]\n",
      "[[0.78449327 0.21550667]]\n",
      "[[0.04556264 0.9544373 ]]\n",
      "[[0.94918615 0.05081388]]\n",
      "[[0.99709916 0.00290085]]\n",
      "[[0.8952087  0.10479125]]\n",
      "[[0.9934916  0.00650839]]\n",
      "[[0.6372612  0.36273888]]\n",
      "[[0.6193912 0.3806088]]\n",
      "[[0.7412044 0.2587956]]\n",
      "[[0.985746   0.01425393]]\n",
      "[[0.6918027 0.3081973]]\n",
      "[[0.25730354 0.74269646]]\n",
      "[[0.54015243 0.45984757]]\n",
      "[[0.12010849 0.8798916 ]]\n",
      "[[0.827379 0.172621]]\n",
      "[[0.99511683 0.00488316]]\n",
      "[[0.20793176 0.79206824]]\n",
      "[[0.94309825 0.05690171]]\n",
      "[[0.73297054 0.26702946]]\n",
      "[[0.9545563  0.04544378]]\n",
      "[[0.89284754 0.10715251]]\n",
      "[[0.3344628 0.6655372]]\n",
      "[[0.12433966 0.87566036]]\n",
      "[[0.0778503 0.9221497]]\n",
      "[[0.4207858  0.57921416]]\n",
      "[[0.68284917 0.3171509 ]]\n",
      "[[0.15006226 0.8499377 ]]\n",
      "[[0.94129896 0.05870104]]\n",
      "[[0.33155513 0.66844493]]\n",
      "[[0.78787947 0.21212049]]\n",
      "[[0.53580284 0.46419716]]\n",
      "[[0.97416425 0.02583576]]\n",
      "[[0.81373477 0.18626525]]\n",
      "[[0.58097345 0.41902658]]\n",
      "[[0.9747308 0.0252692]]\n",
      "[[0.89019704 0.10980293]]\n",
      "[[0.04018827 0.95981175]]\n",
      "[[0.7860173  0.21398264]]\n",
      "[[0.899258   0.10074197]]\n",
      "[[0.72608924 0.27391076]]\n",
      "[[0.988226   0.01177396]]\n",
      "[[0.78764844 0.21235153]]\n",
      "[[0.94756454 0.0524355 ]]\n",
      "[[0.838249   0.16175097]]\n",
      "[[0.99753547 0.00246449]]\n",
      "[[0.93116194 0.06883804]]\n",
      "[[0.8870677  0.11293229]]\n",
      "[[0.9587138  0.04128614]]\n",
      "[[0.99492407 0.00507594]]\n",
      "[[0.9976209  0.00237913]]\n",
      "[[0.20699543 0.7930045 ]]\n",
      "[[0.49433154 0.50566846]]\n",
      "[[0.9756217  0.02437832]]\n",
      "[[0.34682256 0.65317744]]\n",
      "[[0.39941165 0.6005884 ]]\n",
      "[[0.5514469  0.44855314]]\n",
      "[[0.71999127 0.28000873]]\n",
      "[[0.5656671  0.43433288]]\n",
      "[[0.44937643 0.5506236 ]]\n",
      "[[0.95776767 0.0422323 ]]\n",
      "[[0.78116125 0.21883878]]\n",
      "[[0.43350458 0.56649536]]\n",
      "[[0.968635   0.03136498]]\n",
      "[[0.71015054 0.2898494 ]]\n",
      "[[0.8905292  0.10947081]]\n",
      "[[0.6737421  0.32625785]]\n",
      "[[0.79310656 0.20689346]]\n",
      "[[0.9882833  0.01171673]]\n",
      "[[0.67733926 0.32266074]]\n",
      "[[0.608454   0.39154604]]\n",
      "[[0.7850847  0.21491534]]\n",
      "[[0.3799672 0.6200328]]\n",
      "[[0.73149645 0.26850358]]\n",
      "[[0.8614723 0.1385277]]\n",
      "[[0.9885188  0.01148122]]\n",
      "[[0.44992775 0.5500722 ]]\n",
      "[[0.67800474 0.32199526]]\n",
      "[[0.9514816  0.04851846]]\n",
      "[[0.960366   0.03963398]]\n",
      "[[0.7234149  0.27658504]]\n",
      "[[0.92894363 0.07105639]]\n",
      "[[0.8525426  0.14745742]]\n",
      "[[0.99292713 0.00707288]]\n",
      "[[0.6765623  0.32343775]]\n",
      "[[0.7509197  0.24908033]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.26666666666666666"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_a_classifier_eval([test_Angry])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted with overall classifier: C://Users//yg9ca//Documents//2_Layer(s)//H_A_neurons_56_batches_128_filters_96_dropout_0.2_kerSize_4_dense_1.hdf5\n",
      "[[0.903051   0.09694894]]\n",
      "[[0.3827464  0.61725354]]\n",
      "[[0.37757036 0.6224296 ]]\n",
      "[[0.727453   0.27254707]]\n",
      "[[0.52340287 0.47659713]]\n",
      "[[0.7122225  0.28777745]]\n",
      "[[0.00717487 0.99282515]]\n",
      "[[0.05102117 0.9489788 ]]\n",
      "[[0.9712091  0.02879093]]\n",
      "[[0.2921629 0.7078371]]\n",
      "[[0.97863513 0.02136491]]\n",
      "[[0.6279473 0.3720528]]\n",
      "[[0.6349242  0.36507586]]\n",
      "[[0.4693732 0.5306268]]\n",
      "[[0.89618665 0.10381338]]\n",
      "[[0.54521745 0.45478258]]\n",
      "[[0.46872753 0.5312725 ]]\n",
      "[[0.7020035  0.29799652]]\n",
      "[[0.11252715 0.8874728 ]]\n",
      "[[0.9964167  0.00358324]]\n",
      "[[0.16235703 0.83764297]]\n",
      "[[0.15928978 0.8407102 ]]\n",
      "[[0.4896204  0.51037955]]\n",
      "[[0.99355906 0.00644098]]\n",
      "[[0.9743148  0.02568513]]\n",
      "[[0.5821247  0.41787526]]\n",
      "[[0.09370043 0.90629953]]\n",
      "[[0.8122503  0.18774961]]\n",
      "[[0.49247983 0.5075201 ]]\n",
      "[[0.8221964  0.17780356]]\n",
      "[[0.72006875 0.27993125]]\n",
      "[[0.9510131  0.04898693]]\n",
      "[[0.90615    0.09385005]]\n",
      "[[0.85626286 0.14373714]]\n",
      "[[0.85020095 0.14979906]]\n",
      "[[0.9284527  0.07154729]]\n",
      "[[0.63738745 0.36261258]]\n",
      "[[0.8784395  0.12156051]]\n",
      "[[0.9397139 0.0602861]]\n",
      "[[0.20534383 0.79465616]]\n",
      "[[0.3841742  0.61582583]]\n",
      "[[0.97499096 0.02500896]]\n",
      "[[0.8375839  0.16241613]]\n",
      "[[0.9825501 0.0174499]]\n",
      "[[0.9864087  0.01359128]]\n",
      "[[0.33009493 0.6699051 ]]\n",
      "[[0.861742   0.13825798]]\n",
      "[[0.9972269  0.00277305]]\n",
      "[[0.99323976 0.0067603 ]]\n",
      "[[0.902028   0.09797196]]\n",
      "[[0.9876747  0.01232528]]\n",
      "[[0.9971064  0.00289366]]\n",
      "[[0.97614384 0.02385617]]\n",
      "[[0.90752643 0.0924736 ]]\n",
      "[[0.75004554 0.24995452]]\n",
      "[[0.9804037  0.01959626]]\n",
      "[[0.17762281 0.82237715]]\n",
      "[[0.9902255  0.00977455]]\n",
      "[[0.8210389  0.17896104]]\n",
      "[[0.9324911  0.06750884]]\n",
      "[[0.49053475 0.5094652 ]]\n",
      "[[0.5213372  0.47866276]]\n",
      "[[0.9697925  0.03020746]]\n",
      "[[0.21644129 0.7835587 ]]\n",
      "[[0.5274561  0.47254387]]\n",
      "[[0.9418725  0.05812761]]\n",
      "[[0.9429708  0.05702915]]\n",
      "[[0.20015167 0.7998484 ]]\n",
      "[[0.90574205 0.09425792]]\n",
      "[[0.92928404 0.07071596]]\n",
      "[[0.6685785 0.3314215]]\n",
      "[[0.9184362  0.08156379]]\n",
      "[[0.8940172 0.1059828]]\n",
      "[[0.90698606 0.09301393]]\n",
      "[[0.9927516  0.00724844]]\n",
      "[[0.99701774 0.0029823 ]]\n",
      "[[0.9820295  0.01797046]]\n",
      "[[0.9063397  0.09366033]]\n",
      "[[0.61443055 0.38556945]]\n",
      "[[0.3950829  0.60491717]]\n",
      "[[0.23701455 0.7629854 ]]\n",
      "[[0.97449696 0.0255031 ]]\n",
      "[[0.735891   0.26410905]]\n",
      "[[0.84973407 0.1502659 ]]\n",
      "[[0.0237391  0.97626096]]\n",
      "[[0.9912027  0.00879728]]\n",
      "[[0.01523406 0.9847659 ]]\n",
      "[[0.1144974  0.88550264]]\n",
      "[[0.6425923  0.35740775]]\n",
      "[[0.9277657  0.07223427]]\n",
      "[[0.5608699  0.43913016]]\n",
      "[[0.96447486 0.03552508]]\n",
      "[[0.8235372  0.17646286]]\n",
      "[[0.90745014 0.09254987]]\n",
      "[[0.81397    0.18603003]]\n",
      "[[0.96855944 0.03144054]]\n",
      "[[0.03278053 0.9672194 ]]\n",
      "[[0.5107844  0.48921558]]\n",
      "[[0.63433677 0.36566326]]\n",
      "[[0.8157817  0.18421829]]\n",
      "[[0.8605727  0.13942733]]\n",
      "[[0.13516603 0.86483395]]\n",
      "[[0.82248676 0.17751323]]\n",
      "[[0.6637154  0.33628455]]\n",
      "[[0.77764887 0.22235115]]\n",
      "[[0.97588706 0.02411294]]\n",
      "[[0.8370329  0.16296713]]\n",
      "[[0.08194219 0.9180578 ]]\n",
      "[[0.7823092  0.21769083]]\n",
      "[[0.928969 0.071031]]\n",
      "[[0.05247959 0.9475204 ]]\n",
      "[[0.94172883 0.0582712 ]]\n",
      "[[0.9487198  0.05128021]]\n",
      "[[0.80463696 0.19536307]]\n",
      "[[0.99480236 0.00519762]]\n",
      "[[0.7691889  0.23081113]]\n",
      "[[0.96040815 0.03959185]]\n",
      "[[0.9634685  0.03653155]]\n",
      "[[0.99378175 0.00621831]]\n",
      "[[0.20790382 0.79209626]]\n",
      "[[0.99370533 0.00629463]]\n",
      "[[0.9923103  0.00768976]]\n",
      "[[0.95981926 0.04018081]]\n",
      "[[0.88993865 0.11006138]]\n",
      "[[0.9318078  0.06819225]]\n",
      "[[0.03169587 0.9683042 ]]\n",
      "[[0.24106137 0.75893867]]\n",
      "[[0.51453406 0.48546594]]\n",
      "[[0.97852296 0.02147713]]\n",
      "[[0.90820754 0.09179249]]\n",
      "[[0.06081083 0.93918914]]\n",
      "[[0.9792363  0.02076368]]\n",
      "[[0.07881052 0.9211894 ]]\n",
      "[[0.86959606 0.13040397]]\n",
      "[[0.95724446 0.04275554]]\n",
      "[[0.31598285 0.6840172 ]]\n",
      "[[0.6192058 0.3807942]]\n",
      "[[0.6996242  0.30037582]]\n",
      "[[0.14916205 0.85083795]]\n",
      "[[0.5582636  0.44173637]]\n",
      "[[0.31076252 0.6892375 ]]\n",
      "[[0.91718    0.08282001]]\n",
      "[[0.09708869 0.90291137]]\n",
      "[[0.8959215  0.10407846]]\n",
      "[[0.96881044 0.0311895 ]]\n",
      "[[0.98676896 0.01323106]]\n",
      "[[0.37789646 0.6221035 ]]\n",
      "[[0.98766476 0.01233524]]\n",
      "[[0.7653947  0.23460533]]\n",
      "[[0.18829174 0.8117083 ]]\n",
      "[[0.78997046 0.21002954]]\n",
      "[[0.9137341  0.08626596]]\n",
      "[[0.52835536 0.47164467]]\n",
      "[[0.9813741  0.01862589]]\n",
      "[[0.98466647 0.01533349]]\n",
      "[[0.34079787 0.6592021 ]]\n",
      "[[0.1790262  0.82097375]]\n",
      "[[0.22487485 0.7751252 ]]\n",
      "[[0.95692825 0.04307172]]\n",
      "[[0.9815035  0.01849656]]\n",
      "[[0.8008505  0.19914947]]\n",
      "[[0.93959546 0.06040454]]\n",
      "[[0.8061034  0.19389656]]\n",
      "[[0.64372814 0.3562719 ]]\n",
      "[[0.9712955  0.02870453]]\n",
      "[[0.92392665 0.0760734 ]]\n",
      "[[0.89979595 0.10020402]]\n",
      "[[0.97186345 0.02813662]]\n",
      "[[0.88093674 0.11906326]]\n",
      "[[0.9066211  0.09337889]]\n",
      "[[0.9948094  0.00519053]]\n",
      "[[0.51682216 0.48317778]]\n",
      "[[0.9792012  0.02079877]]\n",
      "[[0.8868022  0.11319783]]\n",
      "[[0.28864908 0.7113509 ]]\n",
      "[[0.8247753  0.17522468]]\n",
      "[[0.6661937  0.33380628]]\n",
      "[[0.9732816  0.02671841]]\n",
      "[[0.73521304 0.26478696]]\n",
      "[[0.24254982 0.7574502 ]]\n",
      "[[0.8461603  0.15383966]]\n",
      "[[0.9055431 0.0944569]]\n",
      "[[0.39693713 0.60306287]]\n",
      "[[0.9141302  0.08586986]]\n",
      "[[0.8866155  0.11338447]]\n",
      "[[0.9978181 0.0021819]]\n",
      "[[0.7234489  0.27655116]]\n",
      "[[0.6138826  0.38611743]]\n",
      "[[0.57912934 0.42087066]]\n",
      "[[0.9912083  0.00879165]]\n",
      "[[0.9294955  0.07050443]]\n",
      "[[0.88575375 0.11424624]]\n",
      "[[0.9859023  0.01409773]]\n",
      "[[0.00845479 0.9915452 ]]\n",
      "[[0.9982297  0.00177039]]\n",
      "[[0.10735561 0.8926444 ]]\n",
      "[[0.7886151  0.21138482]]\n",
      "[[0.52276087 0.47723916]]\n",
      "[[0.99376523 0.00623476]]\n",
      "[[0.60374933 0.39625067]]\n",
      "[[0.7179838 0.2820162]]\n",
      "[[0.7145857  0.28541434]]\n",
      "[[0.76546144 0.23453848]]\n",
      "[[0.73225516 0.2677448 ]]\n",
      "[[0.9863523  0.01364762]]\n",
      "[[0.9048047 0.0951953]]\n",
      "[[0.78571844 0.21428159]]\n",
      "[[0.82288045 0.17711951]]\n",
      "[[0.06625449 0.9337455 ]]\n",
      "[[0.32319123 0.67680883]]\n",
      "[[0.99698395 0.0030161 ]]\n",
      "[[0.7203258  0.27967417]]\n",
      "[[0.36367568 0.6363243 ]]\n",
      "[[0.6692439  0.33075613]]\n",
      "[[0.8041865  0.19581349]]\n",
      "[[0.8172958  0.18270425]]\n",
      "[[0.8286351 0.1713649]]\n",
      "[[0.9730059  0.02699407]]\n",
      "[[0.8146204  0.18537958]]\n",
      "[[0.9919686  0.00803143]]\n",
      "[[0.48155025 0.5184497 ]]\n",
      "[[0.7559152 0.2440848]]\n",
      "[[0.6226031 0.3773969]]\n",
      "[[0.1527863 0.8472136]]\n",
      "[[0.43768612 0.5623139 ]]\n",
      "[[0.9316008  0.06839919]]\n",
      "[[0.74519163 0.2548084 ]]\n",
      "[[0.98731923 0.01268072]]\n",
      "[[0.9785782  0.02142181]]\n",
      "[[0.95472044 0.04527957]]\n",
      "[[0.997081   0.00291897]]\n",
      "[[0.6498387 0.3501613]]\n",
      "[[0.9853221 0.0146779]]\n",
      "[[0.9644772  0.03552283]]\n",
      "[[0.4806971  0.51930296]]\n",
      "[[0.67277443 0.32722557]]\n",
      "[[0.45314825 0.54685175]]\n",
      "[[0.8689004  0.13109954]]\n",
      "[[0.9755475 0.0244525]]\n",
      "[[0.58196175 0.41803825]]\n",
      "[[0.8323032  0.16769674]]\n",
      "[[0.95990455 0.04009539]]\n",
      "[[0.9673181  0.03268184]]\n",
      "[[0.9901335  0.00986652]]\n",
      "[[0.18864517 0.8113548 ]]\n",
      "[[0.9930508  0.00694923]]\n",
      "[[0.9975545  0.00244555]]\n",
      "[[0.94603074 0.05396925]]\n",
      "[[0.9067992  0.09320082]]\n",
      "[[0.77780396 0.22219598]]\n",
      "[[0.63761497 0.362385  ]]\n",
      "[[0.9569799 0.0430201]]\n",
      "[[0.9573144  0.04268564]]\n",
      "[[0.6566583 0.3433417]]\n",
      "[[0.78347325 0.21652676]]\n",
      "[[0.9527553 0.0472447]]\n",
      "[[0.9351428  0.06485713]]\n",
      "[[0.79999524 0.20000468]]\n",
      "[[0.07194277 0.9280572 ]]\n",
      "[[0.46870777 0.5312923 ]]\n",
      "[[0.83249646 0.16750354]]\n",
      "[[0.530176   0.46982402]]\n",
      "[[0.9156257  0.08437434]]\n",
      "[[0.42822787 0.5717721 ]]\n",
      "[[0.8360208  0.16397911]]\n",
      "[[0.25406232 0.7459377 ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7706766917293233"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_a_classifier_eval([test_Happy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
