{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features are unspecified. Defaut is set to = 272.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import gc\n",
    "import sys\n",
    "import h5py\n",
    "import time\n",
    "import datetime\n",
    "import pickle\n",
    "import librosa\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import pandas as pd\n",
    "from pandas.plotting import parallel_coordinates\n",
    "from pydub import AudioSegment\n",
    "\n",
    "#imported for testing\n",
    "import wave\n",
    "import contextlib\n",
    "\n",
    "# for outputing file\n",
    "from scipy.cluster.vq import vq, kmeans, whiten\n",
    "import scipy.stats.stats as st\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.decomposition import PCA as sklearnPCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import auc, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn import mixture\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "import keras\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.constraints import maxnorm\n",
    "from keras.layers import Convolution1D, Dense, MaxPooling1D, Flatten, Add, Dropout, Input, Activation\n",
    "from keras.layers import TimeDistributed, Bidirectional, LSTM, LeakyReLU\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras import optimizers, regularizers\n",
    "from keras.utils import np_utils, to_categorical\n",
    "\n",
    "from colorama import Fore, Back, Style\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "#warnings.filterwarnings('ignore')\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# confirm TensorFlow sees the GPU\n",
    "from tensorflow.python.client import device_lib\n",
    "# assert 'GPU' in str(device_lib.list_local_devices())\n",
    "\n",
    "# confirm Keras sees the GPU\n",
    "from keras import backend\n",
    "# print(len(backend.tensorflow_backend._get_available_gpus()) > 0)\n",
    "\n",
    "#warnings.filterwarnings('ignore')\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "sample_rate=44100\n",
    "hop_length = 441  # frame size= 2*hop\n",
    "segment_length=int(sample_rate*0.2)  #0.2\n",
    "segment_pad=int(sample_rate*0.02)     #0.02\n",
    "overlappiong=int(sample_rate*0.1)   #0.1\n",
    "\n",
    "NumofFeaturetoUse = 100 # this will re-assigned for different classifiers\n",
    "frame_number = 48\n",
    "\n",
    "try:\n",
    "    NumofFeaturetoUse = int(sys.argv[1])\n",
    "    print('Number of features to use is set to ' + str(sys.argv[1]) )\n",
    "except:\n",
    "    print('Number of features are unspecified. Defaut is set to = 272.')\n",
    "\n",
    "# input new indices file here\n",
    "indices_filename = 'D://indices_filename.npy'\n",
    "indices=np.load(indices_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmsprop = optimizers.RMSprop(lr=0.0001, rho=0.9, epsilon=None, decay=0.0)\n",
    "adam = optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "adagrad = optimizers.Adagrad(lr=0.01, epsilon=None, decay=0.0)\n",
    "adadelta = optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)\n",
    "adamax = optimizers.Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n",
    "nadam = optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avail_modules(directory, prefix):\n",
    "    module_names = []\n",
    "    for item in os.listdir(directory):\n",
    "        if prefix in item:\n",
    "            module_names.append(directory + item)\n",
    "            i = module_names.index(directory + item)\n",
    "            print(str(i) + 'th module')\n",
    "            print(directory + item)\n",
    "            \n",
    "    return module_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_parameters(this_name_hdf5):\n",
    "    parameters = this_name_hdf5.split('_')\n",
    "\n",
    "    neuron_index = parameters.index('neurons') + 1\n",
    "    n_neurons = parameters[neuron_index]\n",
    "    print('n_neurons = ' + str(n_neurons))\n",
    "    \n",
    "    batch_index = parameters.index('batches') + 1\n",
    "    n_batch = parameters[batch_index]\n",
    "    print('n_batch = ' + str(n_batch))\n",
    "    '''\n",
    "    epoch_index = parameters.index('n_epoch') + 1\n",
    "    n_epoch = parameters[epoch_index]\n",
    "    print('n_epoch = ' + str(n_epoch))\n",
    "    '''\n",
    "    filter_index = parameters.index('filters') + 1\n",
    "    nbindex = parameters[filter_index]\n",
    "    print('nbindex = ' + str(nbindex))\n",
    "\n",
    "    dropout_index = parameters.index('dropout') + 1\n",
    "    dropout = parameters[dropout_index]\n",
    "    print('dropout = ' + str(dropout))\n",
    "\n",
    "    try:\n",
    "        class_index = parameters.index('classes') + 1\n",
    "        classes = parameters[class_index]\n",
    "    except:\n",
    "        classes = 2\n",
    "    print('classes = ' + str(classes))\n",
    "\n",
    "    kerSize_index = parameters.index('kerSize') + 1\n",
    "    fillength = parameters[kerSize_index]\n",
    "    print('fillength = ' + str(fillength))\n",
    "\n",
    "    dense_index = parameters.index('dense') + 1\n",
    "    dense_layers = parameters[dense_index][0]\n",
    "    print('dense_layers = ' + str(dense_layers))\n",
    "    \n",
    "    n_epoch = 1000\n",
    "    \n",
    "    ret = [int(n_neurons), int(n_batch), int(n_epoch), int(nbindex), int(fillength), int(classes), float(dropout), int(dense_layers)]\n",
    "    \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_FeatureExtractfromSinglewindow(y,hop_length,sr):\n",
    "\n",
    "    genFeatures=np.array([])\n",
    "\n",
    "    mfcc0 = librosa.feature.mfcc(y=y, sr=sr, n_fft=hop_length*2, hop_length=hop_length, n_mfcc=13)\n",
    "    mfcc=np.transpose(mfcc0)\n",
    "\n",
    "    genFeatures = np.hstack((genFeatures, np.amin(mfcc, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.amax(mfcc, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.median(mfcc, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.mean(mfcc, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.std(mfcc, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.var(mfcc, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, st.skew(mfcc, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, st.kurtosis(mfcc, 0)))\n",
    "    #print(genFeatures.shape)\n",
    "\n",
    "    mfcc_delta=librosa.feature.delta(mfcc0)\n",
    "    mfcc_delta=np.transpose(mfcc_delta)\n",
    "    genFeatures = np.hstack((genFeatures, np.amin(mfcc_delta, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.amax(mfcc_delta, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.median(mfcc_delta, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.mean(mfcc_delta, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.std(mfcc_delta, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.var(mfcc_delta, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, st.skew(mfcc_delta, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, st.kurtosis(mfcc_delta, 0)))\n",
    "    #print(genFeatures.shape)\n",
    "\n",
    "    zcr0=librosa.feature.zero_crossing_rate(y=y, frame_length=hop_length*2, hop_length=hop_length)\n",
    "    zcr=np.transpose(zcr0)\n",
    "    genFeatures = np.hstack((genFeatures, np.amin(zcr, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.amax(zcr, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.median(zcr, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.mean(zcr, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.std(zcr, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.var(zcr, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, st.skew(zcr, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, st.kurtosis(zcr, 0)))\n",
    "    #print(genFeatures.shape)\n",
    "\n",
    "    zcr_delta=librosa.feature.delta(zcr0)\n",
    "    zcr_delta=np.transpose(zcr_delta)\n",
    "    genFeatures = np.hstack((genFeatures, np.amin(zcr_delta, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.amax(zcr_delta, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.median(zcr_delta, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.mean(zcr_delta, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.std(zcr_delta, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.var(zcr_delta, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, st.skew(zcr_delta, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, st.kurtosis(zcr_delta, 0)))\n",
    "    #print(genFeatures.shape)\n",
    "\n",
    "    Erms0=librosa.feature.rms(y=y, frame_length=hop_length*2, hop_length=hop_length)\n",
    "    Erms=np.transpose(Erms0)\n",
    "    genFeatures = np.hstack((genFeatures, np.amin(Erms, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.amax(Erms, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.median(Erms, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.mean(Erms, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.std(Erms, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.var(Erms, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, st.skew(Erms, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, st.kurtosis(Erms, 0)))\n",
    "    #print(genFeatures.shape)\n",
    "\n",
    "    Erms_delta=librosa.feature.delta(Erms0)\n",
    "    Erms_delta=np.transpose(Erms_delta)\n",
    "    genFeatures = np.hstack((genFeatures, np.amin(Erms_delta, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.amax(Erms_delta, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.median(Erms_delta, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.mean(Erms_delta, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.std(Erms_delta, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.var(Erms_delta, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, st.skew(Erms_delta, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, st.kurtosis(Erms_delta, 0)))\n",
    "    #print(genFeatures.shape)\n",
    "\n",
    "    cent0 = librosa.feature.spectral_centroid(y=y, sr=sr, n_fft=hop_length*2, hop_length=hop_length)\n",
    "    cent=np.transpose(cent0)\n",
    "    genFeatures = np.hstack((genFeatures, np.amin(cent, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.amax(cent, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.median(cent, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.mean(cent, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.std(cent, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.var(cent, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, st.skew(cent, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, st.kurtosis(cent, 0)))\n",
    "    #print(genFeatures.shape)\n",
    "\n",
    "    cent_delta=librosa.feature.delta(cent0)\n",
    "    cent_delta=np.transpose(cent_delta)\n",
    "    genFeatures = np.hstack((genFeatures, np.amin(cent_delta, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.amax(cent_delta, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.median(cent_delta, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.mean(cent_delta, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.std(cent_delta, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.var(cent_delta, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, st.skew(cent_delta, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, st.kurtosis(cent_delta, 0)))\n",
    "    #print(genFeatures.shape)\n",
    "    #Each frame of a magnitude spectrogram is normalized and treated as a distribution over frequency bins, from which the mean (centroid) is extracted per frame.\n",
    "\n",
    "    ############### pitch at certain frame\n",
    "    pitches, magnitudes = librosa.core.piptrack(y=y, sr=sr, fmin=75, fmax=8000, n_fft=hop_length*2, hop_length=hop_length)\n",
    "    p=[pitches[magnitudes[:,i].argmax(),i] for i in range(0,pitches.shape[1])]\n",
    "    pitch0=np.array(p)   #shape (305,)\n",
    "    pitch=np.transpose(pitch0)\n",
    "    genFeatures = np.hstack((genFeatures, np.amin(pitch, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.amax(pitch, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.median(pitch, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.mean(pitch, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.std(pitch, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.var(pitch, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, st.skew(pitch, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, st.kurtosis(pitch, 0)))\n",
    "    #print(genFeatures.shape)\n",
    "\n",
    "    pitch_delta=librosa.feature.delta(pitch0)\n",
    "    pitch_delta=np.transpose(pitch_delta)\n",
    "    genFeatures = np.hstack((genFeatures, np.amin(pitch_delta, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.amax(pitch_delta, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.median(pitch_delta, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.mean(pitch_delta, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.std(pitch_delta, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, np.var(pitch_delta, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, st.skew(pitch_delta, 0)))\n",
    "    genFeatures = np.hstack((genFeatures, st.kurtosis(pitch_delta, 0)))\n",
    "    #print(genFeatures.shape)    #272\n",
    "    return genFeatures\n",
    "\n",
    "\n",
    "'''\n",
    "Extract specified amount of features from an audio file\n",
    "'''\n",
    "def function_FeatureExtract1(audiofile, NumofFeatures):\n",
    "    extension = '.wav'\n",
    "    flag_start_all = 0\n",
    "    flag_Y_start = 0\n",
    "    All = np.array([])\n",
    "    NumofFeaturetoUse = NumofFeatures #needs to be reassigned, takes two parameters\n",
    "    ListOfFrame2Vec = np.empty((0, frame_number, NumofFeaturetoUse))\n",
    "    audio, s_rate = librosa.load(audiofile, sr=sample_rate)\n",
    "    segment_start_flag = 0\n",
    "    start_seg = 0\n",
    "    while (start_seg + segment_length) < len(audio):\n",
    "        flag = 1\n",
    "        sound1 = audio[start_seg:(start_seg + segment_length)]\n",
    "\n",
    "        featureSet = function_FeatureExtractfromSinglewindow(sound1, hop_length, sample_rate)\n",
    "\n",
    "        if segment_start_flag == 0:\n",
    "            SegAllFeat = featureSet\n",
    "            segment_start_flag = 1\n",
    "        else:\n",
    "            SegAllFeat = np.vstack((SegAllFeat, featureSet))\n",
    "\n",
    "        start_seg = start_seg + overlappiong\n",
    "\n",
    "    if segment_start_flag == 1:\n",
    "        #print(SegAllFeat.shape)\n",
    "        SegAllFeat = normalize(SegAllFeat, norm='l2', axis=0)\n",
    "\n",
    "    #print(SegAllFeat.shape)\n",
    "    if flag_start_all == 0:\n",
    "        All = SegAllFeat\n",
    "        flag_start_all = 1\n",
    "    else:\n",
    "        All = np.vstack((All, SegAllFeat))\n",
    "\n",
    "    return All\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to find minimum and maximum position in list \n",
    "def maxima(a): \n",
    "  \n",
    "    # inbuilt function to find the position of minimum  \n",
    "    minpos = a.index(min(a)) \n",
    "      \n",
    "    # inbuilt function to find the position of maximum  \n",
    "    maxpos = a.index(max(a))  \n",
    "    return maxpos\n",
    "\n",
    "def minima(a): \n",
    "  \n",
    "    # inbuilt function to find the position of minimum  \n",
    "    minpos = a.index(min(a)) \n",
    "      \n",
    "    # inbuilt function to find the position of maximum  \n",
    "    maxpos = a.index(max(a))  \n",
    "    return minpos\n",
    "\n",
    "def float_compatible(input_np):\n",
    "\n",
    "    x = np.where(input_np >= np.finfo(np.float32).max)\n",
    "    for index in range(0, len(x[0])):\n",
    "        x_position = x[0][index]\n",
    "        y_position = x[1][index]\n",
    "        \n",
    "        input_np[x_position, y_position] = 0.0\n",
    "    input_np = np.nan_to_num(input_np)\n",
    "        \n",
    "    return input_np\n",
    "\n",
    "def update_progress(progress):\n",
    "    bar_length = 100\n",
    "    if isinstance(progress, int):\n",
    "        progress = float(progress)\n",
    "    if not isinstance(progress, float):\n",
    "        progress = 0\n",
    "    if progress < 0:\n",
    "        progress = 0\n",
    "    if progress >= 1:\n",
    "        progress = 1\n",
    "\n",
    "    block = int(round(bar_length * progress))\n",
    "    clear_output(wait = True)\n",
    "    \n",
    "    text = \"Progress: [{0}] {1:.1f}%\".format( \"#\" * block + \"-\" * (bar_length - block), progress * 100)\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_TESS = 'D://Datasets//TESS//PAD_REVERB_NOISE//Happy//'\n",
    "a_TESS = 'D://Datasets//TESS//PAD_REVERB_NOISE//Angry//'\n",
    "n_TESS = 'D://Datasets//TESS//PAD_REVERB_NOISE//Neutral//'\n",
    "s_TESS = 'D://Datasets//TESS//PAD_REVERB_NOISE//Sad//'\n",
    "\n",
    "h_EMO = 'D://Datasets//EMO-DB//wav//Happy//'\n",
    "a_EMO = 'D://Datasets//EMO-DB//wav//Angry//'\n",
    "n_EMO = 'D://Datasets//EMO-DB//wav//Neutral//'\n",
    "s_EMO = 'D://Datasets//EMO-DB//wav//Sad//'\n",
    "o_EMO = 'D://Datasets//EMO-DB//wav//Other//'\n",
    "\n",
    "test_Happy = 'D://Datasets//TRAINING//Happy_test//'\n",
    "test_Angry = 'D://Datasets//TRAINING//Angry_test//'\n",
    "test_Neutral = 'D://Datasets//TRAINING//Neutral_test//'\n",
    "test_Sad = 'D://Datasets//TRAINING//Sad_test//'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_top_conv(num_layers):\n",
    "    \n",
    "    n_neurons = 1024\n",
    "    n_batch = 128\n",
    "    n_epoch = 1000\n",
    "    nbindex = 512\n",
    "    fillength = 4\n",
    "    classes = 2\n",
    "    \n",
    "    #ofilepath = \"C://Users//yg9ca//Documents//3_Layer(s)//CNN_final_Top_1024_128_1000.hdf5\"\n",
    "\n",
    "    model=Sequential()\n",
    "    model.add(Convolution1D(nb_filter=nbindex, filter_length=fillength, activation='relu', input_shape=(frame_number, 272), kernel_constraint=maxnorm(3)))\n",
    "    model.add(MaxPooling1D(pool_size=2, strides=2, padding='same'))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    for layer in range(0, num_layers):\n",
    "        model.add(Convolution1D(nb_filter=nbindex, filter_length=fillength, activation='relu', kernel_constraint=maxnorm(3)))\n",
    "        model.add(MaxPooling1D(pool_size=2, strides=2, padding='same'))\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(n_neurons, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(classes, activation='sigmoid'))\n",
    "    model.compile(loss='mean_squared_error', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_function_FeatureExtract3(InputFolderName):\n",
    "    X = function_FeatureExtract1(InputFolderName, 272)\n",
    "    y_pred = omodel.predict(X)\n",
    "    #print(y_pred)\n",
    "    x = maxima(list(y_pred[0]))\n",
    "    return x\n",
    "\n",
    "def top_classifier_eval(emotionFolders):\n",
    "    print('Predicted with overall classifier: ' + ofilepath)\n",
    "\n",
    "    correct = 0\n",
    "    incorrect = 0\n",
    "    \n",
    "    for emotionFolder in emotionFolders:\n",
    "        if 'Happy' in emotionFolder: val = 0\n",
    "        elif 'Angry' in emotionFolder: val = 0\n",
    "        elif 'Neutral' in emotionFolder: val = 1\n",
    "        else: val = 1\n",
    "\n",
    "        for emotionfile in os.listdir(emotionFolder):\n",
    "\n",
    "            cond1 = '0_' not in emotionfile and '1_' not in emotionfile\n",
    "            cond2 = emotionfile[len(emotionfile)-4] != '-' and emotionfile[len(emotionfile)-6] != '-' \n",
    "            cond3 = len(emotionfile) != len('03-01-05-01-01-01-06-6-8-4')\n",
    "            cond4 = emotionfile[0] != '.'\n",
    "\n",
    "            if cond2 and cond3 and cond4 and cond1:\n",
    "                try:\n",
    "                    x = top_function_FeatureExtract3(\n",
    "                        InputFolderName=emotionFolder+emotionfile)\n",
    "                    if(x == val): correct += 1\n",
    "                    else: incorrect += 1\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "    return correct/(correct+incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#omodel = optimal_top_conv(3)\n",
    "#ofilepath = \"3_Layer(s)//CNN_final_Top_1024_128_1000.hdf5\"\n",
    "#omodel.load_weights(ofilepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "top_classifier_eval([test_Happy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "top_classifier_eval([test_Angry])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "top_classifier_eval([test_Neutral])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "top_classifier_eval([test_Sad])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Top classifier accuracy on unseen Happy data - 0.9774436090225563\n",
    "Top classifier accuracy on unseen Angry data - 0.984313725490196\n",
    "Top classifier accuracy on unseen Neutral data - 0.9518518518518518\n",
    "Top classifier accuracy on unseen Sad data - 0.8954248366013072"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Happy vs Angry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0th module\n",
      "C://Users//yg9ca//Documents//modules//Checkpoint_H_A_neurons_4096_filters_256_dropout_0.2_epoch_50000.hdf5\n",
      "1th module\n",
      "C://Users//yg9ca//Documents//modules//Final_H_A_neurons_4096_filters_256_dropout_0.15_epoch_50000.hdf5\n",
      "2th module\n",
      "C://Users//yg9ca//Documents//modules//Final_H_A_neurons_4096_filters_256_dropout_0.1_epoch_1000.hdf5\n",
      "3th module\n",
      "C://Users//yg9ca//Documents//modules//Final_H_A_neurons_4096_filters_256_dropout_0.2_epoch_50000.hdf5\n"
     ]
    }
   ],
   "source": [
    "directory = 'C://Users//yg9ca//Documents//modules//'\n",
    "module_prefix = 'H_A_'\n",
    "modules = avail_modules(directory, module_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_title = modules[0]\n",
    "#n_neurons, n_batch, n_epoch, nbindex, fillength, classes, dropout, dense_layers = extract_parameters(this_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hafilepath = this_title\n",
    "hamodel = keras.models.load_model(hafilepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h_a_function_FeatureExtract3(InputFolderName):\n",
    "    X = function_FeatureExtract1(InputFolderName, NumofFeaturetoUse)\n",
    "    print(X)\n",
    "    y_pred = hamodel.predict(X)\n",
    "    print(y_pred)\n",
    "    x = maxima(list(y_pred[0]))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h_a_classifier_eval(emotionFolders):\n",
    "    print('Predicted with overall classifier: ' + hafilepath)\n",
    "\n",
    "    correct = 0\n",
    "    incorrect = 0\n",
    "    for emotionFolder in emotionFolders:\n",
    "\n",
    "        if 'Happy' in emotionFolder: val = 0\n",
    "        elif 'Angry' in emotionFolder: val = 1\n",
    "\n",
    "        for emotionfile in os.listdir(emotionFolder):\n",
    "            cond1 = 'deamp_' not in emotionfile and 'WetDry_' not in emotionfile\n",
    "            cond2 = 'noise' not in emotionfile\n",
    "            cond4 = emotionfile[0] != '.'\n",
    "            if cond2 and cond4 and cond1:\n",
    "                # print(correct+incorrect)\n",
    "                x = h_a_function_FeatureExtract3(InputFolderName=emotionFolder+emotionfile)\n",
    "                if(x == val): correct += 1\n",
    "                else: incorrect += 1\n",
    "        \n",
    "    return correct/(correct+incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted with overall classifier: C://Users//yg9ca//Documents//modules//Checkpoint_H_A_neurons_4096_filters_256_dropout_0.2_epoch_50000.hdf5\n",
      "None\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "If predicting from data tensors, you should specify the `steps` argument.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-98-8a13021f9def>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mh_a_classifier_eval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_Angry\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-97-46a1f1aa30d7>\u001b[0m in \u001b[0;36mh_a_classifier_eval\u001b[1;34m(emotionFolders)\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcond2\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcond4\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcond1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m                 \u001b[1;31m# print(correct+incorrect)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m                 \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh_a_function_FeatureExtract3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mInputFolderName\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0memotionFolder\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0memotionfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m                 \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcorrect\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mincorrect\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-96-984706e522b2>\u001b[0m in \u001b[0;36mh_a_function_FeatureExtract3\u001b[1;34m(InputFolderName)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunction_FeatureExtract1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mInputFolderName\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNumofFeaturetoUse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhamodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmaxima\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1373\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1374\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0msteps\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1375\u001b[1;33m             raise ValueError('If predicting from data tensors, '\n\u001b[0m\u001b[0;32m   1376\u001b[0m                              \u001b[1;34m'you should specify the `steps` '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1377\u001b[0m                              'argument.')\n",
      "\u001b[1;31mValueError\u001b[0m: If predicting from data tensors, you should specify the `steps` argument."
     ]
    }
   ],
   "source": [
    "h_a_classifier_eval([test_Angry])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_a_classifier_eval([test_Happy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
