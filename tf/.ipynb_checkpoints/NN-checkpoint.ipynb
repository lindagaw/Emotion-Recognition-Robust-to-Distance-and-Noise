{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import gc\n",
    "import sys\n",
    "import h5py\n",
    "import time\n",
    "import datetime\n",
    "import pickle\n",
    "import librosa\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import pandas as pd\n",
    "from pandas.plotting import parallel_coordinates\n",
    "from pydub import AudioSegment\n",
    "\n",
    "#imported for testing\n",
    "import wave\n",
    "import contextlib\n",
    "\n",
    "# for outputing file\n",
    "from scipy.cluster.vq import vq, kmeans, whiten\n",
    "import scipy.stats.stats as st\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.decomposition import PCA as sklearnPCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import auc, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn import mixture\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import keras\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.constraints import maxnorm\n",
    "from keras.layers import Convolution1D, Dense, MaxPooling1D, Flatten, Add, Dropout, Input, Activation\n",
    "from keras.layers import TimeDistributed, Bidirectional, LSTM, LeakyReLU\n",
    "from keras.models import Sequential, save_model\n",
    "from keras import optimizers, regularizers\n",
    "from keras.utils import np_utils, to_categorical\n",
    "from keras import metrics\n",
    "\n",
    "from colorama import Fore, Back, Style\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "#warnings.filterwarnings('ignore')\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# confirm TensorFlow sees the GPU\n",
    "from tensorflow.python.client import device_lib\n",
    "# assert 'GPU' in str(device_lib.list_local_devices())\n",
    "\n",
    "# confirm Keras sees the GPU\n",
    "from keras import backend\n",
    "# print(len(backend.tensorflow_backend._get_available_gpus()) > 0)\n",
    "\n",
    "#warnings.filterwarnings('ignore')\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "sys.path.insert(1, '..//components//')\n",
    "import load_feat_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_X(x_directory, label):\n",
    "    X = np.array([])\n",
    "    Y = []\n",
    "    for file in os.listdir(x_directory):\n",
    "        if file.endswith('.npy'):\n",
    "            x = np.load(x_directory + file)\n",
    "            x = np.array([list(x)])\n",
    "            if len(X) == 0:\n",
    "                X = x\n",
    "            else:\n",
    "                X = np.vstack((X,x))\n",
    "            \n",
    "            Y.append(label)\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "CaFE_happy = 'D://Datasets//CaFE//npy//Happy//two//'\n",
    "CaFE_angry = 'D://Datasets//CaFE//npy//Angry//two//'\n",
    "CaFE_neutral = 'D://Datasets//CaFE//npy//Neutral//two//'\n",
    "CaFE_sad = 'D://Datasets//CaFE//npy//Sad//two//'\n",
    "\n",
    "CaFE_npy = [CaFE_happy, CaFE_angry, CaFE_neutral, CaFE_sad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StandardScaler(copy=True, with_mean=True, with_std=True)\n"
     ]
    }
   ],
   "source": [
    "X_happy, Y_happy = load_X(CaFE_npy[0], 0)\n",
    "X_angry, Y_angry = load_X(CaFE_npy[1], 0)\n",
    "X_neutral, Y_neutral = load_X(CaFE_npy[2], 1)\n",
    "X_sad, Y_sad = load_X(CaFE_npy[3], 1)\n",
    "\n",
    "Xs = [X_happy, X_angry, X_neutral, X_sad]\n",
    "Ys = [Y_happy, Y_angry, Y_neutral, Y_sad]\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "for item in Xs:\n",
    "    if len(X) == 0: X = item\n",
    "    else: X = np.vstack((X,item))\n",
    "\n",
    "for item in Ys:\n",
    "    Y = Y + item\n",
    "Label = to_categorical(np.array(Y))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "print(scaler.fit(X))\n",
    "featureSet = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " ...\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = optimizers.Adam(lr = 5e-3, beta_1 = 0.9, beta_2 = 0.999, epsilon = None, decay = 0, amsgrad = True)\n",
    "sgd = optimizers.SGD(lr = 0.01, decay = 1e-6, momentum = 0.9, nesterov = True)\n",
    "rmsprop = optimizers.RMSprop(lr = 0.0001, rho = 0.9, epsilon = None, decay = 0.0)\n",
    "adagrad = optimizers.Adagrad(lr = 0.01, epsilon = None, decay = 0.0)\n",
    "adadelta = optimizers.Adadelta(lr = 1.0, rho = 0.95, epsilon = None, decay = 0.0)\n",
    "adamax = optimizers.Adamax(lr = 0.002, beta_1 = 0.9, beta_2 = 0.999, epsilon = None, decay = 0.0)\n",
    "nadam = optimizers.Nadam(lr = 0.002, beta_1 = 0.9, beta_2 = 0.999, epsilon = None, schedule_decay = 0.004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wall(block):\n",
    "    wall = np.zeros((10,10),dtype=np.float)\n",
    "    #block = np.arange(1,7).reshape(2,3)\n",
    "\n",
    "    x = 2\n",
    "    y = 3\n",
    "    wall[x:x+block.shape[0], y:y+block.shape[1]] = block\n",
    "    \n",
    "    return wall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn(title, num_layers, n_neurons, n_batch, nbindex, dropout, classes, dense_layers):\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    #model.add(Convolution1D(nb_filter=nbindex, filter_length=fillength, activation = 'relu', input_shape=(1, 2), kernel_constraint=maxnorm(2)))\n",
    "    #model.add(MaxPooling1D(pool_size=2, strides=2, padding='same'))\n",
    "    #model.add(Dropout(dropout))\n",
    "    #model.add(Flatten())\n",
    "    \n",
    "    for i in range(0, dense_layers):\n",
    "        model.add(Dense(n_neurons, input_dim=2, activation='relu'))\n",
    "        model.add(Dropout(dropout))\n",
    "    \n",
    "    model.add(Dense(classes, activation='softmax'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam,\n",
    "                  metrics=[metrics.categorical_accuracy])\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cnn(prefix):\n",
    "    \n",
    "    save_to_path = prefix\n",
    "\n",
    "    checkpoint_filepath = prefix + \"Checkpoint_\" + title + \".hdf5\"\n",
    "    final_filepath = prefix + \"Final_\" + title + \".hdf5\"\n",
    "\n",
    "    if not os.path.exists(save_to_path):\n",
    "        os.mkdir(save_to_path)\n",
    "\n",
    "    X, X_test, Y, Y_test= train_test_split(featureSet, Label, test_size = 0.25, shuffle = True)\n",
    "    model = create_cnn(title, num_layers, n_neurons, n_batch, nbindex, dropout, classes, dense_layers)\n",
    "    checkpoint = ModelCheckpoint(checkpoint_filepath, monitor = 'val_categorical_accuracy', verbose = 1, save_best_only = True, mode = 'auto')\n",
    "    early_stopping_monitor = EarlyStopping(patience = 10)\n",
    "    callbacks_list = [checkpoint, early_stopping_monitor]\n",
    "    model.fit(X, Y, nb_epoch = n_epoch, batch_size = n_batch,  callbacks = callbacks_list, validation_data = (X_test, Y_test), verbose = 1)\n",
    "    model.save(final_filepath)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_48 (Dense)             (None, 32)                96        \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 162\n",
      "Trainable params: 162\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yg9ca\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1296 samples, validate on 432 samples\n",
      "Epoch 1/1000\n",
      "1296/1296 [==============================] - 1s 617us/step - loss: 0.6696 - categorical_accuracy: 0.6003 - val_loss: 0.6593 - val_categorical_accuracy: 0.6389\n",
      "\n",
      "Epoch 00001: val_categorical_accuracy improved from -inf to 0.63889, saving model to ..//..//modules//tf//Checkpoint_Top_neurons_32_filters_256_dropout_0.2_epoch_1000.hdf5\n",
      "Epoch 2/1000\n",
      "1296/1296 [==============================] - 0s 47us/step - loss: 0.6726 - categorical_accuracy: 0.6034 - val_loss: 0.6447 - val_categorical_accuracy: 0.6435\n",
      "\n",
      "Epoch 00002: val_categorical_accuracy improved from 0.63889 to 0.64352, saving model to ..//..//modules//tf//Checkpoint_Top_neurons_32_filters_256_dropout_0.2_epoch_1000.hdf5\n",
      "Epoch 3/1000\n",
      "1296/1296 [==============================] - 0s 49us/step - loss: 0.6672 - categorical_accuracy: 0.6165 - val_loss: 0.6429 - val_categorical_accuracy: 0.6435\n",
      "\n",
      "Epoch 00003: val_categorical_accuracy did not improve from 0.64352\n",
      "Epoch 4/1000\n",
      "1296/1296 [==============================] - 0s 47us/step - loss: 0.6703 - categorical_accuracy: 0.6042 - val_loss: 0.6435 - val_categorical_accuracy: 0.6435\n",
      "\n",
      "Epoch 00004: val_categorical_accuracy did not improve from 0.64352\n",
      "Epoch 5/1000\n",
      "1296/1296 [==============================] - 0s 45us/step - loss: 0.6689 - categorical_accuracy: 0.6157 - val_loss: 0.6429 - val_categorical_accuracy: 0.6481\n",
      "\n",
      "Epoch 00005: val_categorical_accuracy improved from 0.64352 to 0.64815, saving model to ..//..//modules//tf//Checkpoint_Top_neurons_32_filters_256_dropout_0.2_epoch_1000.hdf5\n",
      "Epoch 6/1000\n",
      "1296/1296 [==============================] - 0s 45us/step - loss: 0.6633 - categorical_accuracy: 0.6134 - val_loss: 0.6511 - val_categorical_accuracy: 0.6458\n",
      "\n",
      "Epoch 00006: val_categorical_accuracy did not improve from 0.64815\n",
      "Epoch 7/1000\n",
      "1296/1296 [==============================] - 0s 43us/step - loss: 0.6637 - categorical_accuracy: 0.6119 - val_loss: 0.6494 - val_categorical_accuracy: 0.6343\n",
      "\n",
      "Epoch 00007: val_categorical_accuracy did not improve from 0.64815\n",
      "Epoch 8/1000\n",
      "1296/1296 [==============================] - 0s 47us/step - loss: 0.6673 - categorical_accuracy: 0.6119 - val_loss: 0.6441 - val_categorical_accuracy: 0.6412\n",
      "\n",
      "Epoch 00008: val_categorical_accuracy did not improve from 0.64815\n",
      "Epoch 9/1000\n",
      "1296/1296 [==============================] - 0s 42us/step - loss: 0.6659 - categorical_accuracy: 0.6127 - val_loss: 0.6459 - val_categorical_accuracy: 0.6481\n",
      "\n",
      "Epoch 00009: val_categorical_accuracy did not improve from 0.64815\n",
      "Epoch 10/1000\n",
      "1296/1296 [==============================] - 0s 51us/step - loss: 0.6622 - categorical_accuracy: 0.6150 - val_loss: 0.6496 - val_categorical_accuracy: 0.6343\n",
      "\n",
      "Epoch 00010: val_categorical_accuracy did not improve from 0.64815\n",
      "Epoch 11/1000\n",
      "1296/1296 [==============================] - 0s 47us/step - loss: 0.6637 - categorical_accuracy: 0.6181 - val_loss: 0.6427 - val_categorical_accuracy: 0.6412\n",
      "\n",
      "Epoch 00011: val_categorical_accuracy did not improve from 0.64815\n",
      "Epoch 12/1000\n",
      "1296/1296 [==============================] - 0s 45us/step - loss: 0.6666 - categorical_accuracy: 0.6157 - val_loss: 0.6414 - val_categorical_accuracy: 0.6458\n",
      "\n",
      "Epoch 00012: val_categorical_accuracy did not improve from 0.64815\n",
      "Epoch 13/1000\n",
      "1296/1296 [==============================] - 0s 46us/step - loss: 0.6661 - categorical_accuracy: 0.6034 - val_loss: 0.6508 - val_categorical_accuracy: 0.6366\n",
      "\n",
      "Epoch 00013: val_categorical_accuracy did not improve from 0.64815\n",
      "Epoch 14/1000\n",
      "1296/1296 [==============================] - 0s 47us/step - loss: 0.6653 - categorical_accuracy: 0.6134 - val_loss: 0.6425 - val_categorical_accuracy: 0.6366\n",
      "\n",
      "Epoch 00014: val_categorical_accuracy did not improve from 0.64815\n",
      "Epoch 15/1000\n",
      "1296/1296 [==============================] - 0s 48us/step - loss: 0.6669 - categorical_accuracy: 0.6181 - val_loss: 0.6479 - val_categorical_accuracy: 0.6343\n",
      "\n",
      "Epoch 00015: val_categorical_accuracy did not improve from 0.64815\n",
      "Epoch 16/1000\n",
      "1296/1296 [==============================] - 0s 48us/step - loss: 0.6643 - categorical_accuracy: 0.6019 - val_loss: 0.6420 - val_categorical_accuracy: 0.6412\n",
      "\n",
      "Epoch 00016: val_categorical_accuracy did not improve from 0.64815\n",
      "Epoch 17/1000\n",
      "1296/1296 [==============================] - 0s 52us/step - loss: 0.6642 - categorical_accuracy: 0.6196 - val_loss: 0.6446 - val_categorical_accuracy: 0.6389\n",
      "\n",
      "Epoch 00017: val_categorical_accuracy did not improve from 0.64815\n",
      "Epoch 18/1000\n",
      "1296/1296 [==============================] - 0s 46us/step - loss: 0.6616 - categorical_accuracy: 0.6157 - val_loss: 0.6474 - val_categorical_accuracy: 0.6343\n",
      "\n",
      "Epoch 00018: val_categorical_accuracy did not improve from 0.64815\n",
      "Epoch 19/1000\n",
      "1296/1296 [==============================] - 0s 46us/step - loss: 0.6640 - categorical_accuracy: 0.6142 - val_loss: 0.6460 - val_categorical_accuracy: 0.6366\n",
      "\n",
      "Epoch 00019: val_categorical_accuracy did not improve from 0.64815\n",
      "Epoch 20/1000\n",
      "1296/1296 [==============================] - 0s 42us/step - loss: 0.6653 - categorical_accuracy: 0.6142 - val_loss: 0.6427 - val_categorical_accuracy: 0.6412\n",
      "\n",
      "Epoch 00020: val_categorical_accuracy did not improve from 0.64815\n",
      "Epoch 21/1000\n",
      "1296/1296 [==============================] - 0s 49us/step - loss: 0.6640 - categorical_accuracy: 0.6157 - val_loss: 0.6437 - val_categorical_accuracy: 0.6389\n",
      "\n",
      "Epoch 00021: val_categorical_accuracy did not improve from 0.64815\n",
      "Epoch 22/1000\n",
      "1296/1296 [==============================] - 0s 50us/step - loss: 0.6624 - categorical_accuracy: 0.6165 - val_loss: 0.6433 - val_categorical_accuracy: 0.6389\n",
      "\n",
      "Epoch 00022: val_categorical_accuracy did not improve from 0.64815\n"
     ]
    }
   ],
   "source": [
    "classes = 2\n",
    "n_neurons = 32\n",
    "dense_layers = 1\n",
    "num_layers = 1\n",
    "fillength = 1\n",
    "nbindex = 256\n",
    "dropout = 0.2\n",
    "n_batch = 32\n",
    "n_epoch = 1000\n",
    "title = 'Top_neurons_' + str(n_neurons) + '_filters_' + str(nbindex) + '_dropout_' + str(dropout) + '_epoch_' + str(n_epoch)\n",
    "prefix = '..//..//modules//tf//'\n",
    "final_filepath = prefix + \"Final_\" + title + \".hdf5\"\n",
    "\n",
    "model = train_cnn(prefix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
