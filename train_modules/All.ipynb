{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import gc\n",
    "import sys\n",
    "import h5py\n",
    "import time\n",
    "import datetime\n",
    "import pickle\n",
    "import librosa\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import pandas as pd\n",
    "from pandas.plotting import parallel_coordinates\n",
    "from pydub import AudioSegment\n",
    "\n",
    "#imported for testing\n",
    "import wave\n",
    "import contextlib\n",
    "\n",
    "# for outputing file\n",
    "from scipy.cluster.vq import vq, kmeans, whiten\n",
    "import scipy.stats.stats as st\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.decomposition import PCA as sklearnPCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import auc, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn import mixture\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "import keras\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.constraints import maxnorm\n",
    "from keras.layers import Convolution1D, Dense, MaxPooling1D, Flatten, Add, Dropout, Input, Activation\n",
    "from keras.layers import TimeDistributed, Bidirectional, LSTM, LeakyReLU\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers, regularizers\n",
    "from keras.utils import np_utils, to_categorical\n",
    "\n",
    "#from colorama import Fore, Back, Style\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "#warnings.filterwarnings('ignore')\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# confirm TensorFlow sees the GPU\n",
    "from tensorflow.python.client import device_lib\n",
    "# assert 'GPU' in str(device_lib.list_local_devices())\n",
    "\n",
    "# confirm Keras sees the GPU\n",
    "from keras import backend\n",
    "# print(len(backend.tensorflow_backend._get_available_gpus()) > 0)\n",
    "\n",
    "#warnings.filterwarnings('ignore')\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "sys.path.insert(1, '..//components//')\n",
    "import load_feat_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def empirical_class_mean(X, Y, c, func, penultimate_neuron_num):\n",
    "    \"\"\"\n",
    "    X is of shame (batch, num_of_small_segs, feats_in_one_seg).\n",
    "    Y is of shape (batch, class).\n",
    "    c is a class, of type integer.\n",
    "    func is the output of the penultimate layer of DNNs. \n",
    "    func should be called as func(x), s.t. x is one xample in X.\n",
    "    \"\"\"\n",
    "    N_c = 0\n",
    "    for item in Y:\n",
    "        if np.amax(item) == c:\n",
    "            N_c += 1\n",
    "    sum_func = np.array([0] * penultimate_neuron_num)\n",
    "    for index in range(0, len(Y)):\n",
    "        if Y[index] == c:\n",
    "            sum_func = np.add(func(X[index]), sum_func)\n",
    "    return sum_func / N_c\n",
    "\n",
    "\n",
    "def empirical_class_means(X, Y, C, func, penultimate_neuron_num):\n",
    "    \"\"\"\n",
    "    Calculate the empirical class means for all classes and output a numpy array.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for c in C:\n",
    "        miu = empirical_class_mean(X, Y, c, func, penultimate_neuron_num)\n",
    "        result.append(miu)\n",
    "        \n",
    "    return np.array(result)\n",
    "\n",
    "def emprical_covariance(X, Y, C, func, penultimate_neuron_num):\n",
    "    assert len(X) == len(Y)\n",
    "    N = len(Y)\n",
    "    \n",
    "    mius = empirical_class_means(X, Y, C, func, penultimate_neuron_num)\n",
    "    miu = mius[0]\n",
    "    \n",
    "    sum_all_classes = np.array(miu.shape)\n",
    "    \n",
    "    print(mius)\n",
    "    \n",
    "    for c in C:\n",
    "        sum_single_c = np.array(miu.shape)\n",
    "        for index in range(0, len(Y)):\n",
    "            if Y[index] == c:\n",
    "                miu = mius[c]\n",
    "                difference = func(X[index]) - miu\n",
    "                transpose = np.transpose(difference)\n",
    "                result = difference * transpose\n",
    "                sum_single_c = np.add(sum_single_c, result)\n",
    "        \n",
    "        sum_all_classes = np.add(sum_all_classes, sum_single_c)\n",
    "        \n",
    "    covariance = sum_all_classes/N\n",
    "    \n",
    "    return covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate = 44100\n",
    "frame_number = 48\n",
    "hop_length = 441  # frame size= 2 * hop\n",
    "segment_length = int(sample_rate * 0.2)  # 0.2\n",
    "segment_pad = int(sample_rate * 0.02)     # 0.02\n",
    "overlapping = int(sample_rate * 0.1)   # 0.1\n",
    "\n",
    "classes = 5\n",
    "NumofFeaturetoUse = 272\n",
    "n_neurons = 1024 * 4\n",
    "dense_layers = 10\n",
    "num_layers = 3\n",
    "fillength = 3\n",
    "nbindex = 1024 * 3\n",
    "dropout = 0.2\n",
    "n_batch = 128\n",
    "n_epoch = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_noised_npy = load_feat_directories.allnoised_npy\n",
    "all_noised_npy_test = load_feat_directories.allnoised_npy_test\n",
    "home_noised_npy = load_feat_directories.homenoised_npy\n",
    "home_noised_npy_test = load_feat_directories.homenoised_npy_test\n",
    "\n",
    "for index in range(0, 5):\n",
    "    if not os.path.exists(all_noised_npy[index]):\n",
    "        print(all_noised_npy[index] + ' does not exist. Breaking the loop... ')\n",
    "        break\n",
    "\n",
    "    if not os.path.exists(home_noised_npy[index]):\n",
    "        print(home_noised_npy[index] + 'does not exist. Breaking the loop... ')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprise_vector(path):\n",
    "    vec_to_return = []\n",
    "    for fname in os.listdir(path):\n",
    "        current_vec = np.load(path + fname)\n",
    "        vec_to_return.append(current_vec)\n",
    "\n",
    "    vec_to_return = np.array(vec_to_return)\n",
    "    return vec_to_return\n",
    "\n",
    "def comprise_label(feature_vector, label):\n",
    "    label_vec_to_ret = []\n",
    "    length = len(list(feature_vector))\n",
    "    for index in range(0, length):\n",
    "        current_label = [label]\n",
    "        label_vec_to_ret.append(current_label)\n",
    "    label_vec_to_ret = np.array(label_vec_to_ret)\n",
    "\n",
    "    return label_vec_to_ret\n",
    "\n",
    "def float_compatible(input_np):\n",
    "    x = np.where(input_np >= np.finfo(np.float32).max)\n",
    "    for index in range(0, len(x[0])):\n",
    "        x_position = x[0][index]\n",
    "        y_position = x[1][index]\n",
    "        input_np[x_position, y_position] = 0.0\n",
    "    input_np = np.nan_to_num(input_np)\n",
    "\n",
    "    return input_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..//..//Datasets//padded_deamplified_homenoised_reverberated//npy//Happy_npy//\n",
      "..//..//Datasets//padded_deamplified_homenoised_reverberated//npy//Angry_npy//\n",
      "..//..//Datasets//padded_deamplified_homenoised_reverberated//npy//Neutral_npy//\n",
      "..//..//Datasets//padded_deamplified_homenoised_reverberated//npy//Sad_npy//\n",
      "..//..//Datasets//padded_deamplified_homenoised_reverberated//npy//Other_npy//\n"
     ]
    }
   ],
   "source": [
    "for index in [0, 1, 2, 3, 4]:\n",
    "    if not os.path.exists(home_noised_npy[index]):\n",
    "        print(home_noised_npy[index] + 'does not exist.')\n",
    "    else:\n",
    "        path = home_noised_npy[index]\n",
    "        print(path)\n",
    "        if index == 0:\n",
    "            h_feature_vector_home = comprise_vector(path)\n",
    "            h_label_vector_home = comprise_label(h_feature_vector_home, index)\n",
    "        elif index == 1:\n",
    "            a_feature_vector_home = comprise_vector(path)\n",
    "            a_label_vector_home = comprise_label(a_feature_vector_home, index)\n",
    "        elif index == 2:\n",
    "            n_feature_vector_home = comprise_vector(path)\n",
    "            n_label_vector_home = comprise_label(n_feature_vector_home, index)\n",
    "        elif index == 3:\n",
    "            s_feature_vector_home = comprise_vector(path)\n",
    "            s_label_vector_home = comprise_label(s_feature_vector_home, index)\n",
    "        else:\n",
    "            o_feature_vector_home = comprise_vector(path)\n",
    "            o_label_vector_home = comprise_label(o_feature_vector_home, index)\n",
    "\n",
    "for index in [0, 1, 2, 3, 4]:\n",
    "    \n",
    "    if not os.path.exists(home_noised_npy_test[index]):\n",
    "        print(home_noised_npy_test[index] + 'does not exist.')\n",
    "    else:\n",
    "        path = home_noised_npy_test[index]\n",
    "        print(path)\n",
    "        if index == 0:\n",
    "            h_feature_vector_home_test = comprise_vector(path)\n",
    "            h_label_vector_home_test = comprise_label(h_feature_vector_home_test, index)\n",
    "        elif index == 1:\n",
    "            a_feature_vector_home_test = comprise_vector(path)\n",
    "            a_label_vector_home_test = comprise_label(a_feature_vector_home_test, index)\n",
    "        elif index == 2:\n",
    "            n_feature_vector_home_test = comprise_vector(path)\n",
    "            n_label_vector_home_test = comprise_label(n_feature_vector_home_test, index)\n",
    "        elif index == 3:\n",
    "            s_feature_vector_home_test = comprise_vector(path)\n",
    "            s_label_vector_home_test = comprise_label(s_feature_vector_home_test, index)\n",
    "        else:\n",
    "            o_feature_vector_home_test = comprise_vector(path)\n",
    "            o_label_vector_home_test = comprise_label(o_feature_vector_home_test, index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data: (39916, 48, 272)\n",
      "training label: (39916, 5)\n",
      "evaluation data: (8843, 48, 272)\n",
      "evaluation label: (8843, 5)\n"
     ]
    }
   ],
   "source": [
    "# Load training npy files\n",
    "featureSet = float_compatible(np.vstack((h_feature_vector_home, a_feature_vector_home, n_feature_vector_home, s_feature_vector_home, o_feature_vector_home)))\n",
    "Label = (np.vstack((h_label_vector_home, a_label_vector_home, n_label_vector_home, s_label_vector_home, o_label_vector_home)))\n",
    "\n",
    "Label[Label == 0] = 0\n",
    "Label[Label == 1] = 1\n",
    "Label[Label == 2] = 2\n",
    "Label[Label == 3] = 3\n",
    "Label[Label == 4] = 4\n",
    "\n",
    "Label = to_categorical(Label)\n",
    "#featureSet = np.split(featureSet, np.array([NumofFeaturetoUse]), axis = 2)[0]\n",
    "print('training data: ' + str(featureSet.shape))\n",
    "print('training label: ' + str(Label.shape))\n",
    "\n",
    "# Load testing npy files\n",
    "featureSet_val = float_compatible(np.vstack((h_feature_vector_home_test, a_feature_vector_home_test, n_feature_vector_home_test, s_feature_vector_home_test, o_feature_vector_home_test)))\n",
    "Label_val = (np.vstack((h_label_vector_home_test, a_label_vector_home_test, n_label_vector_home_test, s_label_vector_home_test, o_label_vector_home_test)))\n",
    "\n",
    "Label_val[Label_val == 0] = 0\n",
    "Label_val[Label_val == 1] = 1\n",
    "Label_val[Label_val == 2] = 2\n",
    "Label_val[Label_val == 3] = 3\n",
    "Label_val[Label_val == 4] = 4\n",
    "\n",
    "Label_val = to_categorical(Label_val)\n",
    "#featureSet_val = np.split(featureSet_val, np.array([NumofFeaturetoUse]), axis = 2)[0]\n",
    "print('evaluation data: ' + str(featureSet_val.shape))\n",
    "print('evaluation label: ' + str(Label_val.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = optimizers.Adam(lr = 1e-4, beta_1 = 0.9, beta_2 = 0.999, epsilon = None, decay = 0, amsgrad = True)\n",
    "sgd = optimizers.SGD(lr = 0.01, decay = 1e-6, momentum = 0.9, nesterov = True)\n",
    "rmsprop = optimizers.RMSprop(lr = 0.0001, rho = 0.9, epsilon = None, decay = 0.0)\n",
    "adagrad = optimizers.Adagrad(lr = 0.01, epsilon = None, decay = 0.0)\n",
    "adadelta = optimizers.Adadelta(lr = 1.0, rho = 0.95, epsilon = None, decay = 0.0)\n",
    "adamax = optimizers.Adamax(lr = 0.002, beta_1 = 0.9, beta_2 = 0.999, epsilon = None, decay = 0.0)\n",
    "nadam = optimizers.Nadam(lr = 0.002, beta_1 = 0.9, beta_2 = 0.999, epsilon = None, schedule_decay = 0.004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record(str_message, log_file):\n",
    "    str_message = str_message + '\\n'\n",
    "    file = open(log_file, 'a')\n",
    "    file.write(str_message)\n",
    "    file.close()\n",
    "\n",
    "def create_cnn(title, num_layers, n_neurons, n_batch, nbindex, dropout, classes, dense_layers):\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Convolution1D(nb_filter=nbindex, filter_length=fillength, activation = 'relu',\n",
    "                            input_shape=(featureSet.shape[1], featureSet.shape[2]), kernel_constraint=maxnorm(3)))\n",
    "    model.add(MaxPooling1D(pool_size=2, strides=2, padding='valid'))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(Convolution1D(nb_filter=nbindex*2, filter_length=fillength, activation = 'relu',\n",
    "                            kernel_constraint=maxnorm(3)))\n",
    "    model.add(MaxPooling1D(pool_size=2, strides=2, padding='valid'))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(Convolution1D(nb_filter=nbindex*2, filter_length=fillength, activation = 'relu',\n",
    "                            kernel_constraint=maxnorm(3)))\n",
    "    model.add(MaxPooling1D(pool_size=2, strides=2, padding='valid'))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(Convolution1D(nb_filter=nbindex, filter_length=fillength, activation = 'relu',\n",
    "                            kernel_constraint=maxnorm(3)))  \n",
    "    model.add(MaxPooling1D(pool_size=2, strides=2, padding='valid'))\n",
    "    model.add(Dropout(dropout))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    for i in range(0, dense_layers):\n",
    "        model.add(Dense(n_neurons, activation = 'relu'))\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(Dense(classes, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=adam,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cnn(prefix, source):\n",
    "    \n",
    "    save_to_path = prefix\n",
    "\n",
    "    checkpoint_filepath = prefix + \"Checkpoint_\" + source + title + \".hdf5\"\n",
    "    final_filepath = prefix + \"Final_\" + source + title + \".hdf5\"\n",
    "\n",
    "    if not os.path.exists(save_to_path):\n",
    "        os.mkdir(save_to_path)\n",
    "\n",
    "    X, X_test, Y, Y_test= train_test_split(featureSet, Label, test_size = 0.25, shuffle = True)\n",
    "    model = create_cnn(title, num_layers, n_neurons, n_batch, nbindex, dropout, classes, dense_layers)\n",
    "    checkpoint = ModelCheckpoint(checkpoint_filepath, monitor = 'val_acc', verbose = 0, save_best_only = True, mode = 'auto')\n",
    "    early_stopping_monitor = EarlyStopping(patience = 50)\n",
    "    callbacks_list = [checkpoint, early_stopping_monitor]\n",
    "    model.fit(X, Y, nb_epoch = n_epoch, batch_size = n_batch,  callbacks = callbacks_list, validation_data = (X_test, Y_test), verbose = 1)\n",
    "    #model.save_weights(final_filepath)\n",
    "    #model.load(checkpoint_filepath)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_index(a): \n",
    "    a = list(a)\n",
    "    # inbuilt function to find the position of minimum  \n",
    "    minpos = a.index(min(a))  \n",
    "    # inbuilt function to find the position of maximum  \n",
    "    maxpos = a.index(max(a))    \n",
    "    return maxpos\n",
    "\n",
    "def predict_cnn(model):\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    for item in list(Label_val):\n",
    "            y_true.append(max_index(item))\n",
    "\n",
    "    for item in list(model.predict(featureSet_val)):\n",
    "            y_pred.append(max_index(item))\n",
    "\n",
    "    print('Accuracy: ' + str(accuracy_score(y_true, y_pred)))\n",
    "    #print('Precision: ' + str(precision_score(y_true, y_pred)))\n",
    "    #print('Recall: ' + str(recall_score(y_true, y_pred)))\n",
    "    #print('f1 score: ' + str(f1_score(y_true, y_pred)))\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    \n",
    "    # Plot non-normalized confusion matrix\n",
    "    #plot_confusion_matrix(y_true, y_pred, classes=[0, 1, 2, 3, 4],\n",
    "    #                  title='Confusion matrix, without normalization')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yg9ca/.conda/envs/myenv/lib/python3.7/site-packages/ipykernel_launcher.py:12: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", input_shape=(48, 272), kernel_constraint=<keras.con..., filters=3072, kernel_size=3)`\n",
      "  if sys.path[0] == '':\n",
      "/home/yg9ca/.conda/envs/myenv/lib/python3.7/site-packages/ipykernel_launcher.py:17: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", kernel_constraint=<keras.con..., filters=6144, kernel_size=3)`\n",
      "/home/yg9ca/.conda/envs/myenv/lib/python3.7/site-packages/ipykernel_launcher.py:22: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", kernel_constraint=<keras.con..., filters=6144, kernel_size=3)`\n",
      "/home/yg9ca/.conda/envs/myenv/lib/python3.7/site-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", kernel_constraint=<keras.con..., filters=3072, kernel_size=3)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_5 (Conv1D)            (None, 46, 3072)          2509824   \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 23, 3072)          0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 23, 3072)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 21, 6144)          56629248  \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 10, 6144)          0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 10, 6144)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 8, 6144)           113252352 \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 4, 6144)           0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 4, 6144)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 2, 3072)           56626176  \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 1, 3072)           0         \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 1, 3072)           0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 4096)              12587008  \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 5)                 20485     \n",
      "=================================================================\n",
      "Total params: 392,656,901\n",
      "Trainable params: 392,656,901\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "title = 'All_neurons_home_' + str(n_neurons) + '_filters_' + str(nbindex) + '_dropout_' + str(dropout) + '_epoch_' + str(n_epoch) + '_dense_' + str(dense_layers)\n",
    "\n",
    "prefix = '..//..//modules//'\n",
    "source = 'home_'\n",
    "\n",
    "if not os.path.exists(prefix):\n",
    "    os.mkdir(prefix)\n",
    "\n",
    "final_filepath = prefix + \"Checkpoint_\" + source + title + \".hdf5\"\n",
    "model = create_cnn(title, num_layers, n_neurons, n_batch, nbindex, dropout, classes, dense_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yg9ca/.conda/envs/myenv/lib/python3.7/site-packages/ipykernel_launcher.py:12: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", input_shape=(48, 272), kernel_constraint=<keras.con..., filters=3072, kernel_size=3)`\n",
      "  if sys.path[0] == '':\n",
      "/home/yg9ca/.conda/envs/myenv/lib/python3.7/site-packages/ipykernel_launcher.py:17: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", kernel_constraint=<keras.con..., filters=6144, kernel_size=3)`\n",
      "/home/yg9ca/.conda/envs/myenv/lib/python3.7/site-packages/ipykernel_launcher.py:22: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", kernel_constraint=<keras.con..., filters=6144, kernel_size=3)`\n",
      "/home/yg9ca/.conda/envs/myenv/lib/python3.7/site-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", kernel_constraint=<keras.con..., filters=3072, kernel_size=3)`\n",
      "/home/yg9ca/.conda/envs/myenv/lib/python3.7/site-packages/ipykernel_launcher.py:16: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "original = sys.stdout\n",
    "sys.stdout = open(prefix + \"Checkpoint_\" + source + title + \".txt\", 'w')\n",
    "\n",
    "model = train_cnn(prefix, source)\n",
    "\n",
    "sys.stdout = original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(prefix + \"Checkpoint_\" + source + title + \".hdf5\")\n",
    "predict_cnn(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
