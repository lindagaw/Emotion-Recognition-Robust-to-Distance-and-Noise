{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import gc\n",
    "import sys\n",
    "import h5py\n",
    "import time\n",
    "import datetime\n",
    "import pickle\n",
    "import librosa\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import pandas as pd\n",
    "from pandas.plotting import parallel_coordinates\n",
    "from pydub import AudioSegment\n",
    "\n",
    "#imported for testing\n",
    "import wave\n",
    "import contextlib\n",
    "\n",
    "# for outputing file\n",
    "from scipy.cluster.vq import vq, kmeans, whiten\n",
    "import scipy.stats.stats as st\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.decomposition import PCA as sklearnPCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import auc, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn import mixture\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "import keras\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.constraints import maxnorm\n",
    "from keras.layers import Convolution1D, Dense, MaxPooling1D, Flatten, Add, Dropout, Input, Activation\n",
    "from keras.layers import TimeDistributed, Bidirectional, LSTM, LeakyReLU\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers, regularizers\n",
    "from keras.utils import np_utils, to_categorical\n",
    "\n",
    "#from colorama import Fore, Back, Style\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "#warnings.filterwarnings('ignore')\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# confirm TensorFlow sees the GPU\n",
    "from tensorflow.python.client import device_lib\n",
    "# assert 'GPU' in str(device_lib.list_local_devices())\n",
    "\n",
    "# confirm Keras sees the GPU\n",
    "from keras import backend\n",
    "# print(len(backend.tensorflow_backend._get_available_gpus()) > 0)\n",
    "\n",
    "#warnings.filterwarnings('ignore')\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "sys.path.insert(1, '..//components//')\n",
    "import load_feat_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate = 44100\n",
    "frame_number = 48\n",
    "hop_length = 441  # frame size= 2 * hop\n",
    "segment_length = int(sample_rate * 0.2)  # 0.2\n",
    "segment_pad = int(sample_rate * 0.02)     # 0.02\n",
    "overlapping = int(sample_rate * 0.1)   # 0.1\n",
    "\n",
    "classes = 2\n",
    "NumofFeaturetoUse = 272\n",
    "n_neurons = 2048\n",
    "dense_layers = 2\n",
    "num_layers = 3\n",
    "fillength = 3\n",
    "nbindex = 1024\n",
    "dropout = 0.2\n",
    "n_batch = 128\n",
    "n_epoch = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_progress(progress):\n",
    "    bar_length = 100\n",
    "    if isinstance(progress, int):\n",
    "        progress = float(progress)\n",
    "    if not isinstance(progress, float):\n",
    "        progress = 0\n",
    "    if progress < 0:\n",
    "        progress = 0\n",
    "    if progress >= 1:\n",
    "        progress = 1\n",
    "\n",
    "    block = int(round(bar_length * progress))\n",
    "    clear_output(wait = True)\n",
    "    \n",
    "    text = \"Progress: [{0}] {1:.1f}%\".format( \"#\" * block + \"-\" * (bar_length - block), progress * 100)\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprise_vector(path):\n",
    "    vec_to_return = []\n",
    "    for fname in os.listdir(path):\n",
    "        current_vec = np.load(path + fname)\n",
    "        vec_to_return.append(current_vec)\n",
    "\n",
    "    vec_to_return = np.array(vec_to_return)\n",
    "    return vec_to_return\n",
    "\n",
    "def comprise_label(feature_vector, label):\n",
    "    label_vec_to_ret = []\n",
    "    length = len(list(feature_vector))\n",
    "    for index in range(0, length):\n",
    "        current_label = [label]\n",
    "        label_vec_to_ret.append(current_label)\n",
    "    label_vec_to_ret = np.array(label_vec_to_ret)\n",
    "\n",
    "    return label_vec_to_ret\n",
    "\n",
    "def float_compatible(input_np):\n",
    "\n",
    "    x = np.where(input_np >= np.finfo(np.float32).max)\n",
    "    for index in range(0, len(x[0])):\n",
    "        x_position = x[0][index]\n",
    "        y_position = x[1][index]\n",
    "        input_np[x_position, y_position] = 0.0\n",
    "    input_np = np.nan_to_num(input_np)\n",
    "\n",
    "    return input_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allnoised_npy[0, 1, 2, 3, 4] ==> H, A, N, S, O\n",
    "# homenoised_npy[0, 1, 2, 3, 4] ==> H, A, N, S, O\n",
    "all_noised_npy = load_feat_directories.allnoised_npy\n",
    "all_noised_npy_test = load_feat_directories.allnoised_npy_test\n",
    "home_noised_npy = load_feat_directories.homenoised_npy\n",
    "home_noised_npy_test = load_feat_directories.homenoised_npy_test\n",
    "\n",
    "for index in range(0, 5):\n",
    "    #x = os.path.exists(all_noised_npy[index])\n",
    "    #y = os.path.exists(home_noised_npy[index])\n",
    "    if not os.path.exists(all_noised_npy[index]):\n",
    "        print(all_noised_npy[index] + ' does not exist. Breaking the loop... ')\n",
    "        break\n",
    "\n",
    "    if not os.path.exists(home_noised_npy[index]):\n",
    "        print(home_noised_npy[index] + 'does not exist. Breaking the loop... ')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "//Users//yegao//Documents//Datasets//padded_deamplified_allnoised_reverberated//npy//Happy_npy//\n",
      "//Users//yegao//Documents//Datasets//padded_deamplified_allnoised_reverberated//npy//Angry_npy//\n",
      "//Users//yegao//Documents//Datasets//padded_deamplified_allnoised_reverberated//npy//Neutral_npy//\n",
      "//Users//yegao//Documents//Datasets//padded_deamplified_allnoised_reverberated//npy//Sad_npy//\n",
      "//Users//yegao//Documents//Datasets//padded_deamplified_allnoised_reverberated//npy//Other_npy//\n",
      "//Users//yegao//Documents//Datasets//padded_deamplified_allnoised_reverberated//npy//Happy_npy_test//\n",
      "//Users//yegao//Documents//Datasets//padded_deamplified_homenoised_reverberated//npy//Happy_npy_test//\n",
      "//Users//yegao//Documents//Datasets//padded_deamplified_allnoised_reverberated//npy//Angry_npy_test//\n",
      "//Users//yegao//Documents//Datasets//padded_deamplified_homenoised_reverberated//npy//Angry_npy_test//\n",
      "//Users//yegao//Documents//Datasets//padded_deamplified_allnoised_reverberated//npy//Neutral_npy_test//\n",
      "//Users//yegao//Documents//Datasets//padded_deamplified_homenoised_reverberated//npy//Neutral_npy_test//\n",
      "//Users//yegao//Documents//Datasets//padded_deamplified_allnoised_reverberated//npy//Sad_npy_test//\n",
      "//Users//yegao//Documents//Datasets//padded_deamplified_homenoised_reverberated//npy//Sad_npy_test//\n",
      "//Users//yegao//Documents//Datasets//padded_deamplified_allnoised_reverberated//npy//Other_npy_test//\n",
      "//Users//yegao//Documents//Datasets//padded_deamplified_homenoised_reverberated//npy//Other_npy_test//\n"
     ]
    }
   ],
   "source": [
    "for index in [0, 1, 2, 3, 4]:\n",
    "    if not os.path.exists(all_noised_npy[index]):\n",
    "        print(all_noised_npy[index] + ' does not exist.')\n",
    "    else:\n",
    "        path = all_noised_npy[index]\n",
    "        print(path)\n",
    "        if index == 0:\n",
    "            h_feature_vector_all = comprise_vector(path)\n",
    "            h_label_vector_all = comprise_label(h_feature_vector_all, index)\n",
    "        elif index == 1:\n",
    "            a_feature_vector_all = comprise_vector(path)\n",
    "            a_label_vector_all = comprise_label(a_feature_vector_all, index)\n",
    "        elif index == 2:\n",
    "            n_feature_vector_all = comprise_vector(path)\n",
    "            n_label_vector_all = comprise_label(n_feature_vector_all, index)\n",
    "        elif index == 3:\n",
    "            s_feature_vector_all = comprise_vector(path)\n",
    "            s_label_vector_all = comprise_label(s_feature_vector_all, index)\n",
    "        else:\n",
    "            o_feature_vector_all = comprise_vector(path)\n",
    "            o_label_vector_all = comprise_label(o_feature_vector_all, index)\n",
    "\n",
    "    if not os.path.exists(home_noised_npy[index]):\n",
    "        print(home_noised_npy[index] + 'does not exist.')\n",
    "    else:\n",
    "        path = home_noised_npy[index]\n",
    "        print(path)\n",
    "        if index == 0:\n",
    "            h_feature_vector_home = comprise_vector(path)\n",
    "            h_label_vector_home = comprise_label(h_feature_vector_home, index)\n",
    "        elif index == 1:\n",
    "            a_feature_vector_home = comprise_vector(path)\n",
    "            a_label_vector_home = comprise_label(a_feature_vector_home, index)\n",
    "        elif index == 2:\n",
    "            n_feature_vector_home = comprise_vector(path)\n",
    "            n_label_vector_home = comprise_label(n_feature_vector_home, index)\n",
    "        elif index == 3:\n",
    "            s_feature_vector_home = comprise_vector(path)\n",
    "            s_label_vector_home = comprise_label(s_feature_vector_home, index)\n",
    "        else:\n",
    "            o_feature_vector_home = comprise_vector(path)\n",
    "            o_label_vector_home = comprise_label(o_feature_vector_home, index)\n",
    "\n",
    "for index in [0, 1, 2, 3, 4]:\n",
    "    if not os.path.exists(all_noised_npy_test[index]):\n",
    "        print(all_noised_npy_test[index] + ' does not exist.')\n",
    "    else:\n",
    "        path = all_noised_npy_test[index]\n",
    "        print(path)\n",
    "        if index == 0:\n",
    "            h_feature_vector_all_test = comprise_vector(path)\n",
    "            h_label_vector_all_test = comprise_label(h_feature_vector_all_test, index)\n",
    "        elif index == 1:\n",
    "            a_feature_vector_all_test = comprise_vector(path)\n",
    "            a_label_vector_all_test = comprise_label(a_feature_vector_all_test, index)\n",
    "        elif index == 2:\n",
    "            n_feature_vector_all_test = comprise_vector(path)\n",
    "            n_label_vector_all_test = comprise_label(n_feature_vector_all_test, index)\n",
    "        elif index == 3:\n",
    "            s_feature_vector_all_test = comprise_vector(path)\n",
    "            s_label_vector_all_test = comprise_label(s_feature_vector_all_test, index)\n",
    "        else:\n",
    "            o_feature_vector_all_test = comprise_vector(path)\n",
    "            o_label_vector_all_test = comprise_label(o_feature_vector_all_test, index)\n",
    "\n",
    "    if not os.path.exists(home_noised_npy_test[index]):\n",
    "        print(home_noised_npy_test[index] + 'does not exist.')\n",
    "    else:\n",
    "        path = home_noised_npy_test[index]\n",
    "        print(path)\n",
    "        if index == 0:\n",
    "            h_feature_vector_home_test = comprise_vector(path)\n",
    "            h_label_vector_home_test = comprise_label(h_feature_vector_home_test, index)\n",
    "        elif index == 1:\n",
    "            a_feature_vector_home_test = comprise_vector(path)\n",
    "            a_label_vector_home_test = comprise_label(a_feature_vector_home_test, index)\n",
    "        elif index == 2:\n",
    "            n_feature_vector_home_test = comprise_vector(path)\n",
    "            n_label_vector_home_test = comprise_label(n_feature_vector_home_test, index)\n",
    "        elif index == 3:\n",
    "            s_feature_vector_home_test = comprise_vector(path)\n",
    "            s_label_vector_home_test = comprise_label(s_feature_vector_home_test, index)\n",
    "        else:\n",
    "            o_feature_vector_home_test = comprise_vector(path)\n",
    "            o_label_vector_home_test = comprise_label(o_feature_vector_home_test, index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training npy files\n",
    "featureSet = float_compatible(np.vstack((h_feature_vector_home, a_feature_vector_home)))\n",
    "Label = to_categorical(np.vstack((h_label_vector_home, a_label_vector_home)))\n",
    "#featureSet = np.split(featureSet, np.array([NumofFeaturetoUse]), axis = 2)[0]\n",
    "print('training data: ' + str(featureSet.shape))\n",
    "print('training label: ' + str(Label.shape))\n",
    "\n",
    "# Load testing npy files\n",
    "featureSet_val = float_compatible(np.vstack((h_feature_vector_home_test, a_feature_vector_home_test)))\n",
    "Label_val = to_categorical(np.vstack((h_label_vector_home_test, a_label_vector_home_test)))\n",
    "#featureSet_val = np.split(featureSet_val, np.array([NumofFeaturetoUse]), axis = 2)[0]\n",
    "print('evaluation data: ' + str(featureSet_val.shape))\n",
    "print('evaluation label: ' + str(Label_val.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = optimizers.Adam(lr = 1e-4, beta_1 = 0.9, beta_2 = 0.999, epsilon = None, decay = 0, amsgrad = True)\n",
    "sgd = optimizers.SGD(lr = 0.01, decay = 1e-6, momentum = 0.9, nesterov = True)\n",
    "rmsprop = optimizers.RMSprop(lr = 0.0001, rho = 0.9, epsilon = None, decay = 0.0)\n",
    "adagrad = optimizers.Adagrad(lr = 0.01, epsilon = None, decay = 0.0)\n",
    "adadelta = optimizers.Adadelta(lr = 1.0, rho = 0.95, epsilon = None, decay = 0.0)\n",
    "adamax = optimizers.Adamax(lr = 0.002, beta_1 = 0.9, beta_2 = 0.999, epsilon = None, decay = 0.0)\n",
    "nadam = optimizers.Nadam(lr = 0.002, beta_1 = 0.9, beta_2 = 0.999, epsilon = None, schedule_decay = 0.004)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def record(str_message, log_file):\n",
    "    str_message = str_message + '\\n'\n",
    "    file = open(log_file, 'a')\n",
    "    file.write(str_message)\n",
    "    file.close()\n",
    "\n",
    "def create_cnn(title, num_layers, n_neurons, n_batch, nbindex, dropout, classes, dense_layers):\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Convolution1D(nb_filter=nbindex, filter_length=fillength, activation = 'relu',\n",
    "                            input_shape=(featureSet.shape[1], featureSet.shape[2]), kernel_constraint=maxnorm(3)))\n",
    "    model.add(MaxPooling1D(pool_size=2, strides=2, padding='valid'))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(Convolution1D(nb_filter=nbindex*2, filter_length=fillength, activation = 'relu',\n",
    "                            kernel_constraint=maxnorm(3)))\n",
    "    model.add(MaxPooling1D(pool_size=2, strides=2, padding='valid'))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(Convolution1D(nb_filter=nbindex*2, filter_length=fillength, activation = 'relu',\n",
    "                            kernel_constraint=maxnorm(3)))\n",
    "    model.add(MaxPooling1D(pool_size=2, strides=2, padding='valid'))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(Convolution1D(nb_filter=nbindex, filter_length=fillength, activation = 'relu',\n",
    "                            kernel_constraint=maxnorm(3)))  \n",
    "    model.add(MaxPooling1D(pool_size=2, strides=2, padding='valid'))\n",
    "    model.add(Dropout(dropout))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    for i in range(0, dense_layers):\n",
    "        model.add(Dense(n_neurons, activation = 'relu'))\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(Dense(classes, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=adam,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn(title, num_layers, n_neurons, n_batch, nbindex, dropout, classes, dense_layers):\n",
    "    \n",
    "    in_layer = Input(shape=(featureSet.shape[1], featureSet.shape[2]))\n",
    "\n",
    "    first = Sequential()\n",
    "    first.add(Dense(classes, activation='softmax')(in_layer)(in_layer))\n",
    "    second = Sequential()\n",
    "    second.add(Dense(classes, activation='softmax')(in_layer)(in_layer))\n",
    "    \n",
    "    merged = keras.layers.Concatenate([first, second])\n",
    "    \n",
    "    out_layer = Dense(classes, activation='softmax')(merge)\n",
    "\n",
    "    model = keras.models.Model(inputs=in_layer, outputs=out_layer)\n",
    "    #\n",
    "\n",
    "    print(model.summary())\n",
    "    plot_model(model, to_file='shared_input_layer.png')\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cnn(prefix, source):\n",
    "    \n",
    "    save_to_path = prefix\n",
    "\n",
    "    checkpoint_filepath = prefix + \"Checkpoint_\" + source + title + \".hdf5\"\n",
    "    final_filepath = prefix + \"Final_\" + source + title + \".hdf5\"\n",
    "\n",
    "    if not os.path.exists(save_to_path):\n",
    "        os.mkdir(save_to_path)\n",
    "\n",
    "    X, X_test, Y, Y_test= train_test_split(featureSet, Label, test_size = 0.25, shuffle = True)\n",
    "    model = create_cnn(title, num_layers, n_neurons, n_batch, nbindex, dropout, classes, dense_layers)\n",
    "    checkpoint = ModelCheckpoint(checkpoint_filepath, monitor = 'val_acc', verbose = 0, save_best_only = True, mode = 'auto')\n",
    "    early_stopping_monitor = EarlyStopping(patience = 50)\n",
    "    callbacks_list = [checkpoint, early_stopping_monitor]\n",
    "    model.fit(X, Y, nb_epoch = n_epoch, batch_size = n_batch,  callbacks = callbacks_list, validation_data = (X_test, Y_test), verbose = 1)\n",
    "    #model.save(final_filepath)\n",
    "    #model.load(checkpoint_filepath)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_cnn(model):\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    for item in list(Label_val):\n",
    "            if item[0] > item[1]:\n",
    "                y_true.append(0)\n",
    "            elif item[0] < item[1]:\n",
    "                y_true.append(1)\n",
    "            else:\n",
    "                y_true.append(0)\n",
    "\n",
    "    for item in list(model.predict(featureSet_val)):\n",
    "            if item[0] > item[1]:\n",
    "                y_pred.append(0)\n",
    "            elif item[0] < item[1]:\n",
    "                y_pred.append(1)\n",
    "            else:\n",
    "                y_pred.append(0)\n",
    "\n",
    "    print('Accuracy: ' + str(accuracy_score(y_true, y_pred)))\n",
    "    print('Precision: ' + str(precision_score(y_true, y_pred)))\n",
    "    print('Recall: ' + str(recall_score(y_true, y_pred)))\n",
    "    print('f1 score: ' + str(f1_score(y_true, y_pred)))\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "\n",
    "    print('true positive ' + str(tp))\n",
    "    print('false positive ' + str(fp))\n",
    "    print('false negative ' + str(fn))\n",
    "    print('true negative ' + str(tn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0925 09:10:55.843405 139779401963328 deprecation_wrapper.py:119] From /home/yg9ca/.conda/envs/myenv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0925 09:10:55.844452 139779401963328 deprecation_wrapper.py:119] From /home/yg9ca/.conda/envs/myenv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0925 09:10:55.850580 139779401963328 deprecation_wrapper.py:119] From /home/yg9ca/.conda/envs/myenv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'Tensor' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-f895c0ae7838>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mfinal_filepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"Checkpoint_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msource\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtitle\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".hdf5\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_cnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_neurons\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdense_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-5561f024b8f0>\u001b[0m in \u001b[0;36mcreate_cnn\u001b[0;34m(title, num_layers, n_neurons, n_batch, nbindex, dropout, classes, dense_layers)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mfirst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mfirst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0msecond\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0msecond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Tensor' object is not callable"
     ]
    }
   ],
   "source": [
    "title = 'H_A_neurons_shared_architecture_' + str(n_neurons) + '_filters_' + str(nbindex) + '_dropout_' + str(dropout) + '_epoch_' + str(n_epoch) + '_dense_' + str(dense_layers)\n",
    "\n",
    "prefix = '..//..//modules//'\n",
    "source = 'home_'\n",
    "\n",
    "if not os.path.exists(prefix):\n",
    "    os.mkdir(prefix)\n",
    "\n",
    "final_filepath = prefix + \"Checkpoint_\" + source + title + \".hdf5\"\n",
    "model = create_cnn(title, num_layers, n_neurons, n_batch, nbindex, dropout, classes, dense_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yg9ca/.conda/envs/myenv/lib/python3.7/site-packages/ipykernel_launcher.py:12: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", input_shape=(48, 272), kernel_constraint=<keras.con..., filters=1024, kernel_size=3)`\n",
      "  if sys.path[0] == '':\n",
      "/home/yg9ca/.conda/envs/myenv/lib/python3.7/site-packages/ipykernel_launcher.py:17: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", kernel_constraint=<keras.con..., filters=3072, kernel_size=3)`\n",
      "/home/yg9ca/.conda/envs/myenv/lib/python3.7/site-packages/ipykernel_launcher.py:26: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", kernel_constraint=<keras.con..., filters=3072, kernel_size=3)`\n",
      "/home/yg9ca/.conda/envs/myenv/lib/python3.7/site-packages/ipykernel_launcher.py:31: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", kernel_constraint=<keras.con..., filters=1024, kernel_size=3)`\n",
      "/home/yg9ca/.conda/envs/myenv/lib/python3.7/site-packages/ipykernel_launcher.py:16: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13201 samples, validate on 4401 samples\n",
      "Epoch 1/1000\n",
      "13201/13201 [==============================] - 167s 13ms/step - loss: 8.0566 - acc: 0.4958 - val_loss: 7.9217 - val_acc: 0.5085\n",
      "Epoch 2/1000\n",
      "13201/13201 [==============================] - 164s 12ms/step - loss: 8.1232 - acc: 0.4960 - val_loss: 7.9217 - val_acc: 0.5085\n",
      "Epoch 3/1000\n",
      "13201/13201 [==============================] - 165s 12ms/step - loss: 8.1232 - acc: 0.4960 - val_loss: 7.9217 - val_acc: 0.5085\n",
      "Epoch 4/1000\n",
      "13201/13201 [==============================] - 165s 13ms/step - loss: 8.1232 - acc: 0.4960 - val_loss: 7.9217 - val_acc: 0.5085\n",
      "Epoch 5/1000\n",
      "13201/13201 [==============================] - 163s 12ms/step - loss: 8.1232 - acc: 0.4960 - val_loss: 7.9217 - val_acc: 0.5085\n",
      "Epoch 6/1000\n",
      "13201/13201 [==============================] - 164s 12ms/step - loss: 8.1232 - acc: 0.4960 - val_loss: 7.9217 - val_acc: 0.5085\n",
      "Epoch 7/1000\n",
      " 6400/13201 [=============>................] - ETA: 1:20 - loss: 8.0112 - acc: 0.5030"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-4fb2ae6963d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_cnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-43-819154ff5b0c>\u001b[0m in \u001b[0;36mtrain_cnn\u001b[0;34m(prefix, source)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mearly_stopping_monitor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatience\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mcallbacks_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping_monitor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_batch\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallbacks_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;31m#model.save(final_filepath)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m#model.load(checkpoint_filepath)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/myenv/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/.conda/envs/myenv/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/myenv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/myenv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/myenv/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = train_cnn(prefix, source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(prefix + \"Checkpoint_\" + source + title + \".hdf5\")\n",
    "predict_cnn(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9109518935516888\n",
      "Precision: 0.9208025343189018\n",
      "Recall: 0.898043254376931\n",
      "f1 score: 0.9092805005213764\n",
      "true positive 1744\n",
      "false positive 150\n",
      "false negative 198\n",
      "true negative 1816\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model(prefix + \"Checkpoint_\" + source + title + \".hdf5\")\n",
    "predict_cnn(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9086489252814739\n",
      "Precision: 0.9168858495528669\n",
      "Recall: 0.8975283213182287\n",
      "f1 score: 0.907103825136612\n",
      "true positive 1743\n",
      "false positive 158\n",
      "false negative 199\n",
      "true negative 1808\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model(prefix + \"Checkpoint_\" + source + title + \".hdf5\")\n",
    "predict_cnn(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
