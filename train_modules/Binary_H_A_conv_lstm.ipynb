{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data: (16632, 48, 100)\n",
      "training label: (16632, 2)\n",
      "evaluation data: (3684, 48, 100)\n",
      "evaluation label: (3684, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ash Gao\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:192: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(input_shape=(48, 100), kernel_constraint=<keras.con..., filters=1024, kernel_size=4)`\n",
      "C:\\Users\\Ash Gao\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:204: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(kernel_constraint=<keras.con..., filters=3072, kernel_size=4)`\n",
      "C:\\Users\\Ash Gao\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:210: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(kernel_constraint=<keras.con..., filters=2048, kernel_size=4)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_10 (Conv1D)           (None, 45, 1024)          410624    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 45, 1024)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 22, 1024)          0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 22, 1024)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 19, 3072)          12585984  \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 19, 3072)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 9, 3072)           0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 9, 3072)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 6, 2048)           25167872  \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 6, 2048)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 3, 2048)           0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 3, 2048)           0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 6144)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 12290     \n",
      "=================================================================\n",
      "Total params: 38,176,770\n",
      "Trainable params: 38,176,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ash Gao\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:245: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12474 samples, validate on 4158 samples\n",
      "Epoch 1/1000\n",
      "12474/12474 [==============================] - 795s 64ms/step - loss: 0.6574 - acc: 0.6089 - val_loss: 0.6178 - val_acc: 0.6720\n",
      "Epoch 2/1000\n",
      "12474/12474 [==============================] - 771s 62ms/step - loss: 0.6109 - acc: 0.6734 - val_loss: 0.5870 - val_acc: 0.6979\n",
      "Epoch 3/1000\n",
      "12474/12474 [==============================] - 770s 62ms/step - loss: 0.5798 - acc: 0.6962 - val_loss: 0.5728 - val_acc: 0.7085\n",
      "Epoch 4/1000\n",
      "12474/12474 [==============================] - 770s 62ms/step - loss: 0.5532 - acc: 0.7189 - val_loss: 0.5521 - val_acc: 0.7174\n",
      "Epoch 5/1000\n",
      "12474/12474 [==============================] - 770s 62ms/step - loss: 0.5204 - acc: 0.7464 - val_loss: 0.5624 - val_acc: 0.7100\n",
      "Epoch 6/1000\n",
      "12474/12474 [==============================] - 762s 61ms/step - loss: 0.5080 - acc: 0.7461 - val_loss: 0.5002 - val_acc: 0.7672\n",
      "Epoch 7/1000\n",
      "12474/12474 [==============================] - 763s 61ms/step - loss: 0.4549 - acc: 0.7895 - val_loss: 0.4944 - val_acc: 0.7583\n",
      "Epoch 8/1000\n",
      "12474/12474 [==============================] - 765s 61ms/step - loss: 0.4194 - acc: 0.8069 - val_loss: 0.4629 - val_acc: 0.7840\n",
      "Epoch 9/1000\n",
      "12474/12474 [==============================] - 756s 61ms/step - loss: 0.3910 - acc: 0.8288 - val_loss: 0.4519 - val_acc: 0.7879\n",
      "Epoch 10/1000\n",
      "12474/12474 [==============================] - 744s 60ms/step - loss: 0.3638 - acc: 0.8422 - val_loss: 0.4326 - val_acc: 0.7934\n",
      "Epoch 11/1000\n",
      "12474/12474 [==============================] - 739s 59ms/step - loss: 0.3316 - acc: 0.8628 - val_loss: 0.4518 - val_acc: 0.7888\n",
      "Epoch 12/1000\n",
      "12474/12474 [==============================] - 743s 60ms/step - loss: 0.3075 - acc: 0.8749 - val_loss: 0.4250 - val_acc: 0.7997\n",
      "Epoch 13/1000\n",
      "12474/12474 [==============================] - 753s 60ms/step - loss: 0.2735 - acc: 0.8935 - val_loss: 0.3886 - val_acc: 0.8252\n",
      "Epoch 14/1000\n",
      "12474/12474 [==============================] - 744s 60ms/step - loss: 0.2504 - acc: 0.9048 - val_loss: 0.5091 - val_acc: 0.7682\n",
      "Epoch 15/1000\n",
      "12474/12474 [==============================] - 743s 60ms/step - loss: 0.2266 - acc: 0.9175 - val_loss: 0.3806 - val_acc: 0.8297\n",
      "Epoch 16/1000\n",
      "12474/12474 [==============================] - 744s 60ms/step - loss: 0.2003 - acc: 0.9296 - val_loss: 0.3691 - val_acc: 0.8377\n",
      "Epoch 17/1000\n",
      "12474/12474 [==============================] - 740s 59ms/step - loss: 0.1841 - acc: 0.9381 - val_loss: 0.3620 - val_acc: 0.8420\n",
      "Epoch 18/1000\n",
      "12474/12474 [==============================] - 743s 60ms/step - loss: 0.1658 - acc: 0.9481 - val_loss: 0.3682 - val_acc: 0.8348\n",
      "Epoch 19/1000\n",
      "12474/12474 [==============================] - 740s 59ms/step - loss: 0.1463 - acc: 0.9557 - val_loss: 0.3446 - val_acc: 0.8473\n",
      "Epoch 20/1000\n",
      "12474/12474 [==============================] - 743s 60ms/step - loss: 0.1267 - acc: 0.9650 - val_loss: 0.4386 - val_acc: 0.8074\n",
      "Epoch 21/1000\n",
      "12474/12474 [==============================] - 742s 59ms/step - loss: 0.1210 - acc: 0.9638 - val_loss: 0.3306 - val_acc: 0.8639\n",
      "Epoch 22/1000\n",
      "12474/12474 [==============================] - 743s 60ms/step - loss: 0.0995 - acc: 0.9759 - val_loss: 0.3321 - val_acc: 0.8658\n",
      "Epoch 23/1000\n",
      "12474/12474 [==============================] - 751s 60ms/step - loss: 0.0896 - acc: 0.9785 - val_loss: 0.3473 - val_acc: 0.8521\n",
      "Epoch 24/1000\n",
      "12474/12474 [==============================] - 748s 60ms/step - loss: 0.0782 - acc: 0.9838 - val_loss: 0.3656 - val_acc: 0.8449\n",
      "Epoch 25/1000\n",
      "12474/12474 [==============================] - 745s 60ms/step - loss: 0.0808 - acc: 0.9809 - val_loss: 0.4045 - val_acc: 0.8458\n",
      "Epoch 26/1000\n",
      "12474/12474 [==============================] - 745s 60ms/step - loss: 0.0705 - acc: 0.9855 - val_loss: 0.3445 - val_acc: 0.8552\n",
      "Epoch 27/1000\n",
      "12474/12474 [==============================] - 748s 60ms/step - loss: 0.0653 - acc: 0.9859 - val_loss: 0.3215 - val_acc: 0.8706\n",
      "Epoch 28/1000\n",
      "12474/12474 [==============================] - 745s 60ms/step - loss: 0.0545 - acc: 0.9903 - val_loss: 0.3153 - val_acc: 0.8752\n",
      "Epoch 29/1000\n",
      "12474/12474 [==============================] - 741s 59ms/step - loss: 0.0461 - acc: 0.9934 - val_loss: 0.3200 - val_acc: 0.8737\n",
      "Epoch 30/1000\n",
      "12474/12474 [==============================] - 743s 60ms/step - loss: 0.0450 - acc: 0.9931 - val_loss: 0.3166 - val_acc: 0.8797\n",
      "Epoch 31/1000\n",
      "12474/12474 [==============================] - 741s 59ms/step - loss: 0.0401 - acc: 0.9942 - val_loss: 0.3236 - val_acc: 0.8810\n",
      "Epoch 32/1000\n",
      "12474/12474 [==============================] - 741s 59ms/step - loss: 0.0365 - acc: 0.9948 - val_loss: 0.3246 - val_acc: 0.8730\n",
      "Epoch 33/1000\n",
      "12474/12474 [==============================] - 742s 59ms/step - loss: 0.0332 - acc: 0.9955 - val_loss: 0.3459 - val_acc: 0.8723\n",
      "Epoch 34/1000\n",
      "12474/12474 [==============================] - 740s 59ms/step - loss: 0.0321 - acc: 0.9964 - val_loss: 0.3229 - val_acc: 0.8790\n",
      "Epoch 35/1000\n",
      "12474/12474 [==============================] - 740s 59ms/step - loss: 0.0272 - acc: 0.9972 - val_loss: 0.3320 - val_acc: 0.8797\n",
      "Epoch 36/1000\n",
      "12474/12474 [==============================] - 743s 60ms/step - loss: 0.0254 - acc: 0.9977 - val_loss: 0.3117 - val_acc: 0.8829\n",
      "Epoch 37/1000\n",
      "12474/12474 [==============================] - 739s 59ms/step - loss: 0.0237 - acc: 0.9974 - val_loss: 0.3188 - val_acc: 0.8819\n",
      "Epoch 38/1000\n",
      "12474/12474 [==============================] - 738s 59ms/step - loss: 0.0231 - acc: 0.9976 - val_loss: 0.3191 - val_acc: 0.8843\n",
      "Epoch 39/1000\n",
      "12474/12474 [==============================] - 743s 60ms/step - loss: 0.0203 - acc: 0.9982 - val_loss: 0.3208 - val_acc: 0.8836\n",
      "Epoch 40/1000\n",
      "12474/12474 [==============================] - 746s 60ms/step - loss: 0.0218 - acc: 0.9970 - val_loss: 0.3355 - val_acc: 0.8766\n",
      "Epoch 41/1000\n",
      "12474/12474 [==============================] - 740s 59ms/step - loss: 0.0203 - acc: 0.9977 - val_loss: 0.3210 - val_acc: 0.8874\n",
      "Epoch 42/1000\n",
      "12474/12474 [==============================] - 741s 59ms/step - loss: 0.0179 - acc: 0.9984 - val_loss: 0.3303 - val_acc: 0.8822\n",
      "Epoch 43/1000\n",
      "12474/12474 [==============================] - 742s 59ms/step - loss: 0.0164 - acc: 0.9987 - val_loss: 0.3328 - val_acc: 0.8824\n",
      "Epoch 44/1000\n",
      "12474/12474 [==============================] - 743s 60ms/step - loss: 0.0156 - acc: 0.9985 - val_loss: 0.3246 - val_acc: 0.8886\n",
      "Epoch 45/1000\n",
      "12474/12474 [==============================] - 742s 59ms/step - loss: 0.0132 - acc: 0.9995 - val_loss: 0.3258 - val_acc: 0.8918\n",
      "Epoch 46/1000\n",
      "12474/12474 [==============================] - 746s 60ms/step - loss: 0.0132 - acc: 0.9991 - val_loss: 0.3548 - val_acc: 0.8831\n",
      "Epoch 47/1000\n",
      "12474/12474 [==============================] - 742s 59ms/step - loss: 0.0132 - acc: 0.9990 - val_loss: 0.3590 - val_acc: 0.8728\n",
      "Epoch 48/1000\n",
      "12474/12474 [==============================] - 742s 59ms/step - loss: 0.0120 - acc: 0.9994 - val_loss: 0.3374 - val_acc: 0.8865\n",
      "Epoch 49/1000\n",
      "12474/12474 [==============================] - 748s 60ms/step - loss: 0.0115 - acc: 0.9993 - val_loss: 0.3275 - val_acc: 0.8889\n",
      "Epoch 50/1000\n",
      "12474/12474 [==============================] - 743s 60ms/step - loss: 0.0115 - acc: 0.9994 - val_loss: 0.3396 - val_acc: 0.8843\n",
      "Epoch 51/1000\n",
      "12474/12474 [==============================] - 745s 60ms/step - loss: 0.0106 - acc: 0.9992 - val_loss: 0.3280 - val_acc: 0.8923\n",
      "Epoch 52/1000\n",
      "12474/12474 [==============================] - 740s 59ms/step - loss: 0.0102 - acc: 0.9990 - val_loss: 0.3284 - val_acc: 0.8891\n",
      "Epoch 53/1000\n",
      "12474/12474 [==============================] - 745s 60ms/step - loss: 0.0090 - acc: 0.9998 - val_loss: 0.3321 - val_acc: 0.8841\n",
      "Epoch 54/1000\n",
      "12474/12474 [==============================] - 745s 60ms/step - loss: 0.0089 - acc: 0.9997 - val_loss: 0.3403 - val_acc: 0.8899\n",
      "Epoch 55/1000\n",
      "12474/12474 [==============================] - 742s 59ms/step - loss: 0.0094 - acc: 0.9996 - val_loss: 0.3334 - val_acc: 0.8884\n",
      "Epoch 56/1000\n",
      "12474/12474 [==============================] - 746s 60ms/step - loss: 0.0107 - acc: 0.9990 - val_loss: 0.3388 - val_acc: 0.8865\n",
      "Epoch 57/1000\n",
      "12474/12474 [==============================] - 745s 60ms/step - loss: 0.0086 - acc: 0.9994 - val_loss: 0.3355 - val_acc: 0.8886\n",
      "Epoch 58/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12474/12474 [==============================] - 743s 60ms/step - loss: 0.0096 - acc: 0.9992 - val_loss: 0.3478 - val_acc: 0.8908\n",
      "Epoch 59/1000\n",
      "12474/12474 [==============================] - 742s 60ms/step - loss: 0.0085 - acc: 0.9993 - val_loss: 0.3364 - val_acc: 0.8911\n",
      "Epoch 60/1000\n",
      "12474/12474 [==============================] - 740s 59ms/step - loss: 0.0072 - acc: 0.9998 - val_loss: 0.3451 - val_acc: 0.8901\n",
      "Epoch 61/1000\n",
      "12474/12474 [==============================] - 755s 61ms/step - loss: 0.0073 - acc: 0.9995 - val_loss: 0.3418 - val_acc: 0.8915\n",
      "Epoch 62/1000\n",
      " 2944/12474 [======>.......................] - ETA: 8:59 - loss: 0.0067 - acc: 0.9997"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-b58d1f89e4f3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[0mfinal_filepath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprefix\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"_Layer(s)//Final_\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtitle\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\".hdf5\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[1;31m#model = load_model(final_filepath)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 290\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_cnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    291\u001b[0m \u001b[0mpredict_cnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-b58d1f89e4f3>\u001b[0m in \u001b[0;36mtrain_cnn\u001b[1;34m()\u001b[0m\n\u001b[0;32m    243\u001b[0m     \u001b[0mcallbacks_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mearly_stopping_monitor\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 245\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn_batch\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcallbacks_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinal_filepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1176\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    202\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2977\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2978\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2979\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2980\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2981\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2936\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2937\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2938\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2939\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import gc\n",
    "import sys\n",
    "import h5py\n",
    "import time\n",
    "import datetime\n",
    "import pickle\n",
    "import librosa\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import pandas as pd\n",
    "from pandas.plotting import parallel_coordinates\n",
    "from pydub import AudioSegment\n",
    "\n",
    "#imported for testing\n",
    "import wave\n",
    "import contextlib\n",
    "\n",
    "# for outputing file\n",
    "from scipy.cluster.vq import vq, kmeans, whiten\n",
    "import scipy.stats.stats as st\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.decomposition import PCA as sklearnPCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import auc, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn import mixture\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "import keras\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.constraints import maxnorm\n",
    "from keras.layers import Convolution1D, Dense, MaxPooling1D, Flatten, Add, Dropout, Input, Activation\n",
    "from keras.layers import TimeDistributed, Bidirectional, LSTM, LeakyReLU\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers, regularizers\n",
    "from keras.utils import np_utils, to_categorical\n",
    "\n",
    "from colorama import Fore, Back, Style\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "#warnings.filterwarnings('ignore')\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# confirm TensorFlow sees the GPU\n",
    "from tensorflow.python.client import device_lib\n",
    "# assert 'GPU' in str(device_lib.list_local_devices())\n",
    "\n",
    "# confirm Keras sees the GPU\n",
    "from keras import backend\n",
    "# print(len(backend.tensorflow_backend._get_available_gpus()) > 0)\n",
    "\n",
    "#warnings.filterwarnings('ignore')\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "sample_rate = 44100\n",
    "frame_number = 48\n",
    "hop_length = 441  # frame size= 2 * hop\n",
    "segment_length = int(sample_rate * 0.2)  # 0.2\n",
    "segment_pad = int(sample_rate * 0.02)     # 0.02\n",
    "overlapping = int(sample_rate * 0.1)   # 0.1\n",
    "\n",
    "classes = 2\n",
    "NumofFeaturetoUse = 100\n",
    "n_neurons = 4096\n",
    "dense_layers = 1\n",
    "num_layers = 4\n",
    "fillength = 4\n",
    "nbindex = 1024\n",
    "dropout = 0.15\n",
    "n_batch = 128\n",
    "n_epoch = 1000\n",
    "\n",
    "def update_progress(progress):\n",
    "    bar_length = 100\n",
    "    if isinstance(progress, int):\n",
    "        progress = float(progress)\n",
    "    if not isinstance(progress, float):\n",
    "        progress = 0\n",
    "    if progress < 0:\n",
    "        progress = 0\n",
    "    if progress >= 1:\n",
    "        progress = 1\n",
    "\n",
    "    block = int(round(bar_length * progress))\n",
    "    clear_output(wait = True)\n",
    "    \n",
    "    text = \"Progress: [{0}] {1:.1f}%\".format( \"#\" * block + \"-\" * (bar_length - block), progress * 100)\n",
    "    print(text)\n",
    "\n",
    "prefix = '..//'\n",
    "h_feature_vector = np.load(prefix + 'Features//h_feature_vector_48.npy')\n",
    "h_label_vector = np.load(prefix + 'Features//h_label_vector_48.npy')\n",
    "a_feature_vector = np.load(prefix + 'Features//a_feature_vector_48.npy')\n",
    "a_label_vector = np.load(prefix + 'Features//a_label_vector_48.npy')\n",
    "n_feature_vector = np.load(prefix + 'Features//n_feature_vector_48.npy')\n",
    "n_label_vector = np.load(prefix + 'Features//n_label_vector_48.npy')\n",
    "s_feature_vector = np.load(prefix + 'Features//s_feature_vector_48.npy')\n",
    "s_label_vector = np.load(prefix + 'Features//s_label_vector_48.npy')\n",
    "\n",
    "h_feature_vector_test = np.load(prefix + 'Features//h_feature_vector_test_48.npy')\n",
    "h_label_vector_test = np.load(prefix + 'Features//h_label_vector_test_48.npy')\n",
    "a_feature_vector_test = np.load(prefix + 'Features//a_feature_vector_test_48.npy')\n",
    "a_label_vector_test = np.load(prefix + 'Features//a_label_vector_test_48.npy')\n",
    "n_feature_vector_test = np.load(prefix + 'Features//n_feature_vector_test_48.npy')\n",
    "n_label_vector_test = np.load(prefix + 'Features//n_label_vector_test_48.npy')\n",
    "s_feature_vector_test = np.load(prefix + 'Features//s_feature_vector_test_48.npy')\n",
    "s_label_vector_test = np.load(prefix + 'Features//s_label_vector_test_48.npy')\n",
    "\n",
    "h_label_vector[h_label_vector == 0] = 0\n",
    "a_label_vector[a_label_vector == 1] = 1\n",
    "h_label_vector_test[h_label_vector_test == 0] = 0\n",
    "a_label_vector_test[a_label_vector_test == 1] = 1\n",
    "\n",
    "h_label_vector = to_categorical(h_label_vector, num_classes = 2)\n",
    "a_label_vector = to_categorical(a_label_vector, num_classes = 2)\n",
    "h_label_vector_test = to_categorical(h_label_vector_test, num_classes = 2)\n",
    "a_label_vector_test = to_categorical(a_label_vector_test, num_classes = 2)\n",
    "\n",
    "# Load training npy files\n",
    "featureSet_training = np.vstack((h_feature_vector, a_feature_vector))\n",
    "label_training = np.vstack((h_label_vector, a_label_vector))\n",
    "\n",
    "# Load testing npy files\n",
    "featureSet_testing = np.vstack((h_feature_vector_test, a_feature_vector_test))\n",
    "label_testing = np.vstack((h_label_vector_test, a_label_vector_test))\n",
    "\n",
    "def float_compatible(input_np):\n",
    "\n",
    "    x = np.where(input_np >= np.finfo(np.float32).max)\n",
    "    for index in range(0, len(x[0])):\n",
    "        x_position = x[0][index]\n",
    "        y_position = x[1][index]      \n",
    "        input_np[x_position, y_position] = 0.0\n",
    "    input_np = np.nan_to_num(input_np)\n",
    "        \n",
    "    return input_np\n",
    "\n",
    "train_data = float_compatible((featureSet_training).astype(np.float32))\n",
    "eval_data = float_compatible((featureSet_testing).astype(np.float32))\n",
    "\n",
    "adam = optimizers.Adam(lr = 3e-5, beta_1 = 0.9, beta_2 = 0.999, epsilon = None, decay = 0, amsgrad = True)\n",
    "sgd = optimizers.SGD(lr = 0.01, decay = 1e-6, momentum = 0.9, nesterov = True)\n",
    "rmsprop = optimizers.RMSprop(lr = 0.0001, rho = 0.9, epsilon = None, decay = 0.0)\n",
    "adagrad = optimizers.Adagrad(lr = 0.01, epsilon = None, decay = 0.0)\n",
    "adadelta = optimizers.Adadelta(lr = 1.0, rho = 0.95, epsilon = None, decay = 0.0)\n",
    "adamax = optimizers.Adamax(lr = 0.002, beta_1 = 0.9, beta_2 = 0.999, epsilon = None, decay = 0.0)\n",
    "nadam = optimizers.Nadam(lr = 0.002, beta_1 = 0.9, beta_2 = 0.999, epsilon = None, schedule_decay = 0.004)\n",
    "\n",
    "featureSet = train_data\n",
    "Label = label_training\n",
    "featureSet = np.split(featureSet, np.array([NumofFeaturetoUse]), axis = 2)[0]\n",
    "\n",
    "print('training data: ' + str(featureSet.shape))\n",
    "print('training label: ' + str(Label.shape))\n",
    "\n",
    "featureSet_val = eval_data\n",
    "Label_val = label_testing\n",
    "featureSet_val = np.split(featureSet_val, np.array([NumofFeaturetoUse]), axis = 2)[0]\n",
    "\n",
    "print('evaluation data: ' + str(featureSet_val.shape))\n",
    "print('evaluation label: ' + str(Label_val.shape))\n",
    "\n",
    "def record(str_message, log_file):\n",
    "    str_message = str_message + '\\n'\n",
    "    file = open(log_file, 'a')\n",
    "    file.write(str_message)\n",
    "    file.close()\n",
    "\n",
    "def create_cnn(title, num_layers, n_neurons, n_batch, nbindex, dropout, classes, dense_layers):\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Convolution1D(nb_filter=nbindex, filter_length=fillength,\n",
    "                            input_shape=(featureSet.shape[1], featureSet.shape[2]), kernel_constraint=maxnorm(3)))\n",
    "    model.add(LeakyReLU(alpha=0.05))\n",
    "    model.add(MaxPooling1D(pool_size=2, strides=2, padding='valid'))\n",
    "    model.add(Dropout(dropout))\n",
    "    '''\n",
    "    model.add(Convolution1D(nb_filter=nbindex*2, filter_length=fillength,\n",
    "                            kernel_constraint=maxnorm(3)))\n",
    "    model.add(LeakyReLU(alpha=0.05))\n",
    "    model.add(MaxPooling1D(pool_size=2, strides=2, padding='valid'))\n",
    "    model.add(Dropout(dropout))\n",
    "    '''\n",
    "    model.add(Convolution1D(nb_filter=nbindex*3, filter_length=fillength,\n",
    "                            kernel_constraint=maxnorm(3)))\n",
    "    model.add(LeakyReLU(alpha=0.05))\n",
    "    model.add(MaxPooling1D(pool_size=2, strides=2, padding='valid'))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(Convolution1D(nb_filter=nbindex*2, filter_length=fillength,\n",
    "                            kernel_constraint=maxnorm(3)))  \n",
    "    model.add(LeakyReLU(alpha=0.05))\n",
    "    model.add(MaxPooling1D(pool_size=2, strides=2, padding='valid'))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(classes, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_cnn():\n",
    "    \n",
    "    save_to_path = prefix + str(num_layers) + \"_Layer(s)//\"\n",
    "\n",
    "    checkpoint_filepath = prefix + str(num_layers) + \"_Layer(s)//Checkpoint_\" + title + \".hdf5\"\n",
    "    final_filepath = prefix + str(num_layers) + \"_Layer(s)//Final_\" + title + \".hdf5\"\n",
    "\n",
    "    if not os.path.exists(save_to_path):\n",
    "        os.mkdir(save_to_path)\n",
    "\n",
    "    X, X_test, Y, Y_test= train_test_split(featureSet, Label, test_size = 0.25, shuffle = True)\n",
    "\n",
    "    model = create_cnn(title, num_layers, n_neurons, n_batch, nbindex, dropout, classes, dense_layers)\n",
    "\n",
    "    checkpoint = ModelCheckpoint(checkpoint_filepath, monitor = 'val_acc', verbose = 0, save_best_only = True, mode = 'auto')\n",
    "\n",
    "    early_stopping_monitor = EarlyStopping(patience = 500)\n",
    "\n",
    "    callbacks_list = [checkpoint, early_stopping_monitor]\n",
    "\n",
    "    model.fit(X, Y, nb_epoch = n_epoch, batch_size = n_batch,  callbacks = callbacks_list, validation_data = (X_test, Y_test), verbose = 1)\n",
    "\n",
    "    model.save_weights(final_filepath)\n",
    "\n",
    "    model.load_weights(checkpoint_filepath)\n",
    "\n",
    "    return model\n",
    "\n",
    "def predict_cnn(model):\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    for item in list(Label_val):\n",
    "            if item[0] > item[1]:\n",
    "                y_true.append(0)\n",
    "            elif item[0] < item[1]:\n",
    "                y_true.append(1)\n",
    "            else:\n",
    "                y_true.append(0)\n",
    "\n",
    "    for item in list(model.predict(featureSet_val)):\n",
    "            if item[0] > item[1]:\n",
    "                y_pred.append(0)\n",
    "            elif item[0] < item[1]:\n",
    "                y_pred.append(1)\n",
    "            else:\n",
    "                y_pred.append(0)\n",
    "\n",
    "    print('Accuracy: ' + str(accuracy_score(y_true, y_pred)))\n",
    "    print('Precision: ' + str(precision_score(y_true, y_pred)))\n",
    "    print('Recall: ' + str(recall_score(y_true, y_pred)))\n",
    "    print('f1 score: ' + str(f1_score(y_true, y_pred)))\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "\n",
    "    print('true positive ' + str(tp))\n",
    "    print('false positive ' + str(fp))\n",
    "    print('false negative ' + str(fn))\n",
    "    print('true negative ' + str(tn))\n",
    "\n",
    "title = 'H_A_neurons_' + str(n_neurons) + '_filters_' + str(\n",
    "    nbindex) + '_dropout_' + str(dropout) + '_epoch_' + str(n_epoch)\n",
    "\n",
    "final_filepath = prefix + str(num_layers) + \"_Layer(s)//Final_\" + title + \".hdf5\"\n",
    "#model = load_model(final_filepath)\n",
    "model = train_cnn()\n",
    "predict_cnn(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
