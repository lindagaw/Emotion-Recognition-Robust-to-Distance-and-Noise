{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data: (16632, 48, 100)\n",
      "training label: (16632, 2)\n",
      "evaluation data: (3684, 48, 100)\n",
      "evaluation label: (3684, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0907 23:20:44.360385  3760 deprecation_wrapper.py:119] From C:\\Users\\Ash Gao\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "C:\\Users\\Ash Gao\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:192: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(input_shape=(48, 100), kernel_constraint=<keras.con..., filters=128, kernel_size=5)`\n",
      "W0907 23:20:44.387346  3760 deprecation_wrapper.py:119] From C:\\Users\\Ash Gao\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0907 23:20:44.398285  3760 deprecation_wrapper.py:119] From C:\\Users\\Ash Gao\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0907 23:20:44.573177  3760 deprecation_wrapper.py:119] From C:\\Users\\Ash Gao\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0907 23:20:44.577197  3760 deprecation_wrapper.py:119] From C:\\Users\\Ash Gao\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0907 23:20:44.602099  3760 deprecation.py:506] From C:\\Users\\Ash Gao\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "C:\\Users\\Ash Gao\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:204: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(kernel_constraint=<keras.con..., filters=384, kernel_size=5)`\n",
      "C:\\Users\\Ash Gao\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:210: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(kernel_constraint=<keras.con..., filters=256, kernel_size=5)`\n",
      "W0907 23:20:44.780278  3760 deprecation_wrapper.py:119] From C:\\Users\\Ash Gao\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0907 23:20:44.787259  3760 deprecation_wrapper.py:119] From C:\\Users\\Ash Gao\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0907 23:20:44.791249  3760 deprecation.py:323] From C:\\Users\\Ash Gao\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "C:\\Users\\Ash Gao\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:245: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 44, 128)           64128     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 44, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 22, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 22, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 18, 384)           246144    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 18, 384)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 9, 384)            0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 9, 384)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 5, 256)            491776    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 5, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 1026      \n",
      "=================================================================\n",
      "Total params: 803,074\n",
      "Trainable params: 803,074\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 12474 samples, validate on 4158 samples\n",
      "Epoch 1/50000\n",
      "12474/12474 [==============================] - 24s 2ms/step - loss: 0.6819 - acc: 0.5600 - val_loss: 0.6675 - val_acc: 0.6169\n",
      "Epoch 2/50000\n",
      "12474/12474 [==============================] - 24s 2ms/step - loss: 0.6581 - acc: 0.6187 - val_loss: 0.6465 - val_acc: 0.6376\n",
      "Epoch 3/50000\n",
      "12474/12474 [==============================] - 25s 2ms/step - loss: 0.6407 - acc: 0.6379 - val_loss: 0.6306 - val_acc: 0.6510\n",
      "Epoch 4/50000\n",
      "12474/12474 [==============================] - 23s 2ms/step - loss: 0.6257 - acc: 0.6614 - val_loss: 0.6186 - val_acc: 0.6691\n",
      "Epoch 5/50000\n",
      "12474/12474 [==============================] - 23s 2ms/step - loss: 0.6172 - acc: 0.6617 - val_loss: 0.6078 - val_acc: 0.6739\n",
      "Epoch 6/50000\n",
      "12474/12474 [==============================] - 23s 2ms/step - loss: 0.6049 - acc: 0.6776 - val_loss: 0.5980 - val_acc: 0.6818\n",
      "Epoch 7/50000\n",
      "12474/12474 [==============================] - 24s 2ms/step - loss: 0.5934 - acc: 0.6934 - val_loss: 0.5895 - val_acc: 0.6905\n",
      "Epoch 8/50000\n",
      "12474/12474 [==============================] - 23s 2ms/step - loss: 0.5850 - acc: 0.6966 - val_loss: 0.5853 - val_acc: 0.7011\n",
      "Epoch 9/50000\n",
      "12474/12474 [==============================] - 23s 2ms/step - loss: 0.5758 - acc: 0.7064 - val_loss: 0.5745 - val_acc: 0.7051\n",
      "Epoch 10/50000\n",
      "12474/12474 [==============================] - 24s 2ms/step - loss: 0.5669 - acc: 0.7083 - val_loss: 0.5689 - val_acc: 0.7114\n",
      "Epoch 11/50000\n",
      "12474/12474 [==============================] - 23s 2ms/step - loss: 0.5607 - acc: 0.7134 - val_loss: 0.5626 - val_acc: 0.7155\n",
      "Epoch 12/50000\n",
      "12474/12474 [==============================] - 24s 2ms/step - loss: 0.5517 - acc: 0.7210 - val_loss: 0.5574 - val_acc: 0.7201\n",
      "Epoch 13/50000\n",
      "12474/12474 [==============================] - 23s 2ms/step - loss: 0.5466 - acc: 0.7247 - val_loss: 0.5528 - val_acc: 0.7225\n",
      "Epoch 14/50000\n",
      "12474/12474 [==============================] - 23s 2ms/step - loss: 0.5426 - acc: 0.7266 - val_loss: 0.5509 - val_acc: 0.7227\n",
      "Epoch 15/50000\n",
      "12474/12474 [==============================] - 23s 2ms/step - loss: 0.5336 - acc: 0.7348 - val_loss: 0.5452 - val_acc: 0.7251\n",
      "Epoch 16/50000\n",
      "12474/12474 [==============================] - 24s 2ms/step - loss: 0.5246 - acc: 0.7438 - val_loss: 0.5421 - val_acc: 0.7342\n",
      "Epoch 17/50000\n",
      "12474/12474 [==============================] - 23s 2ms/step - loss: 0.5227 - acc: 0.7399 - val_loss: 0.5381 - val_acc: 0.7342\n",
      "Epoch 18/50000\n",
      "12474/12474 [==============================] - 24s 2ms/step - loss: 0.5139 - acc: 0.7477 - val_loss: 0.5319 - val_acc: 0.7381\n",
      "Epoch 19/50000\n",
      "12474/12474 [==============================] - 23s 2ms/step - loss: 0.5058 - acc: 0.7533 - val_loss: 0.5299 - val_acc: 0.7350\n",
      "Epoch 20/50000\n",
      "12474/12474 [==============================] - 24s 2ms/step - loss: 0.5013 - acc: 0.7560 - val_loss: 0.5323 - val_acc: 0.7258\n",
      "Epoch 21/50000\n",
      "12474/12474 [==============================] - 24s 2ms/step - loss: 0.4989 - acc: 0.7594 - val_loss: 0.5251 - val_acc: 0.7374\n",
      "Epoch 22/50000\n",
      "12474/12474 [==============================] - 24s 2ms/step - loss: 0.4902 - acc: 0.7648 - val_loss: 0.5206 - val_acc: 0.7412\n",
      "Epoch 23/50000\n",
      "12474/12474 [==============================] - 23s 2ms/step - loss: 0.4852 - acc: 0.7706 - val_loss: 0.5318 - val_acc: 0.7309\n",
      "Epoch 24/50000\n",
      "12474/12474 [==============================] - 23s 2ms/step - loss: 0.4834 - acc: 0.7676 - val_loss: 0.5320 - val_acc: 0.7261\n",
      "Epoch 25/50000\n",
      "12474/12474 [==============================] - 23s 2ms/step - loss: 0.4767 - acc: 0.7708 - val_loss: 0.5116 - val_acc: 0.7468\n",
      "Epoch 26/50000\n",
      "12474/12474 [==============================] - 23s 2ms/step - loss: 0.4719 - acc: 0.7767 - val_loss: 0.5089 - val_acc: 0.7518\n",
      "Epoch 27/50000\n",
      "12474/12474 [==============================] - 23s 2ms/step - loss: 0.4681 - acc: 0.7795 - val_loss: 0.5075 - val_acc: 0.7487\n",
      "Epoch 28/50000\n",
      "12474/12474 [==============================] - 17s 1ms/step - loss: 0.4599 - acc: 0.7798 - val_loss: 0.5160 - val_acc: 0.7415\n",
      "Epoch 29/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.4566 - acc: 0.7872 - val_loss: 0.5016 - val_acc: 0.7566\n",
      "Epoch 30/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.4532 - acc: 0.7884 - val_loss: 0.4975 - val_acc: 0.7573\n",
      "Epoch 31/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.4493 - acc: 0.7848 - val_loss: 0.4966 - val_acc: 0.7564\n",
      "Epoch 32/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.4417 - acc: 0.7951 - val_loss: 0.4928 - val_acc: 0.7614\n",
      "Epoch 33/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.4353 - acc: 0.8013 - val_loss: 0.4915 - val_acc: 0.7612\n",
      "Epoch 34/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.4322 - acc: 0.8044 - val_loss: 0.4963 - val_acc: 0.7588\n",
      "Epoch 35/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.4256 - acc: 0.8046 - val_loss: 0.4853 - val_acc: 0.7617\n",
      "Epoch 36/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.4232 - acc: 0.8060 - val_loss: 0.4824 - val_acc: 0.7655\n",
      "Epoch 37/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.4233 - acc: 0.8035 - val_loss: 0.4818 - val_acc: 0.7696\n",
      "Epoch 38/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.4161 - acc: 0.8121 - val_loss: 0.4824 - val_acc: 0.7646\n",
      "Epoch 39/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.4135 - acc: 0.8114 - val_loss: 0.4764 - val_acc: 0.7718\n",
      "Epoch 40/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.4092 - acc: 0.8155 - val_loss: 0.4851 - val_acc: 0.7670\n",
      "Epoch 41/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.4010 - acc: 0.8220 - val_loss: 0.4710 - val_acc: 0.7713\n",
      "Epoch 42/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.3974 - acc: 0.8187 - val_loss: 0.4721 - val_acc: 0.7667\n",
      "Epoch 43/50000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.3950 - acc: 0.8231 - val_loss: 0.4696 - val_acc: 0.7725\n",
      "Epoch 44/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.3891 - acc: 0.8236 - val_loss: 0.4676 - val_acc: 0.7734\n",
      "Epoch 45/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.3890 - acc: 0.8260 - val_loss: 0.4703 - val_acc: 0.7730\n",
      "Epoch 46/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.3786 - acc: 0.8311 - val_loss: 0.4612 - val_acc: 0.7771\n",
      "Epoch 47/50000\n",
      "12474/12474 [==============================] - 15s 1ms/step - loss: 0.3748 - acc: 0.8357 - val_loss: 0.4580 - val_acc: 0.7775\n",
      "Epoch 48/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.3737 - acc: 0.8349 - val_loss: 0.4570 - val_acc: 0.7814\n",
      "Epoch 49/50000\n",
      "12474/12474 [==============================] - 15s 1ms/step - loss: 0.3735 - acc: 0.8343 - val_loss: 0.4583 - val_acc: 0.7780\n",
      "Epoch 50/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.3664 - acc: 0.8401 - val_loss: 0.4537 - val_acc: 0.7826\n",
      "Epoch 51/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.3628 - acc: 0.8394 - val_loss: 0.4514 - val_acc: 0.7843\n",
      "Epoch 52/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.3567 - acc: 0.8426 - val_loss: 0.4611 - val_acc: 0.7744\n",
      "Epoch 53/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.3560 - acc: 0.8443 - val_loss: 0.4644 - val_acc: 0.7783\n",
      "Epoch 54/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.3482 - acc: 0.8497 - val_loss: 0.4593 - val_acc: 0.7710\n",
      "Epoch 55/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.3495 - acc: 0.8482 - val_loss: 0.4436 - val_acc: 0.7869\n",
      "Epoch 56/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.3417 - acc: 0.8535 - val_loss: 0.4425 - val_acc: 0.7898\n",
      "Epoch 57/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.3427 - acc: 0.8517 - val_loss: 0.4417 - val_acc: 0.7884\n",
      "Epoch 58/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.3393 - acc: 0.8547 - val_loss: 0.4413 - val_acc: 0.7903\n",
      "Epoch 59/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.3341 - acc: 0.8555 - val_loss: 0.4381 - val_acc: 0.7927\n",
      "Epoch 60/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.3270 - acc: 0.8627 - val_loss: 0.4438 - val_acc: 0.7872\n",
      "Epoch 61/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.3261 - acc: 0.8623 - val_loss: 0.4346 - val_acc: 0.7934\n",
      "Epoch 62/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.3233 - acc: 0.8655 - val_loss: 0.4327 - val_acc: 0.7944\n",
      "Epoch 63/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.3172 - acc: 0.8640 - val_loss: 0.4365 - val_acc: 0.7929\n",
      "Epoch 64/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.3173 - acc: 0.8646 - val_loss: 0.4291 - val_acc: 0.7941\n",
      "Epoch 65/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.3121 - acc: 0.8669 - val_loss: 0.4405 - val_acc: 0.7949\n",
      "Epoch 66/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.3090 - acc: 0.8749 - val_loss: 0.4322 - val_acc: 0.7975\n",
      "Epoch 67/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.3068 - acc: 0.8717 - val_loss: 0.4276 - val_acc: 0.7963\n",
      "Epoch 68/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.2996 - acc: 0.8748 - val_loss: 0.4221 - val_acc: 0.7989\n",
      "Epoch 69/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.2982 - acc: 0.8798 - val_loss: 0.4290 - val_acc: 0.8018\n",
      "Epoch 70/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.2919 - acc: 0.8805 - val_loss: 0.4294 - val_acc: 0.7975\n",
      "Epoch 71/50000\n",
      "12474/12474 [==============================] - 15s 1ms/step - loss: 0.2910 - acc: 0.8785 - val_loss: 0.4214 - val_acc: 0.7985\n",
      "Epoch 72/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.2895 - acc: 0.8811 - val_loss: 0.4279 - val_acc: 0.7956\n",
      "Epoch 73/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.2839 - acc: 0.8819 - val_loss: 0.4216 - val_acc: 0.8054\n",
      "Epoch 74/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.2780 - acc: 0.8858 - val_loss: 0.4182 - val_acc: 0.8066\n",
      "Epoch 75/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.2746 - acc: 0.8865 - val_loss: 0.4143 - val_acc: 0.8050\n",
      "Epoch 76/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.2764 - acc: 0.8879 - val_loss: 0.4201 - val_acc: 0.8021\n",
      "Epoch 77/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.2741 - acc: 0.8854 - val_loss: 0.4173 - val_acc: 0.8023\n",
      "Epoch 78/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.2658 - acc: 0.8917 - val_loss: 0.4128 - val_acc: 0.8071\n",
      "Epoch 79/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.2629 - acc: 0.8915 - val_loss: 0.4184 - val_acc: 0.8086\n",
      "Epoch 80/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.2619 - acc: 0.8915 - val_loss: 0.4115 - val_acc: 0.8095\n",
      "Epoch 81/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.2586 - acc: 0.8945 - val_loss: 0.4166 - val_acc: 0.8102\n",
      "Epoch 82/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.2599 - acc: 0.8928 - val_loss: 0.4117 - val_acc: 0.8131\n",
      "Epoch 83/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.2486 - acc: 0.9006 - val_loss: 0.4093 - val_acc: 0.8095\n",
      "Epoch 84/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.2521 - acc: 0.8979 - val_loss: 0.4127 - val_acc: 0.8057\n",
      "Epoch 85/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.2465 - acc: 0.8992 - val_loss: 0.4074 - val_acc: 0.8064\n",
      "Epoch 86/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.2384 - acc: 0.9069 - val_loss: 0.4083 - val_acc: 0.8167\n",
      "Epoch 87/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.2383 - acc: 0.9061 - val_loss: 0.4034 - val_acc: 0.8177\n",
      "Epoch 88/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.2353 - acc: 0.9057 - val_loss: 0.4095 - val_acc: 0.8074\n",
      "Epoch 89/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.2343 - acc: 0.9083 - val_loss: 0.4180 - val_acc: 0.8013\n",
      "Epoch 90/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.2287 - acc: 0.9113 - val_loss: 0.4034 - val_acc: 0.8107\n",
      "Epoch 91/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.2293 - acc: 0.9089 - val_loss: 0.3991 - val_acc: 0.8155\n",
      "Epoch 92/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.2270 - acc: 0.9108 - val_loss: 0.3972 - val_acc: 0.8189\n",
      "Epoch 93/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.2221 - acc: 0.9139 - val_loss: 0.4033 - val_acc: 0.8131\n",
      "Epoch 94/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.2206 - acc: 0.9128 - val_loss: 0.4031 - val_acc: 0.8090\n",
      "Epoch 95/50000\n",
      "12474/12474 [==============================] - 15s 1ms/step - loss: 0.2132 - acc: 0.9203 - val_loss: 0.4144 - val_acc: 0.8086\n",
      "Epoch 96/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.2232 - acc: 0.9108 - val_loss: 0.4027 - val_acc: 0.8114\n",
      "Epoch 97/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.2089 - acc: 0.9190 - val_loss: 0.3946 - val_acc: 0.8225\n",
      "Epoch 98/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.2075 - acc: 0.9200 - val_loss: 0.4023 - val_acc: 0.8232\n",
      "Epoch 99/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.2034 - acc: 0.9191 - val_loss: 0.3933 - val_acc: 0.8228\n",
      "Epoch 100/50000\n",
      "12474/12474 [==============================] - 17s 1ms/step - loss: 0.2012 - acc: 0.9230 - val_loss: 0.3954 - val_acc: 0.8184\n",
      "Epoch 101/50000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.1990 - acc: 0.9248 - val_loss: 0.3976 - val_acc: 0.8201\n",
      "Epoch 102/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.1953 - acc: 0.9270 - val_loss: 0.3914 - val_acc: 0.8215\n",
      "Epoch 103/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.2010 - acc: 0.9229 - val_loss: 0.3988 - val_acc: 0.8170\n",
      "Epoch 104/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.1952 - acc: 0.9264 - val_loss: 0.3931 - val_acc: 0.8252\n",
      "Epoch 105/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.1923 - acc: 0.9276 - val_loss: 0.3928 - val_acc: 0.8232\n",
      "Epoch 106/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.1861 - acc: 0.9300 - val_loss: 0.3903 - val_acc: 0.8247\n",
      "Epoch 107/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.1863 - acc: 0.9305 - val_loss: 0.4133 - val_acc: 0.8151\n",
      "Epoch 108/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.1852 - acc: 0.9294 - val_loss: 0.3894 - val_acc: 0.8273\n",
      "Epoch 109/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.1794 - acc: 0.9342 - val_loss: 0.3902 - val_acc: 0.8271\n",
      "Epoch 110/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.1797 - acc: 0.9335 - val_loss: 0.3911 - val_acc: 0.8268\n",
      "Epoch 111/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.1810 - acc: 0.9329 - val_loss: 0.4096 - val_acc: 0.8182\n",
      "Epoch 112/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.1785 - acc: 0.9316 - val_loss: 0.3847 - val_acc: 0.8280\n",
      "Epoch 113/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.1751 - acc: 0.9347 - val_loss: 0.3844 - val_acc: 0.8302\n",
      "Epoch 114/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.1716 - acc: 0.9361 - val_loss: 0.3842 - val_acc: 0.8280\n",
      "Epoch 115/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.1658 - acc: 0.9387 - val_loss: 0.3953 - val_acc: 0.8280\n",
      "Epoch 116/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.1670 - acc: 0.9385 - val_loss: 0.3879 - val_acc: 0.8290\n",
      "Epoch 117/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.1673 - acc: 0.9378 - val_loss: 0.3853 - val_acc: 0.8321\n",
      "Epoch 118/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.1612 - acc: 0.9407 - val_loss: 0.3824 - val_acc: 0.8324\n",
      "Epoch 119/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.1632 - acc: 0.9412 - val_loss: 0.3852 - val_acc: 0.8338\n",
      "Epoch 120/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.1594 - acc: 0.9424 - val_loss: 0.3924 - val_acc: 0.8266\n",
      "Epoch 121/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.1556 - acc: 0.9437 - val_loss: 0.3793 - val_acc: 0.8331\n",
      "Epoch 122/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.1555 - acc: 0.9432 - val_loss: 0.3875 - val_acc: 0.8297\n",
      "Epoch 123/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.1531 - acc: 0.9449 - val_loss: 0.3850 - val_acc: 0.8309\n",
      "Epoch 124/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.1524 - acc: 0.9461 - val_loss: 0.3971 - val_acc: 0.8261\n",
      "Epoch 125/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.1478 - acc: 0.9489 - val_loss: 0.3784 - val_acc: 0.8365\n",
      "Epoch 126/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.1489 - acc: 0.9453 - val_loss: 0.3946 - val_acc: 0.8268\n",
      "Epoch 127/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.1445 - acc: 0.9476 - val_loss: 0.3803 - val_acc: 0.8336\n",
      "Epoch 128/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.1423 - acc: 0.9505 - val_loss: 0.3745 - val_acc: 0.8372\n",
      "Epoch 129/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.1396 - acc: 0.9508 - val_loss: 0.3804 - val_acc: 0.8338\n",
      "Epoch 130/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.1393 - acc: 0.9519 - val_loss: 0.3775 - val_acc: 0.8357\n",
      "Epoch 131/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.1426 - acc: 0.9484 - val_loss: 0.3827 - val_acc: 0.8329\n",
      "Epoch 132/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.1342 - acc: 0.9531 - val_loss: 0.3834 - val_acc: 0.8357\n",
      "Epoch 133/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.1405 - acc: 0.9497 - val_loss: 0.3981 - val_acc: 0.8307\n",
      "Epoch 134/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.1311 - acc: 0.9524 - val_loss: 0.3794 - val_acc: 0.8374\n",
      "Epoch 135/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.1295 - acc: 0.9553 - val_loss: 0.3798 - val_acc: 0.8345\n",
      "Epoch 136/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.1287 - acc: 0.9534 - val_loss: 0.3735 - val_acc: 0.8389\n",
      "Epoch 137/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.1253 - acc: 0.9579 - val_loss: 0.3739 - val_acc: 0.8381\n",
      "Epoch 138/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.1235 - acc: 0.9575 - val_loss: 0.3738 - val_acc: 0.8403\n",
      "Epoch 139/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.1304 - acc: 0.9539 - val_loss: 0.3753 - val_acc: 0.8389\n",
      "Epoch 140/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.1229 - acc: 0.9574 - val_loss: 0.3738 - val_acc: 0.8442\n",
      "Epoch 141/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.1220 - acc: 0.9599 - val_loss: 0.3774 - val_acc: 0.8408\n",
      "Epoch 142/50000\n",
      "12474/12474 [==============================] - 15s 1ms/step - loss: 0.1186 - acc: 0.9598 - val_loss: 0.3763 - val_acc: 0.8408\n",
      "Epoch 143/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.1166 - acc: 0.9602 - val_loss: 0.3726 - val_acc: 0.8430\n",
      "Epoch 144/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.1184 - acc: 0.9598 - val_loss: 0.3730 - val_acc: 0.8422\n",
      "Epoch 145/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.1153 - acc: 0.9606 - val_loss: 0.3706 - val_acc: 0.8454\n",
      "Epoch 146/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.1137 - acc: 0.9619 - val_loss: 0.3758 - val_acc: 0.8415\n",
      "Epoch 147/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.1141 - acc: 0.9606 - val_loss: 0.3809 - val_acc: 0.8434\n",
      "Epoch 148/50000\n",
      "12474/12474 [==============================] - 15s 1ms/step - loss: 0.1103 - acc: 0.9660 - val_loss: 0.3842 - val_acc: 0.8384\n",
      "Epoch 149/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.1101 - acc: 0.9637 - val_loss: 0.3763 - val_acc: 0.8432\n",
      "Epoch 150/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.1103 - acc: 0.9630 - val_loss: 0.3704 - val_acc: 0.8454\n",
      "Epoch 151/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.1087 - acc: 0.9651 - val_loss: 0.3734 - val_acc: 0.8442\n",
      "Epoch 152/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.1045 - acc: 0.9652 - val_loss: 0.3703 - val_acc: 0.8458\n",
      "Epoch 153/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.1058 - acc: 0.9650 - val_loss: 0.3773 - val_acc: 0.8442\n",
      "Epoch 154/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.1031 - acc: 0.9669 - val_loss: 0.3737 - val_acc: 0.8456\n",
      "Epoch 155/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.1030 - acc: 0.9650 - val_loss: 0.3722 - val_acc: 0.8485\n",
      "Epoch 156/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.1001 - acc: 0.9683 - val_loss: 0.3840 - val_acc: 0.8415\n",
      "Epoch 157/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.0985 - acc: 0.9672 - val_loss: 0.3790 - val_acc: 0.8425\n",
      "Epoch 158/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.0984 - acc: 0.9687 - val_loss: 0.3947 - val_acc: 0.8377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 159/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.0987 - acc: 0.9662 - val_loss: 0.3724 - val_acc: 0.8466\n",
      "Epoch 160/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.0899 - acc: 0.9732 - val_loss: 0.3723 - val_acc: 0.8451\n",
      "Epoch 161/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.0940 - acc: 0.9705 - val_loss: 0.3704 - val_acc: 0.8502\n",
      "Epoch 162/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.0994 - acc: 0.9679 - val_loss: 0.3687 - val_acc: 0.8487\n",
      "Epoch 163/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.0914 - acc: 0.9711 - val_loss: 0.3679 - val_acc: 0.8521\n",
      "Epoch 164/50000\n",
      "12474/12474 [==============================] - 15s 1ms/step - loss: 0.0863 - acc: 0.9747 - val_loss: 0.3714 - val_acc: 0.8487\n",
      "Epoch 165/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.0901 - acc: 0.9715 - val_loss: 0.3751 - val_acc: 0.8492\n",
      "Epoch 166/50000\n",
      "12474/12474 [==============================] - 15s 1ms/step - loss: 0.0879 - acc: 0.9724 - val_loss: 0.3723 - val_acc: 0.8485\n",
      "Epoch 167/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.0905 - acc: 0.9711 - val_loss: 0.3721 - val_acc: 0.8468\n",
      "Epoch 168/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.0860 - acc: 0.9728 - val_loss: 0.3723 - val_acc: 0.8478\n",
      "Epoch 169/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.0881 - acc: 0.9719 - val_loss: 0.3682 - val_acc: 0.8499\n",
      "Epoch 170/50000\n",
      "12474/12474 [==============================] - 15s 1ms/step - loss: 0.0854 - acc: 0.9735 - val_loss: 0.3674 - val_acc: 0.8509\n",
      "Epoch 171/50000\n",
      "12474/12474 [==============================] - 15s 1ms/step - loss: 0.0857 - acc: 0.9734 - val_loss: 0.3699 - val_acc: 0.8514\n",
      "Epoch 172/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.0839 - acc: 0.9746 - val_loss: 0.3690 - val_acc: 0.8502\n",
      "Epoch 173/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.0792 - acc: 0.9771 - val_loss: 0.3722 - val_acc: 0.8487\n",
      "Epoch 174/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.0852 - acc: 0.9724 - val_loss: 0.3773 - val_acc: 0.8504\n",
      "Epoch 175/50000\n",
      "12474/12474 [==============================] - 15s 1ms/step - loss: 0.0786 - acc: 0.9761 - val_loss: 0.3703 - val_acc: 0.8521\n",
      "Epoch 176/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.0779 - acc: 0.9756 - val_loss: 0.3724 - val_acc: 0.8531\n",
      "Epoch 177/50000\n",
      "12474/12474 [==============================] - 15s 1ms/step - loss: 0.0792 - acc: 0.9748 - val_loss: 0.3701 - val_acc: 0.8514\n",
      "Epoch 178/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.0761 - acc: 0.9770 - val_loss: 0.3719 - val_acc: 0.8523\n",
      "Epoch 179/50000\n",
      "12474/12474 [==============================] - 15s 1ms/step - loss: 0.0757 - acc: 0.9780 - val_loss: 0.3849 - val_acc: 0.8487\n",
      "Epoch 180/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.0838 - acc: 0.9739 - val_loss: 0.3752 - val_acc: 0.8509\n",
      "Epoch 181/50000\n",
      "12474/12474 [==============================] - 15s 1ms/step - loss: 0.0795 - acc: 0.9739 - val_loss: 0.3769 - val_acc: 0.8480\n",
      "Epoch 182/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.0717 - acc: 0.9789 - val_loss: 0.3827 - val_acc: 0.8514\n",
      "Epoch 183/50000\n",
      "12474/12474 [==============================] - 15s 1ms/step - loss: 0.0730 - acc: 0.9775 - val_loss: 0.3741 - val_acc: 0.8526\n",
      "Epoch 184/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.0721 - acc: 0.9784 - val_loss: 0.3717 - val_acc: 0.8531\n",
      "Epoch 185/50000\n",
      "12474/12474 [==============================] - 15s 1ms/step - loss: 0.0708 - acc: 0.9776 - val_loss: 0.3690 - val_acc: 0.8511\n",
      "Epoch 186/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.0670 - acc: 0.9827 - val_loss: 0.3685 - val_acc: 0.8533\n",
      "Epoch 187/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.0706 - acc: 0.9783 - val_loss: 0.3714 - val_acc: 0.8526\n",
      "Epoch 188/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.0707 - acc: 0.9780 - val_loss: 0.3936 - val_acc: 0.8451\n",
      "Epoch 189/50000\n",
      "12474/12474 [==============================] - 15s 1ms/step - loss: 0.0696 - acc: 0.9786 - val_loss: 0.3698 - val_acc: 0.8519\n",
      "Epoch 190/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.0693 - acc: 0.9780 - val_loss: 0.3730 - val_acc: 0.8519\n",
      "Epoch 191/50000\n",
      "12474/12474 [==============================] - 15s 1ms/step - loss: 0.0678 - acc: 0.9798 - val_loss: 0.3673 - val_acc: 0.8528\n",
      "Epoch 192/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.0708 - acc: 0.9781 - val_loss: 0.3743 - val_acc: 0.8540\n",
      "Epoch 193/50000\n",
      "12474/12474 [==============================] - 15s 1ms/step - loss: 0.0666 - acc: 0.9812 - val_loss: 0.3688 - val_acc: 0.8547\n",
      "Epoch 194/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.0634 - acc: 0.9836 - val_loss: 0.3705 - val_acc: 0.8499\n",
      "Epoch 195/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.0680 - acc: 0.9799 - val_loss: 0.3736 - val_acc: 0.8545\n",
      "Epoch 196/50000\n",
      "12474/12474 [==============================] - 15s 1ms/step - loss: 0.0595 - acc: 0.9843 - val_loss: 0.3713 - val_acc: 0.8516\n",
      "Epoch 197/50000\n",
      "12474/12474 [==============================] - 15s 1ms/step - loss: 0.0604 - acc: 0.9831 - val_loss: 0.3713 - val_acc: 0.8552\n",
      "Epoch 198/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.0617 - acc: 0.9828 - val_loss: 0.3917 - val_acc: 0.8514\n",
      "Epoch 199/50000\n",
      "12474/12474 [==============================] - 15s 1ms/step - loss: 0.0612 - acc: 0.9816 - val_loss: 0.3841 - val_acc: 0.8514\n",
      "Epoch 200/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.0623 - acc: 0.9807 - val_loss: 0.3733 - val_acc: 0.8552\n",
      "Epoch 201/50000\n",
      "12474/12474 [==============================] - 15s 1ms/step - loss: 0.0582 - acc: 0.9840 - val_loss: 0.3699 - val_acc: 0.8564\n",
      "Epoch 202/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.0606 - acc: 0.9825 - val_loss: 0.3738 - val_acc: 0.8526\n",
      "Epoch 203/50000\n",
      "12474/12474 [==============================] - 15s 1ms/step - loss: 0.0586 - acc: 0.9837 - val_loss: 0.3735 - val_acc: 0.8574\n",
      "Epoch 204/50000\n",
      "12474/12474 [==============================] - 15s 1ms/step - loss: 0.0568 - acc: 0.9839 - val_loss: 0.3889 - val_acc: 0.8497\n",
      "Epoch 205/50000\n",
      "12474/12474 [==============================] - 15s 1ms/step - loss: 0.0561 - acc: 0.9846 - val_loss: 0.3744 - val_acc: 0.8579\n",
      "Epoch 206/50000\n",
      "12474/12474 [==============================] - 15s 1ms/step - loss: 0.0560 - acc: 0.9852 - val_loss: 0.3749 - val_acc: 0.8567\n",
      "Epoch 207/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.0538 - acc: 0.9850 - val_loss: 0.4041 - val_acc: 0.8456\n",
      "Epoch 208/50000\n",
      "12474/12474 [==============================] - 15s 1ms/step - loss: 0.0566 - acc: 0.9838 - val_loss: 0.3703 - val_acc: 0.8598\n",
      "Epoch 209/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.0549 - acc: 0.9840 - val_loss: 0.3760 - val_acc: 0.8550\n",
      "Epoch 210/50000\n",
      "12474/12474 [==============================] - 15s 1ms/step - loss: 0.0544 - acc: 0.9860 - val_loss: 0.3808 - val_acc: 0.8550\n",
      "Epoch 211/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.0544 - acc: 0.9848 - val_loss: 0.3797 - val_acc: 0.8538\n",
      "Epoch 212/50000\n",
      "12474/12474 [==============================] - 15s 1ms/step - loss: 0.0558 - acc: 0.9840 - val_loss: 0.3748 - val_acc: 0.8559\n",
      "Epoch 213/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.0517 - acc: 0.9850 - val_loss: 0.3866 - val_acc: 0.8543\n",
      "Epoch 214/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.0495 - acc: 0.9877 - val_loss: 0.3734 - val_acc: 0.8540\n",
      "Epoch 215/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.0532 - acc: 0.9849 - val_loss: 0.3725 - val_acc: 0.8576\n",
      "Epoch 216/50000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12474/12474 [==============================] - 15s 1ms/step - loss: 0.0530 - acc: 0.9852 - val_loss: 0.3922 - val_acc: 0.8497\n",
      "Epoch 217/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.0526 - acc: 0.9855 - val_loss: 0.3688 - val_acc: 0.8600\n",
      "Epoch 218/50000\n",
      "12474/12474 [==============================] - 15s 1ms/step - loss: 0.0512 - acc: 0.9859 - val_loss: 0.3750 - val_acc: 0.8559\n",
      "Epoch 219/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.0523 - acc: 0.9848 - val_loss: 0.3694 - val_acc: 0.8552\n",
      "Epoch 220/50000\n",
      "12474/12474 [==============================] - 15s 1ms/step - loss: 0.0492 - acc: 0.9865 - val_loss: 0.3716 - val_acc: 0.8576\n",
      "Epoch 221/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.0512 - acc: 0.9862 - val_loss: 0.3772 - val_acc: 0.8593\n",
      "Epoch 222/50000\n",
      "12474/12474 [==============================] - 15s 1ms/step - loss: 0.0475 - acc: 0.9865 - val_loss: 0.3753 - val_acc: 0.8588\n",
      "Epoch 223/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.0493 - acc: 0.9869 - val_loss: 0.3738 - val_acc: 0.8576\n",
      "Epoch 224/50000\n",
      "12474/12474 [==============================] - 15s 1ms/step - loss: 0.0491 - acc: 0.9853 - val_loss: 0.3750 - val_acc: 0.8581\n",
      "Epoch 225/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.0470 - acc: 0.9867 - val_loss: 0.3765 - val_acc: 0.8593\n",
      "Epoch 226/50000\n",
      "12474/12474 [==============================] - 15s 1ms/step - loss: 0.0469 - acc: 0.9866 - val_loss: 0.3847 - val_acc: 0.8567\n",
      "Epoch 227/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.0429 - acc: 0.9902 - val_loss: 0.3752 - val_acc: 0.8600\n",
      "Epoch 228/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.0431 - acc: 0.9877 - val_loss: 0.3758 - val_acc: 0.8576\n",
      "Epoch 229/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.0472 - acc: 0.9875 - val_loss: 0.3744 - val_acc: 0.8576\n",
      "Epoch 230/50000\n",
      "12474/12474 [==============================] - 15s 1ms/step - loss: 0.0461 - acc: 0.9873 - val_loss: 0.3725 - val_acc: 0.8595\n",
      "Epoch 231/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.0432 - acc: 0.9883 - val_loss: 0.3791 - val_acc: 0.8588\n",
      "Epoch 232/50000\n",
      "12474/12474 [==============================] - 15s 1ms/step - loss: 0.0441 - acc: 0.9878 - val_loss: 0.4089 - val_acc: 0.8519\n",
      "Epoch 233/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.0457 - acc: 0.9871 - val_loss: 0.3743 - val_acc: 0.8579\n",
      "Epoch 234/50000\n",
      "12474/12474 [==============================] - 15s 1ms/step - loss: 0.0425 - acc: 0.9889 - val_loss: 0.3745 - val_acc: 0.8603\n",
      "Epoch 235/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.0420 - acc: 0.9889 - val_loss: 0.3746 - val_acc: 0.8593\n",
      "Epoch 236/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.0419 - acc: 0.9893 - val_loss: 0.3812 - val_acc: 0.8576\n",
      "Epoch 237/50000\n",
      "12474/12474 [==============================] - 15s 1ms/step - loss: 0.0421 - acc: 0.9892 - val_loss: 0.3806 - val_acc: 0.8593\n",
      "Epoch 238/50000\n",
      "12474/12474 [==============================] - 15s 1ms/step - loss: 0.0441 - acc: 0.9882 - val_loss: 0.3750 - val_acc: 0.8593\n",
      "Epoch 239/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.0410 - acc: 0.9894 - val_loss: 0.3733 - val_acc: 0.8608\n",
      "Epoch 240/50000\n",
      "12474/12474 [==============================] - 16s 1ms/step - loss: 0.0399 - acc: 0.9893 - val_loss: 0.3870 - val_acc: 0.8564\n",
      "Epoch 241/50000\n",
      "12474/12474 [==============================] - 15s 1ms/step - loss: 0.0431 - acc: 0.9880 - val_loss: 0.3845 - val_acc: 0.8567\n",
      "Accuracy: 0.738056460369164\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "str() argument 2 must be str, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d71a8a1f8b07>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[1;31m#model = load_model(final_filepath)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_cnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 291\u001b[1;33m \u001b[0mpredict_cnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-d71a8a1f8b07>\u001b[0m in \u001b[0;36mpredict_cnn\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m    272\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Accuracy: '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 274\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Precision: '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprecision_score\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    275\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Recall: '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecall_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'f1 score: '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: str() argument 2 must be str, not tuple"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import gc\n",
    "import sys\n",
    "import h5py\n",
    "import time\n",
    "import datetime\n",
    "import pickle\n",
    "import librosa\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import pandas as pd\n",
    "from pandas.plotting import parallel_coordinates\n",
    "from pydub import AudioSegment\n",
    "\n",
    "#imported for testing\n",
    "import wave\n",
    "import contextlib\n",
    "\n",
    "# for outputing file\n",
    "from scipy.cluster.vq import vq, kmeans, whiten\n",
    "import scipy.stats.stats as st\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.decomposition import PCA as sklearnPCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import auc, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn import mixture\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "import keras\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.constraints import maxnorm\n",
    "from keras.layers import Convolution1D, Dense, MaxPooling1D, Flatten, Add, Dropout, Input, Activation\n",
    "from keras.layers import TimeDistributed, Bidirectional, LSTM, LeakyReLU\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers, regularizers\n",
    "from keras.utils import np_utils, to_categorical\n",
    "\n",
    "from colorama import Fore, Back, Style\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "#warnings.filterwarnings('ignore')\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# confirm TensorFlow sees the GPU\n",
    "from tensorflow.python.client import device_lib\n",
    "# assert 'GPU' in str(device_lib.list_local_devices())\n",
    "\n",
    "# confirm Keras sees the GPU\n",
    "from keras import backend\n",
    "# print(len(backend.tensorflow_backend._get_available_gpus()) > 0)\n",
    "\n",
    "#warnings.filterwarnings('ignore')\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "sample_rate = 44100\n",
    "frame_number = 48\n",
    "hop_length = 441  # frame size= 2 * hop\n",
    "segment_length = int(sample_rate * 0.2)  # 0.2\n",
    "segment_pad = int(sample_rate * 0.02)     # 0.02\n",
    "overlapping = int(sample_rate * 0.1)   # 0.1\n",
    "\n",
    "classes = 2\n",
    "NumofFeaturetoUse = 100\n",
    "n_neurons = 4096\n",
    "dense_layers = 1\n",
    "num_layers = 4\n",
    "fillength = 5\n",
    "nbindex = 256\n",
    "dropout = 0.15\n",
    "n_batch = 128\n",
    "n_epoch = 50000\n",
    "\n",
    "def update_progress(progress):\n",
    "    bar_length = 100\n",
    "    if isinstance(progress, int):\n",
    "        progress = float(progress)\n",
    "    if not isinstance(progress, float):\n",
    "        progress = 0\n",
    "    if progress < 0:\n",
    "        progress = 0\n",
    "    if progress >= 1:\n",
    "        progress = 1\n",
    "\n",
    "    block = int(round(bar_length * progress))\n",
    "    clear_output(wait = True)\n",
    "    \n",
    "    text = \"Progress: [{0}] {1:.1f}%\".format( \"#\" * block + \"-\" * (bar_length - block), progress * 100)\n",
    "    print(text)\n",
    "\n",
    "prefix = '..//'\n",
    "h_feature_vector = np.load(prefix + 'Features//h_feature_vector_48.npy')\n",
    "h_label_vector = np.load(prefix + 'Features//h_label_vector_48.npy')\n",
    "a_feature_vector = np.load(prefix + 'Features//a_feature_vector_48.npy')\n",
    "a_label_vector = np.load(prefix + 'Features//a_label_vector_48.npy')\n",
    "n_feature_vector = np.load(prefix + 'Features//n_feature_vector_48.npy')\n",
    "n_label_vector = np.load(prefix + 'Features//n_label_vector_48.npy')\n",
    "s_feature_vector = np.load(prefix + 'Features//s_feature_vector_48.npy')\n",
    "s_label_vector = np.load(prefix + 'Features//s_label_vector_48.npy')\n",
    "\n",
    "h_feature_vector_test = np.load(prefix + 'Features//h_feature_vector_test_48.npy')\n",
    "h_label_vector_test = np.load(prefix + 'Features//h_label_vector_test_48.npy')\n",
    "a_feature_vector_test = np.load(prefix + 'Features//a_feature_vector_test_48.npy')\n",
    "a_label_vector_test = np.load(prefix + 'Features//a_label_vector_test_48.npy')\n",
    "n_feature_vector_test = np.load(prefix + 'Features//n_feature_vector_test_48.npy')\n",
    "n_label_vector_test = np.load(prefix + 'Features//n_label_vector_test_48.npy')\n",
    "s_feature_vector_test = np.load(prefix + 'Features//s_feature_vector_test_48.npy')\n",
    "s_label_vector_test = np.load(prefix + 'Features//s_label_vector_test_48.npy')\n",
    "\n",
    "h_label_vector[h_label_vector == 0] = 0\n",
    "a_label_vector[a_label_vector == 1] = 1\n",
    "h_label_vector_test[h_label_vector_test == 0] = 0\n",
    "a_label_vector_test[a_label_vector_test == 1] = 1\n",
    "\n",
    "h_label_vector = to_categorical(h_label_vector, num_classes = 2)\n",
    "a_label_vector = to_categorical(a_label_vector, num_classes = 2)\n",
    "h_label_vector_test = to_categorical(h_label_vector_test, num_classes = 2)\n",
    "a_label_vector_test = to_categorical(a_label_vector_test, num_classes = 2)\n",
    "\n",
    "# Load training npy files\n",
    "featureSet_training = np.vstack((h_feature_vector, a_feature_vector))\n",
    "label_training = np.vstack((h_label_vector, a_label_vector))\n",
    "\n",
    "# Load testing npy files\n",
    "featureSet_testing = np.vstack((h_feature_vector_test, a_feature_vector_test))\n",
    "label_testing = np.vstack((h_label_vector_test, a_label_vector_test))\n",
    "\n",
    "def float_compatible(input_np):\n",
    "\n",
    "    x = np.where(input_np >= np.finfo(np.float32).max)\n",
    "    for index in range(0, len(x[0])):\n",
    "        x_position = x[0][index]\n",
    "        y_position = x[1][index]      \n",
    "        input_np[x_position, y_position] = 0.0\n",
    "    input_np = np.nan_to_num(input_np)\n",
    "        \n",
    "    return input_np\n",
    "\n",
    "train_data = float_compatible((featureSet_training).astype(np.float32))\n",
    "eval_data = float_compatible((featureSet_testing).astype(np.float32))\n",
    "\n",
    "adam = optimizers.Adam(lr = 3e-5, beta_1 = 0.9, beta_2 = 0.999, epsilon = None, decay = 0, amsgrad = True)\n",
    "sgd = optimizers.SGD(lr = 0.01, decay = 1e-6, momentum = 0.9, nesterov = True)\n",
    "rmsprop = optimizers.RMSprop(lr = 0.0001, rho = 0.9, epsilon = None, decay = 0.0)\n",
    "adagrad = optimizers.Adagrad(lr = 0.01, epsilon = None, decay = 0.0)\n",
    "adadelta = optimizers.Adadelta(lr = 1.0, rho = 0.95, epsilon = None, decay = 0.0)\n",
    "adamax = optimizers.Adamax(lr = 0.002, beta_1 = 0.9, beta_2 = 0.999, epsilon = None, decay = 0.0)\n",
    "nadam = optimizers.Nadam(lr = 0.002, beta_1 = 0.9, beta_2 = 0.999, epsilon = None, schedule_decay = 0.004)\n",
    "\n",
    "featureSet = train_data\n",
    "Label = label_training\n",
    "featureSet = np.split(featureSet, np.array([NumofFeaturetoUse]), axis = 2)[0]\n",
    "\n",
    "print('training data: ' + str(featureSet.shape))\n",
    "print('training label: ' + str(Label.shape))\n",
    "\n",
    "featureSet_val = eval_data\n",
    "Label_val = label_testing\n",
    "featureSet_val = np.split(featureSet_val, np.array([NumofFeaturetoUse]), axis = 2)[0]\n",
    "\n",
    "print('evaluation data: ' + str(featureSet_val.shape))\n",
    "print('evaluation label: ' + str(Label_val.shape))\n",
    "\n",
    "def record(str_message, log_file):\n",
    "    str_message = str_message + '\\n'\n",
    "    file = open(log_file, 'a')\n",
    "    file.write(str_message)\n",
    "    file.close()\n",
    "\n",
    "def create_cnn(title, num_layers, n_neurons, n_batch, nbindex, dropout, classes, dense_layers):\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Convolution1D(nb_filter=nbindex, filter_length=fillength,\n",
    "                            input_shape=(featureSet.shape[1], featureSet.shape[2]), kernel_constraint=maxnorm(3)))\n",
    "    model.add(LeakyReLU(alpha=0.05))\n",
    "    model.add(MaxPooling1D(pool_size=2, strides=2, padding='valid'))\n",
    "    model.add(Dropout(dropout))\n",
    "    '''\n",
    "    model.add(Convolution1D(nb_filter=nbindex*2, filter_length=fillength,\n",
    "                            kernel_constraint=maxnorm(3)))\n",
    "    model.add(LeakyReLU(alpha=0.05))\n",
    "    model.add(MaxPooling1D(pool_size=2, strides=2, padding='valid'))\n",
    "    model.add(Dropout(dropout))\n",
    "    '''\n",
    "    model.add(Convolution1D(nb_filter=nbindex*3, filter_length=fillength,\n",
    "                            kernel_constraint=maxnorm(3)))\n",
    "    model.add(LeakyReLU(alpha=0.05))\n",
    "    model.add(MaxPooling1D(pool_size=2, strides=2, padding='valid'))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(Convolution1D(nb_filter=nbindex*2, filter_length=fillength,\n",
    "                            kernel_constraint=maxnorm(3)))  \n",
    "    model.add(LeakyReLU(alpha=0.05))\n",
    "    model.add(MaxPooling1D(pool_size=2, strides=2, padding='valid'))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(classes, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_cnn():\n",
    "    \n",
    "    save_to_path = prefix + str(num_layers) + \"_Layer(s)//\"\n",
    "\n",
    "    checkpoint_filepath = prefix + str(num_layers) + \"_Layer(s)//Checkpoint_\" + title + \".hdf5\"\n",
    "    final_filepath = prefix + str(num_layers) + \"_Layer(s)//Final_\" + title + \".hdf5\"\n",
    "\n",
    "    if not os.path.exists(save_to_path):\n",
    "        os.mkdir(save_to_path)\n",
    "\n",
    "    X, X_test, Y, Y_test= train_test_split(featureSet, Label, test_size = 0.25, shuffle = True)\n",
    "\n",
    "    model = create_cnn(title, num_layers, n_neurons, n_batch, nbindex, dropout, classes, dense_layers)\n",
    "\n",
    "    checkpoint = ModelCheckpoint(checkpoint_filepath, monitor = 'val_acc', verbose = 0, save_best_only = True, mode = 'auto')\n",
    "\n",
    "    early_stopping_monitor = EarlyStopping(patience = 500)\n",
    "\n",
    "    callbacks_list = [checkpoint, early_stopping_monitor]\n",
    "\n",
    "    model.fit(X, Y, nb_epoch = n_epoch, batch_size = n_batch,  callbacks = callbacks_list, validation_data = (X_test, Y_test), verbose = 1)\n",
    "\n",
    "    model.save_weights(final_filepath)\n",
    "\n",
    "    model.load_weights(checkpoint_filepath)\n",
    "\n",
    "    return model\n",
    "\n",
    "def predict_cnn(model):\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    for item in list(Label_val):\n",
    "            if item[0] > item[1]:\n",
    "                y_true.append(0)\n",
    "            elif item[0] < item[1]:\n",
    "                y_true.append(1)\n",
    "            else:\n",
    "                y_true.append(0)\n",
    "\n",
    "    for item in list(model.predict(featureSet_val)):\n",
    "            if item[0] > item[1]:\n",
    "                y_pred.append(0)\n",
    "            elif item[0] < item[1]:\n",
    "                y_pred.append(1)\n",
    "            else:\n",
    "                y_pred.append(0)\n",
    "\n",
    "    print('Accuracy: ' + str(accuracy_score(y_true, y_pred)))\n",
    "    print('Precision: ' + str(precision_score(y_true, y_pred)))\n",
    "    print('Recall: ' + str(recall_score(y_true, y_pred)))\n",
    "    print('f1 score: ' + str(f1_score(y_true, y_pred)))\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "\n",
    "    print('true positive ' + str(tp))\n",
    "    print('false positive ' + str(fp))\n",
    "    print('false negative ' + str(fn))\n",
    "    print('true negative ' + str(tn))\n",
    "\n",
    "title = 'H_A_neurons_' + str(n_neurons) + '_filters_' + str(\n",
    "    nbindex) + '_dropout_' + str(dropout) + '_epoch_' + str(n_epoch)\n",
    "\n",
    "final_filepath = prefix + str(num_layers) + \"_Layer(s)//Final_\" + title + \".hdf5\"\n",
    "#model = load_model(final_filepath)\n",
    "model = train_cnn()\n",
    "predict_cnn(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
