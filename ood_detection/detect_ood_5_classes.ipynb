{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import h5py\n",
    "\n",
    "import librosa\n",
    "\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "\n",
    "from sklearn.metrics import auc, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mil_squared_error(y_true, y_pred):\n",
    "    return tf.keras.backend.square(tf.keras.backend.max(y_pred) - tf.keras.backend.max(y_true))\n",
    "\n",
    "adam = tf.keras.optimizers.Adam(learning_rate=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41068, 48, 272)\n",
      "(41068, 6)\n",
      "(9995, 48, 272)\n",
      "(9995, 6)\n"
     ]
    }
   ],
   "source": [
    "# load training and testing ... \n",
    "\n",
    "condition = 'all'\n",
    "\n",
    "new_train = '..//train//' + condition + '//'\n",
    "new_test = '..//test//' + condition + '//'\n",
    "new_val = '..//val//' + condition + '//'\n",
    "\n",
    "def load_vectors(path):\n",
    "    files = sorted(os.listdir(path))\n",
    "    X = np.expand_dims(np.zeros((48, 272)), axis=0)\n",
    "    y = []\n",
    "    for npy in files:\n",
    "        current = np.load(path+npy)\n",
    "        X = np.vstack((X, current))\n",
    "        label = [files.index(npy)]*len(current)       \n",
    "        y = y + label       \n",
    "    X = X[1:]\n",
    "    y = tf.keras.utils.to_categorical(y)    \n",
    "    print(X.shape)\n",
    "    print(y.shape)    \n",
    "    return X, y\n",
    "    \n",
    "X_train, y_train = load_vectors(new_train)\n",
    "X_test, y_test = load_vectors(new_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "model = keras.models.load_model('D://GitHub//module//five_class_ood//model.hdf5', custom_objects={'mil_squared_error': mil_squared_error})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = model.predict(X_train)\n",
    "y_preds = [np.argmax(y) for y in y_preds]\n",
    "y_trues = [np.argmax(y) for y in y_trues]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f1_score(y_trues, y_preds, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_model(model):\n",
    "    first_half_model = keras.Sequential()\n",
    "    second_half_model = keras.Sequential()\n",
    "    for i in range(0, len(model.layers)):\n",
    "        \n",
    "        if i < len(model.layers) - 1:\n",
    "            first_half_model.add(model.layers[i])\n",
    "        else:\n",
    "            second_half_model.add(model.layers[i])\n",
    "            \n",
    "    print('the original model has ' + str(len(model.layers)) + ' layers.')\n",
    "    print('the penultimate (a.k.a. first half) model has ' + str(len(first_half_model.layers)) + ' layers.')\n",
    "    print('the penultimate (a.k.a. second half) model has ' + str(len(second_half_model.layers)) + ' layers.')\n",
    "    return first_half_model, second_half_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the original model has 34 layers.\n",
      "the penultimate (a.k.a. first half) model has 33 layers.\n",
      "the penultimate (a.k.a. second half) model has 1 layers.\n"
     ]
    }
   ],
   "source": [
    "first_half_model, second_half_model = split_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ash Gao\\Anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "# now, get emp_mean for each class\n",
    "\n",
    "emp_vals = [[], [], [], [], []]\n",
    "\n",
    "for X, y in zip(first_half_model.predict(X_train), y_train):\n",
    "    emp_vals[np.argmax(y)].append(X)\n",
    "    \n",
    "emp_vals = np.asarray(emp_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emp_mean(emp_val):\n",
    "    result = np.zeros(emp_val[0].shape).tolist()\n",
    "    \n",
    "    for penult_vector in emp_val:\n",
    "        #penult_vector has size (1024, 0)\n",
    "        for index in range(0, len(penult_vector)):\n",
    "            result[index] = penult_vector[index] + result[index]\n",
    "            \n",
    "    for index in range(0, len(result)):\n",
    "        result[index] = result[index]/len(result) \n",
    "        \n",
    "    #result = np.linalg.norm(result)\n",
    "    result = np.expand_dims(np.asarray(result), axis=1)\n",
    "    \n",
    "    print(result.shape)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4096, 1)\n",
      "(4096, 1)\n",
      "(4096, 1)\n",
      "(4096, 1)\n",
      "(4096, 1)\n"
     ]
    }
   ],
   "source": [
    "emp_means = [get_emp_mean(emp_vals[0]), get_emp_mean(emp_vals[1]), \\\n",
    "            get_emp_mean(emp_vals[2]), get_emp_mean(emp_vals[3]), get_emp_mean(emp_vals[4])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4096, 4096)\n"
     ]
    }
   ],
   "source": [
    "# get emprical covariance\n",
    "def get_emp_covar():\n",
    "    \n",
    "    flag = 0\n",
    "    \n",
    "    for X, y in zip(first_half_model.predict(X_train), y_train):\n",
    "        X = np.expand_dims(X, axis=1)\n",
    "        diff = X - emp_means[np.argmax(y)]\n",
    "        transpose = np.transpose(diff)\n",
    "        result = diff @ transpose\n",
    "        \n",
    "        if flag == 0:\n",
    "            emp_covar = result\n",
    "            flag = 1\n",
    "        else:\n",
    "            emp_covar = np.add(emp_covar, result)\n",
    "    \n",
    "    #division_vec = np.zeros(emp_covar.shape)\n",
    "    #division_vec = division_vec + len(y_train)\n",
    "            \n",
    "    emp_covar = emp_covar/len(y_train)\n",
    "    print(emp_covar.shape)\n",
    "    \n",
    "    return emp_covar\n",
    "\n",
    "emp_covar = get_emp_covar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_emp_covar = np.linalg.pinv(emp_covar)\n",
    "\n",
    "def get_emp_mahalanobis(y_pred, c):\n",
    "    emp_mean = emp_means[c]\n",
    "    diff = y_pred - emp_mean\n",
    "    transpose = np.transpose(diff)\n",
    "    \n",
    "    try:\n",
    "        emp_mahalanobis = np.linalg.norm(transpose @ inv_emp_covar @ diff)\n",
    "    except:\n",
    "        result = transpose * inv_emp_covar * diff\n",
    "        emp_mahalanobis = result\n",
    "        \n",
    "    #print(result.shape)\n",
    "    #print(emp_mahalanobis.shape)\n",
    "    \n",
    "    return emp_mahalanobis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_mahalanobis_all_classes = [[], [], [], [], []]\n",
    "\n",
    "for index in range(0, len(emp_vals)):\n",
    "    for y_pred in emp_vals[index]:\n",
    "        emp_m = get_emp_mahalanobis(y_pred, index)\n",
    "        emp_mahalanobis_all_classes[index].append(emp_m)\n",
    "        \n",
    "np.shape(np.asarray(emp_mahalanobis_all_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_mahalanobis_all_classes_stds = []\n",
    "emp_mahalanobis_all_classes_means = []\n",
    "\n",
    "for index in range(0, len(emp_mahalanobis_all_classes)):\n",
    "    mean = np.mean(emp_mahalanobis_all_classes[index])\n",
    "    std = np.std(emp_mahalanobis_all_classes[index])\n",
    "    \n",
    "    emp_mahalanobis_all_classes_means.append(mean)\n",
    "    emp_mahalanobis_all_classes_stds.append(std)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mahalanobis_coeff(m_mean, m_std, m_dists, threshold):    \n",
    "    for i in np.linspace(0, 5, 500):\n",
    "        count = 0\n",
    "        for m_dist in m_dists:\n",
    "            m_dist = np.linalg.norm(m_dist)\n",
    "            if m_mean - i*m_std < m_dist and m_mean + i*m_std > m_dist:\n",
    "                count += 1\n",
    "                \n",
    "        if count/len(m_dists) > threshold:            \n",
    "            #print(count/len(m_dists))\n",
    "            return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_mahalanobis_all_classes_coeffs = []\n",
    "\n",
    "for m_mean, m_std, m_dists in zip(emp_mahalanobis_all_classes_means, \\\n",
    "                                 emp_mahalanobis_all_classes_stds, emp_mahalanobis_all_classes):\n",
    "    coeff = get_mahalanobis_coeff(m_mean, m_std, m_dists, threshold=0.75)\n",
    "    emp_mahalanobis_all_classes_coeffs.append(coeff)\n",
    "    \n",
    "print(emp_mahalanobis_all_classes_coeffs)\n",
    "\n",
    "#emp_mahalanobis_all_classes_coeffs = [5, 5, 5, 5, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_in_distribution(y_pred, c):\n",
    "    m_dist = np.linalg.norm(get_emp_mahalanobis(y_pred, c))\n",
    "    std = emp_mahalanobis_all_classes_stds[c]\n",
    "    mean = emp_mahalanobis_all_classes_means[c]\n",
    "    coeff = emp_mahalanobis_all_classes_coeffs[c]\n",
    "    \n",
    "    if mean - coeff*m_std < m_dist and mean + coeff*m_std > m_dist:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load Calm samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectors(path, include_only):\n",
    "    files = sorted(os.listdir(path))\n",
    "    X = np.expand_dims(np.zeros((48, 272)), axis=0)\n",
    "    y = []\n",
    "    for npy in files:\n",
    "        if include_only in npy:        \n",
    "            current = np.load(path+npy)\n",
    "            X = np.vstack((X, current))\n",
    "            label = [files.index(npy)]*len(current)       \n",
    "            y = y + label\n",
    "            \n",
    "    X = X[1:]\n",
    "    #y = to_categorical(y)    \n",
    "    return X, y\n",
    "\n",
    "#Other_X, _ = load_vectors(new_test, 'Other')\n",
    "Calm_X, _ = load_vectors(new_test, 'Calm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preds_by_whole_model_Other = model.predict(Other_X)\n",
    "#preds_by_first_half_model_Other = first_half_model(Other_X)\n",
    "\n",
    "preds_by_whole_model_Calm = model.predict(Calm_X)\n",
    "preds_by_first_half_model_Calm = first_half_model.predict(Calm_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ind_samples_Other = 0\n",
    "#ood_samples_Other = 0\n",
    "\n",
    "ind_samples_Calm = 0\n",
    "ood_samples_Calm = 0\n",
    "'''\n",
    "for y_pred, c in zip(preds_by_first_half_model_Other, preds_by_whole_model_Other):\n",
    "    y_pred = np.expand_dims(y_pred, axis=0)\n",
    "    c = np.argmax(c)\n",
    "    \n",
    "    if check_if_in_distribution(y_pred, c):\n",
    "        ind_samples_Other += 1\n",
    "    else:\n",
    "        ood_samples_Other += 1\n",
    "'''        \n",
    "for y_pred, c in zip(preds_by_first_half_model_Calm, preds_by_whole_model_Calm):\n",
    "    y_pred = np.expand_dims(y_pred, axis=1)\n",
    "    c = np.argmax(c)\n",
    "    \n",
    "    if check_if_in_distribution(y_pred, c):\n",
    "        ind_samples_Calm += 1\n",
    "    else:\n",
    "        ood_samples_Calm += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(ind_samples_Other)\n",
    "#print(ood_samples_Other)\n",
    "print(ind_samples_Calm)\n",
    "print(ood_samples_Calm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ind and ood rates of the testing set with emotions in these 5 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test, y_test = load_vectors(new_test)\n",
    "\n",
    "preds_by_whole_model = model.predict(X_test)\n",
    "preds_by_first_half_model = first_half_model.predict(X_test)\n",
    "\n",
    "ind_samples = 0\n",
    "ood_samples = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for y_pred, c in zip(preds_by_first_half_model, preds_by_whole_model):\n",
    "    y_pred = np.expand_dims(y_pred, axis=1)\n",
    "    c = np.argmax(c)\n",
    "    \n",
    "    if check_if_in_distribution(y_pred, c):\n",
    "        ind_samples += 1\n",
    "    else:\n",
    "        ood_samples += 1\n",
    "        \n",
    "print(ind_samples)\n",
    "print(ood_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectors_with_calm(path):\n",
    "    files = sorted(os.listdir(path))\n",
    "    X = np.expand_dims(np.zeros((48, 272)), axis=0)\n",
    "    y = []\n",
    "    for npy in files:\n",
    "        current = np.load(path+npy)\n",
    "        X = np.vstack((X, current))\n",
    "        label = [files.index(npy)]*len(current)       \n",
    "        y = y + label       \n",
    "    X = X[1:]\n",
    "    y = tf.keras.utils.to_categorical(y)    \n",
    "    print(X.shape)\n",
    "    print(y.shape)    \n",
    "    return X, y\n",
    "    \n",
    "#X_train, y_train = load_vectors(new_train)\n",
    "X_test, y_test = load_vectors_with_calm(new_test)\n",
    "y_pred = [np.argmax(y) for y in model.predict(X_test)]\n",
    "y_true = [np.argmax(y) for y in y_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### without out of distribution technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import auc, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "print(f1_score(y_true, y_pred, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### with out of distribution technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = load_vectors_with_calm(new_test)\n",
    "\n",
    "first_half_pred = first_half_model.predict(X_test)\n",
    "whole_model_pred = model.predict(X_test)\n",
    "\n",
    "usable_X = []\n",
    "usable_true_labels = []\n",
    "\n",
    "for X, y_pred, c, true_label in zip(X_test, first_half_pred, whole_model_pred, y_test):\n",
    "    y_pred = np.expand_dims(y_pred, axis=0)\n",
    "    c = np.argmax(c)\n",
    "    if check_if_in_distribution(y_pred, c):\n",
    "        usable_X.append(X)\n",
    "        usable_true_labels.append(true_label)\n",
    "    \n",
    "usable_X = np.asarray(usable_X)\n",
    "\n",
    "y_true = [np.argmax(y) for y in usable_true_labels]\n",
    "y_pred = [np.argmax(y) for y in model.predict(usable_X)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f1_score(y_true, y_pred, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python37464bitbaseconda520b236a9c6d486bbc01b80a136b32a1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
