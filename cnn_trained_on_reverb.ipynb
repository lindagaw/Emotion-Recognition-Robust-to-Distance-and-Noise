{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import gc\n",
    "import sys\n",
    "import h5py\n",
    "import time\n",
    "import datetime\n",
    "import pickle\n",
    "import librosa\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import pandas as pd\n",
    "from pandas.plotting import parallel_coordinates\n",
    "from pydub import AudioSegment\n",
    "\n",
    "#imported for testing\n",
    "import wave\n",
    "import contextlib\n",
    "\n",
    "# for outputing file\n",
    "from scipy.cluster.vq import vq, kmeans, whiten\n",
    "import scipy.stats.stats as st\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.decomposition import PCA as sklearnPCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import auc, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn import mixture\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "import keras\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.constraints import maxnorm\n",
    "from keras.layers import Convolution1D, Dense, MaxPooling1D, Flatten, Add, Dropout, Input, Activation\n",
    "from keras.layers import TimeDistributed, Bidirectional, LSTM, LeakyReLU\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers, regularizers\n",
    "from keras.utils import np_utils, to_categorical\n",
    "\n",
    "#from tensorflow.python.client import device_lib\n",
    "from keras import backend\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5370, 48, 272)\n",
      "(5370, 5)\n",
      "(1792, 48, 272)\n",
      "(1792, 5)\n",
      "(1626, 48, 272)\n",
      "(1626, 5)\n"
     ]
    }
   ],
   "source": [
    "# load training and testing ... \n",
    "\n",
    "condition = 'reverb'\n",
    "\n",
    "new_train = 'D://Datasets//TRAINING//padded_deamplified_homenoised_reverberated//train//' + condition + '//'\n",
    "new_test = 'D://Datasets//TRAINING//padded_deamplified_homenoised_reverberated//test//' + condition + '//'\n",
    "new_val = 'D://Datasets//TRAINING//padded_deamplified_homenoised_reverberated//val//' + condition + '//'\n",
    "\n",
    "def load_vectors(path):\n",
    "    files = sorted(os.listdir(path))\n",
    "    X = np.expand_dims(np.zeros((48, 272)), axis=0)\n",
    "    y = []\n",
    "    for npy in files:\n",
    "        current = np.load(path+npy)\n",
    "        X = np.vstack((X, current))\n",
    "        label = [files.index(npy)]*len(current)       \n",
    "        y = y + label       \n",
    "    X = X[1:]\n",
    "    y = to_categorical(y)    \n",
    "    print(X.shape)\n",
    "    print(y.shape)    \n",
    "    return X, y\n",
    "    \n",
    "X_train, y_train = load_vectors(new_train)\n",
    "X_val, y_val = load_vectors(new_val)\n",
    "X_test, y_test = load_vectors(new_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mil_squared_error(y_true, y_pred):\n",
    "    return tf.keras.backend.square(tf.keras.backend.max(y_pred) - tf.keras.backend.max(y_true))\n",
    "\n",
    "adam = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
    "\n",
    "def create_cnn(num_layers, n_neurons, n_batch, nbindex, dropout, classes, dense_layers):\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Convolution1D(nb_filter=nbindex, filter_length=fillength, activation = 'relu',\n",
    "                            input_shape=(X_train.shape[1], X_train.shape[2]), kernel_constraint=maxnorm(3)))\n",
    "    model.add(MaxPooling1D(pool_size=2, strides=2, padding='valid'))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(Convolution1D(nb_filter=nbindex*2, filter_length=fillength, activation = 'relu',\n",
    "                            kernel_constraint=maxnorm(3)))\n",
    "    model.add(MaxPooling1D(pool_size=2, strides=2, padding='valid'))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(Convolution1D(nb_filter=nbindex*2, filter_length=fillength, activation = 'relu',\n",
    "                            kernel_constraint=maxnorm(3)))\n",
    "    model.add(MaxPooling1D(pool_size=2, strides=2, padding='valid'))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(Convolution1D(nb_filter=nbindex, filter_length=fillength, activation = 'relu',\n",
    "                            kernel_constraint=maxnorm(3)))  \n",
    "    model.add(MaxPooling1D(pool_size=2, strides=2, padding='valid'))\n",
    "    model.add(Dropout(dropout))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    for i in range(0, dense_layers):\n",
    "        model.add(Dense(n_neurons, activation = 'relu'))\n",
    "        model.add(Dropout(dropout))\n",
    "        \n",
    "    # Here, get the empirical class mean and covariance of training samples\n",
    "\n",
    "    model.add(Dense(classes, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=adam,\n",
    "                  metrics=['accuracy', mil_squared_error])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate = 44100\n",
    "frame_number = 48\n",
    "hop_length = 441  # frame size= 2 * hop\n",
    "segment_length = int(sample_rate * 0.2)  # 0.2\n",
    "segment_pad = int(sample_rate * 0.02)     # 0.02\n",
    "overlapping = int(sample_rate * 0.1)   # 0.1\n",
    "\n",
    "classes = 5\n",
    "NumofFeaturetoUse = 272\n",
    "n_neurons = 1024 * 4\n",
    "dense_layers = 3\n",
    "num_layers = 4\n",
    "fillength = 3\n",
    "nbindex = 1024 * 3\n",
    "dropout = 0.2\n",
    "n_batch = 128\n",
    "n_epoch = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ash Gao\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", input_shape=(48, 272), kernel_constraint=<keras.con..., filters=3072, kernel_size=3)`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Users\\Ash Gao\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", kernel_constraint=<keras.con..., filters=6144, kernel_size=3)`\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\Ash Gao\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", kernel_constraint=<keras.con..., filters=6144, kernel_size=3)`\n",
      "C:\\Users\\Ash Gao\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", kernel_constraint=<keras.con..., filters=3072, kernel_size=3)`\n"
     ]
    }
   ],
   "source": [
    "model = create_cnn(num_layers=num_layers, \n",
    "                   n_neurons=n_neurons, \n",
    "                   n_batch=n_batch, \n",
    "                   nbindex=nbindex, \n",
    "                   dropout=dropout, \n",
    "                   classes=classes, \n",
    "                   dense_layers=dense_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit model on training data\n",
      "Train on 5370 samples, validate on 1792 samples\n",
      "Epoch 1/100\n"
     ]
    }
   ],
   "source": [
    "print(\"Fit model on training data\")\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=n_batch,\n",
    "    epochs=n_epoch,\n",
    "    validation_data=(X_val, y_val), verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.metrics import f1_score\n",
    "\n",
    "y_preds = [np.argmax(val) for val in model.predict(X_test)]\n",
    "y_trues = [np.argmax(val) for val in y_test]\n",
    "print(accuracy_score(y_trues, y_preds))\n",
    "\n",
    "model.save('models//cnn.hdf5')\n",
    "\n",
    "return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python37464bitbaseconda520b236a9c6d486bbc01b80a136b32a1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
